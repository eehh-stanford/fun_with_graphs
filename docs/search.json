[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fun with Graphs",
    "section": "",
    "text": "Fun with Graphs\nIn the Fall of 1992, I was a first-year Ph.D. student in biological anthropology at Harvard. Mark Leighton was looking for someone to serve as a TF for his class, Primate Evolutionary Ecology. This was an amazing class that brought the theoretical evolutionary ecology of MacArthur and Levins and the behavioral ecology of Krebs, Davies, and Charnov to bear on problems of primate ecology. I didn’t need to teach, but I signed up anyway, hoping to really learn the material to which I had only a superficial introduction during undergraduate tutorials. This turned out to be a pretty fateful decision.\nOn every exam that Mark administered, there was a section cheekily titled Fun with Graphs, where students had to display their graphical reasoning chops. In general, I don’t think the students had all that much fun with this section. When Bill Durham and I first taught our class, Environmental Change and Emerging Infectious Disease, I brought back this tradition and it has been a staple of all my in-person exams for classes I’ve taught at Stanford ever since.\nI worry that the sort of intuitive graphical reasoning that motivated so much of this incredible theory is not developed in students who must expend all their effort studying for tests which will allow them admission to increasingly competitive universities. I also worry that theoretical ecologists (along with formal demographers and mathematical epidemiologists) have not reproduced themselves culturally and this important material is increasingly not taught.\nYou could think of this book as essentially Nonstandard Uses of R. R graphics are clearly designed for plotting data. However, R is a highly versatile and powerful tool for making theoretical and expository figures in science. That’s the vibe. Maybe there will be more…",
    "crumbs": [
      "Fun with Graphs"
    ]
  },
  {
    "objectID": "interpreting.html#introduction",
    "href": "interpreting.html#introduction",
    "title": "1  Interpreting Scientific Figures",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nUnderstanding scientific figures is an important part of becoming a scientist or a critical consumer of scientific information. This is a skill that, alas, is generally not taught in most schools. Here, I will try to provide a gentle introduction to reading scientific figures, especially theoretical plots. I have companion notes that describe how to generate scientific plots in R.\nWe use theory in science to bring order to the complexity we observe in the world. Theory generates our hypotheses but it also guides us in what we observe, how we measure it, and what we should find surprising. Surprise is essential for the scientific enterprise because it is the surprise that comes when we observe something novel from a process we thought we understood that generates innovation and explanation.\nA couple starting points. We will use some very basic calculus here. Derivatives, second derivatives, and Taylor series."
  },
  {
    "objectID": "interpreting.html#lines",
    "href": "interpreting.html#lines",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.2 Lines",
    "text": "2.2 Lines\nPresumably, we all remember the formula for a straight line from high school algebra:\n\\[\ny = mx + b,\n\\]\nwhere \\(m\\) is the slope and \\(b\\) is the \\(y\\)-intercept.\n\nm &lt;- 2\nb &lt;- 1\ncurve(m*x+b, 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\n\nClearly, this is a straight line. What this means is that for whatever \\(x\\)-value you increment, you will increase by a factor of two.\n\n# define a linear function\nlin &lt;- function(x,m=2,b=1) m*x + b\n# draw curve, add increments\ncurve(lin(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,lin(1),3,lin(1), col=\"red\", lty=3)\nsegments(3,lin(1),3,lin(3), col=\"red\", lty=3)\nsegments(4,lin(4),6,lin(4), col=\"red\", lty=3)\nsegments(6,lin(4),6,lin(6), col=\"red\", lty=3)\nsegments(7,lin(7),9,lin(7), col=\"red\", lty=3)\nsegments(9,lin(7),9,lin(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nOf course, we can show this analytically by calculating the derivative. Let \\(f(x) = mx + b\\), then \\(f'(x) = m\\). Not surprising since \\(m\\) is literally the slope that the rate of change in \\(f(x)\\) is always \\(m\\).\nLinear change is a touchstone. We are often interested if something is changing faster or more slowly than linear.\nWe often use linear functions to approximate more complex functions in some restricted range. For example, in the model of optimal virulence, discussed below, we need to draw a tangent line to the function relating transmissibility to disease-induced mortality. This tangent line is a linear approximation of that function in the vicinity of the optimal virulence.\nNote that this is what a derivative is. It’s linear representation of the slope of a function over an infinitesimal change of the input variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#curves",
    "href": "interpreting.html#curves",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.3 Curves",
    "text": "2.3 Curves\n\n2.3.1 Polynomial Curves\nWhen something is nonlinear, it changes at different rates in different parts of the curve. The simplest extension from a straight line is a polynomial, e.g., a quadratic function.\n\n# quadratic function\nquad &lt;- function(x,m=2,b=1) m*x^2+b\ncurve(quad(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,quad(1),3,quad(1), col=\"red\", lty=3)\nsegments(3,quad(1),3,quad(3), col=\"red\", lty=3)\nsegments(4,quad(4),6,quad(4), col=\"red\", lty=3)\nsegments(6,quad(4),6,quad(6), col=\"red\", lty=3)\nsegments(7,quad(7),9,quad(7), col=\"red\", lty=3)\nsegments(9,quad(7),9,quad(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nThe quadratic curve changes at an increasing rate. For \\(f(x) = mx^2 + b\\), \\(f'(x)=2x\\).\n\n\n2.3.2 Exponential and Logarithmic Curves\nWhen people say that something is growing “exponentially,” what they usually mean is that it’s growing fast. Exponential growth is much more specific than that (and there are, indeed, ways to grow much faster than exponentially!). In continuous time, something grows exponentially if it increases at a constant rate regardless of its size.\nExponential growth has the wild property that the derivative of an exponential is proportional to the exponential itself. For example, if \\(f(x)=e^r\\), then \\(f'(x) = e^r\\). If \\(f(x)=e^{2r}\\), then \\(f'(x) = 2e^{2r}\\), and so on.\n\ncurve(exp(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,exp(1),3,exp(1), col=\"red\", lty=3)\nsegments(3,exp(1),3,exp(3), col=\"red\", lty=3)\nsegments(4,exp(4),6,exp(4), col=\"red\", lty=3)\nsegments(6,exp(4),6,exp(6), col=\"red\", lty=3)\nsegments(7,exp(7),9,exp(7), col=\"red\", lty=3)\nsegments(9,exp(7),9,exp(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nLooking at the increments, we quickly discern another important feature of exponential growth: it sneaks up on you! In the early phase of an exponential-growth process, it can be quite difficult to tell it apart from linear growth or even no growth. The red dotted lines showing the growth between \\(x=1\\) and \\(x=3\\) are barely visible.\nBecause of the explosiveness of exponential growth, the initial conditions can matter a lot for outcomes. Compare the following two curves:\n\ncurve(5*exp(x), 0, 10, lwd=3, col=\"red\", xlab=\"x\", ylab=\"y\")\ncurve(1*exp(x), 0, 10, lwd=3, col=\"black\", add=TRUE)\n\n\n\n\n\n\n\n\nWe predict that virulence of a virus, for example, will increase with the size of the infectious innoculum. The intuition behind this prediction is that a larger innoculum provides a larger initial population size that can quickly increase to overwhelm a host’s immunological defenses. The smaller size of the viral population for any given time after infection arising from the smaller innoculum provides a greater likelihood that the host will control the infection quickly and with less tissue damage, etc.\nIt’s also super-important to note that things can also decrease exponentially! Exponential decay is a thing.\n\ncurve(50*exp(-x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,50*exp(-1),3,50*exp(-1), col=\"red\", lty=3)\nsegments(3,50*exp(-1),3,50*exp(-3), col=\"red\", lty=3)\nsegments(4,50*exp(-4),6,50*exp(-4), col=\"red\", lty=3)\nsegments(6,50*exp(-4),6,50*exp(-6), col=\"red\", lty=3)\nsegments(7,50*exp(-7),9,50*exp(-7), col=\"red\", lty=3)\nsegments(9,50*exp(-7),9,50*exp(-9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nCompare them.\n\n# draw exp first to make sure axes fit\nrequire(viridisLite)\nc &lt;- plasma(3)\ncurve(quad(x), 0, 10, lwd=3, col=c[1],\n      xlab=\"x\", ylab=\"y\",\n      xaxs=\"i\", yaxs=\"i\")\ncurve(lin(x), 0, 10, lwd=3, col=\"black\", add=TRUE)\ncurve(exp(x), 0, 10, lwd=3, col=c[2], add=TRUE)\ncurve(log(x), 0.01, 10, lwd=3, col=c[3], add=TRUE)\nlegend(\"topleft\", c(\"linear\",\"quadratic\",\"exponential\",\"logarithmic\"),\n       col=c(\"black\",c), lty=1, lwd=3)\n\n\n\n\n\n\n\n\n\n\n2.3.3 Power Laws\nIt turns out that much of the world – particularly in biology – scales according to a power law. Nearly everything you can imagine measuring about an organism scales with an organism’s body mass and it does so according to a power law. So for some outcome \\(Y\\) (e.g., lifespan, annual fertility, brain mass, metabolic rate, etc.), where we let \\(W\\) indicate body mass, the scaling relationship takes the form\n\\[\nY = A W^a.\n\\]\nIf \\(a&gt;1\\), this curve will be convex (i.e., increasing returns to size), while if \\(0&lt;a&lt;1\\), the curve will be concave. If \\(a=1\\), then we simply have a straight line with slope \\(A\\) and intercept zero. In comparative biology, the case where \\(a=1\\) is known as “isometry” and the case where \\(a \\neq 1\\) is known as “allometry”.\nIf we take logarithms of both sides of the power-law relation, we get a linearized form,\n\\[\n\\log(Y) = \\log(A) + a \\log(W).\n\\]\nPlotting data on double-logarithmic axes can help in diagnosing a power law.\nWhen \\(a&lt;0\\), we have the case of power-law decay. This provides a very interesting case where the decay of some function can be considerably slower than exponential. For example, most of the common probability distributions that we use (e.g., normal, exponential, Poisson, binomial) have “exponential” tails. This means that the probability associated with a particular value decays exponentially as the values move away from the region of highest probability. In contrast, power-law probability distributions can have fat tails, meaning that extreme values are more likely than they would be under a comparable probability distribution with exponential decay.\nThe key difference between a power law and an exponential, which at first glance appear to be quite similar, is that for the power law, the power is constant (\\(x^a\\)) whereas for an exponential, the power is the variable \\(a^x\\) (where we usually use the specific value of \\(a=e\\), where \\(e\\) is the base of the natural logarithm). Note that we’ve already looked at a comparison between exponential growth and power-law growth, when we compared the quadratic (\\(a=2\\)) to the exponential. Let’s look at power-law decay now.\n\ncurve(0.5^x,0, 10, lwd=3, xaxs=\"i\", xlab=\"x\", ylab=\"y\")\ncurve(x^-2, 0, 10, lwd=3, col=\"red\", add=TRUE)\nlegend(\"topright\",c(\"exponential\",\"power\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe power decay starts much higher (it is, in fact, asymptotic to the \\(y\\)-axis) and declines very rapidly at first. However, while the exponential curve will quickly approach the \\(x\\)-axis (to which it is asymptotic), the polynomial power-law curve will approach it very slowly. For the exponential curve, every \\(x\\)-increment of one reduces the value of \\(y\\) by a half. In contrast, for the power-law, every increment contributes a tiny marginal decay as the values of \\(x\\) increase. For the exponential the ratio of subsequent \\(y\\) values is \\(0.5^{x+1}/0.5^{x} = 0.5^1=0.5\\) for all values of \\(x\\). The analogous ratio for the power law changes for different values of \\(x\\). When \\(x\\) is small, the ratio of successive \\(y\\) values is similar to the exponential. For example, when \\(x=2\\), \\((x+1)^{-2}/x^{-2}=0.44\\). However, when \\(x=100\\), the ratio is nearly one (=0.98), meaning that the curve is decreasing very slowly. Successive values of the function are nearly identical, making the ratio close to one.\n\n\n2.3.4 Why We Care About Power Laws\nThe figure above shows how the probability of very high values decays to zero for the expontial function, but decays slowly for the polynomial power law. This turns out to be very important for thinking about the probability of extreme events.\nWe say that a probability distribution has a heavy tail if, for large values of \\(x\\), the log of the probability of \\(x\\) is sublinear (Nair, Wierman, and Zwart 2022). Remember that when we take the logarithm of an exponential, we linearize it, so this is equivalent to saying that a heavy-tailed distribution decays more slowly than exponential. This is exactly what power-law distributions do. Their tails decay slowly – much slower than an exponential.\nWhat this means is that if a random variable is characterized by a power-law distribution, extreme values, while not likely, are also not impossible. Heavy tails make the impossible only unlikely.\nConsider the case of a normal (Gaussian) distribution, which has exponential tails and the related \\(t\\)-distribution, which has heavy tails. The probability density function (pdf) for the normal distribution is:\n\\[\nf(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}},\n\\] where the parameters \\(\\mu\\) and \\(\\sigma^2\\) represent the mean and variance of the distribution.\nThe pdf for the \\(t\\) distribution is:\n\\[\nf(x) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\pi \\nu}\\; \\Gamma\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n\\] where the parameter \\(\\nu\\) represents the degrees of freedom of the distribution. Don’t worry too much about that first fraction in the pdf. That is just a normalizing factor that ensures that the probability integrates to one (as all pdfs must).\nNote that, as with our last section, the pdf for the normal distribution (with exponential tail) has \\(x\\) in the exponent, while for the \\(t\\)-distribution (with polynomial tail), \\(x\\) is being raised to a constant power related to the degrees of freedom.\nWe can compare the two probability density functions:\n\nx &lt;- seq(-10,10,length=1000)\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", ylim=c(0,0.42), xlim=c(-10,10),\n     xlab=\"Outcome (x)\", ylab=\"Probability of Outcome (P(x))\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")\nlegend(\"topright\",c(\"normal\",\"t\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe normal distribution is higher (i.e., has more mass) in the middle of the plot. The \\(t\\) distribution has less of its mass in the central part of the distribution, and spreads out more of its total mass to its tails.\nI should probably note that this is a particularly extreme \\(t\\) distribution, as it only has one degree of freedom (this distribution is also known as a Cauchy distribution). This makes it particularly easy to see the differences between the normal and the \\(t\\) distribution.\nWe can focus on just the right tail. Normal theory tells us that the probability of observing any values greater than four standard deviations aboce the mean is essentially zero. We can calculate the remaining area in the tail above four standard deviations (for the standard normal distribution) by subtracting the cumulative distribution function at \\(x=4\\) from one.\n\n1-pnorm(4)\n\n[1] 3.167124e-05\n\n\nPretty nearly zero.\nWhat about the same calculation for our \\(t\\) distribution with one degree of freedom?\n\n1-pt(4,1)\n\n[1] 0.07797913\n\n\nNearly eight percent of the total probability remains in the tail! That is, you have a 7.8% chance of observing a value of \\(x \\geq 4\\).\nLet’s compare a plot of the tails of the two distributions:\n\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", ylim=c(0,0.4), xlim=c(0,10),\n     xlab=\"Outcome (x)\", ylab=\"Probability Density of Outcome (f(x))\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")\nabline(h=0.01,lty=3)\naxis(1)\naxis(2)\nlegend(\"topright\",c(\"normal\",\"t\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe horizontal dotted line indicates the value where the probability density is 1%. We can see that the \\(t\\) distribution crosses this thresholds at a value of approximately \\(x=6\\). The term six-sigma event is often used to indicate an outcome that is essentially impossible under our current understanding of a system. This comes from the fact that for a standard normal distribution, the variable \\(x\\) is equivalent to the number of standard deviations from the mean of zero (a standard normal has \\(\\mu=0\\) and \\(\\sigma^2=\\sigma=1\\)). Thus, for a standard normal distribution, an observed value of of \\(x=6\\) is six standard deviations (or \\(6\\sigma\\)) greater than the mean and, based on what we know about the normal distribution, has a probability of essentially zero. Not so for the \\(t\\) distribution! We can see that even extremely high values of \\(x\\) have associated tail probabilities that, while low, are still a long way from zero.\n\n1-pt(6,1)\n\n[1] 0.05256846\n\n1-pt(8,1)\n\n[1] 0.03958342\n\n1-pt(10,1)\n\n[1] 0.03172552\n\n\nThe key question, of course, is whether a \\(t\\) distribution (or other heavy-tailed distribution) represents any phenomena we actually care about scientifically. Turns out, there are lots of things in Nature that follow power laws. One of the more interesting examples where the outcome is actually a \\(t\\) distribution was described by Weitzman (2009). When you have to learn about the mean value of some normally-distributed variable, but have limited opportunities to learn, the distribution of that mean is itself a \\(t\\) distribution. This has huge implications for decision-making that I will discuss elsewhere.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#convexity-and-concavity",
    "href": "interpreting.html#convexity-and-concavity",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.4 Convexity and Concavity",
    "text": "2.4 Convexity and Concavity\nThe derivative of a function provides a measure of how fast a function is changing. The second derivative measures how that rate of change itself is changing. In this sense, it measures the curvature of a function.\nMany theoretical models depend on the curvature of functions to make their predictions. A common assumption employed in many theoretical models is that of concavity. A very common use of concavity in theory is when curve shows diminishing marginal returns. The word “marginal” essentially means the derivative, so diminishing marginal returns means that the derivative of the function is getting smaller for larger values of the input.\nThe classic trade-off model for the evolution of virulence relies on the concavity of transmissibility with respect to disease-induced mortality. If virulence produces decreasing marginal transmissibility with respect to disease-induced mortality, then selection will favor intermediate virulence. Denote virulence by \\(x\\). Both transmission and disease-induced mortality are functions of virulence: \\(\\beta(x)\\) and \\(\\delta(x)\\). The fitness measure for the pathogen is, as usual, \\(R_0\\), which we can write as\n\\[\nR_0 = \\frac{\\beta(x)}{\\mu + \\delta(x)},\n\\]\nwhere \\(\\mu\\) is the disease-independent mortality.\nTo find the optimal value of virulence, differentiate with respect to \\(x\\) and solve for \\(dR_0/dx=0\\). Employing the quotient rule for differentiation and doing a little algebra to tidy up, we get:\n\\[\n\\frac{d \\beta(x)}{d \\delta(x)} = \\frac{\\beta(x^*)}{\\mu + \\delta(x^*)},\n\\]\nwhere \\(x^*\\) indicates the optimal value of virulence.\nThe geometric interpretation of this result is that optimal virulence satisfies the condition that a line, rooted at the origin, is tangent to the curve relating transmissibility to mortality. This result is known as the Marginal Value Theorem in behavioral ecology and, in addition to describing a model for optimal virulence, also predicts the optimal length of time for a foraging bout in a feeding patch or the optimal copula duration when a male has multiple mating opportunities but his sperm can be displaced by subsequent matings.\n\nx &lt;- seq(0,30,length=500)\n# transmissibility function fp&gt; 0 fpp &lt; 0\nf &lt;- function(x) {\n  0.5 - exp(-0.2*(x-7))\n}\n# derivative of the utility function\nfprime &lt;- function(x) {\n  0.2*exp(-0.2*(x-7))\n}\n\n# 1st-degree Taylor series around x: f + fp*(z-x) = 0\n# z = x -(f/fp)\n# solve for tangency; find the root of this\nxinter &lt;- function(x) {\n  return(x - f(x)/fprime(x))\n}\n\nsoln &lt;- uniroot(xinter,c(0,40))\nplot(x,f(x), type=\"l\", lwd=2, xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE,\n     xlab=\"Mortality\",\n     ylab=\"Transmissibility\",\n     ylim=c(0,0.7))\naxis(1,labels=FALSE,tick=FALSE)\naxis(2,labels=FALSE,tick=FALSE)\nbox()\nlines(x,(f(soln$root)/soln$root)*x,col=grey(0.75))\nsegments(soln$root,0,soln$root,f(soln$root), lty=2, col=\"red\")\nsegments(0,f(soln$root),soln$root,f(soln$root), lty=2, col=\"red\")\nmtext(expression(paste(delta,\"*\")),1,at=soln$root, padj=1)\nmtext(expression(paste(beta,\"*\")),2,at=f(soln$root),padj=0.5, adj=1.5, las=2)\nmtext(expression(mu),1,at=5, padj=1)\n\n\n\n\n\n\n\n\nWhat would happen if the function was convex (\\(f''(x)&gt;0\\)), rather than concave? There can be no intermediate optimum for a such a convex function. The optimal virulence is maximum.\nIn one of the most important papers in the field of life history theory, Gadgil and Bossert (1970) noted that the only conditions under which natural selection will favor intermediate reproductive effort are when the fitness gains to effort are concave and, importantly, that the costs of effort are either linear or convex. We can easily visualize why this is the case.\n\nx &lt;- seq(1,11,,110)\ny &lt;- 4*log(x)\ny1 &lt;- 0.1*exp(x/2)\ny2 &lt;- 0.1*exp(x/1.5)\n# maxima\nd1 &lt;- y-y1\nd2 &lt;- y-y2\nmax1 &lt;- x[which(d1==max(d1))]\nmax2 &lt;- x[which(d2==max(d2))]\n\n### concave benefits/concave costs\nplot((x-1)/10,y/11, type=\"l\", lwd=3, \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\",  \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10, y/11 + 0.01*x, lwd=3, col=\"red\")\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\nabline(v=0, col=grey(0.65))\ntitle(\"No Reproduction\")\n\n\n\n\n\n\n\n### concave benefits/convex costs\nplot((x-1)/10,y/11, type=\"l\", lwd=3, \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\", \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10,y1/11, lwd=3, col=\"red\")\nabline(v=max1/11, col=grey(0.65))\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\ntitle(\"Intermediate Reproduction\")\n\n\n\n\n\n\n\n### concave benefits/concave costs, full RE\nplot((x-1)/10,y/11, type=\"l\", lwd=3, col=\"red\", \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\", \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10, y/11 + 0.01*x, lwd=3, col=\"black\")\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\nabline(v=1, col=grey(0.65))\ntitle(\"Maximal (Suicidal) Reproduction\")\n\n\n\n\n\n\n\n\nOnly for the concave benefit/linear cost case does the maximum difference between the curves lie in the middle of the plot.\n\n2.4.1 Concavity Introduces Asymmetries\nSuppose you have a curve representing the fitness, \\(w\\), corresponding to a given level of effort, \\(x\\), similar to the Gadgil-Bossert curves discussed above. Further suppose that this curve is concave, showing diminishing marginal returns so that \\(w'(x)&gt;0\\) and \\(w''&lt;0\\).\nStarting at some point on this curve, say at the mean effort \\(\\bar{x}\\), imagine you flip a coin and get decremented a unit’s worth of fitness if it comes up heads and increase a unit’s worth if it comes up tails. This is known as a lottery, a decision in which there is a discrete, variable payoff. We can plot this as follows:\n\n## risk-aversion\nx &lt;- seq(0,5,length=1000)\nr &lt;- 0.75\nfx &lt;- 1-exp(-r*x)\n## for part deux\naaa &lt;- (fx-0.4882412)^2\nwhich(aaa==min(aaa))\n\n[1] 179\n\n#[1] 179\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"Fitness (w(x))\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(-0.1,5.1), ylim=c(0,1))\n#segments(0,0,5,fx[1000], lwd=2, col=grey(0.75))\naxis(1, at=c(0,2.5,5), labels=c(expression(x[0]), expression(bar(x)),\n                                expression(x[1])), tick=FALSE)\nsegments(2.5,0,2.5,0.846645, lwd=3, lty=1, col=grey(0.65))\nsegments(2.5,0.846645,0,0.846645, lwd=3, lty=1, col=\"red\")\narrows(0,0.846645,0,0.01, lwd=3, lty=1, col=\"red\", length=.25,angle=10)\nsegments(2.5,0.846645,5,0.846645, lwd=3, lty=1, col=\"red\")\narrows(5,0.846645,5,fx[1000], lwd=3, lty=1, col=\"red\", length=.25,angle=10)\nlines(x,fx, lwd=3, col=\"black\")\n\n\n\n\n\n\n\n\nWhere do these seemingly very specific numbers that I use to draw the segments and arrows come from? In particular, the value of 0.846645 is simply the value of the utility function at \\(\\bar{x} = 2.5\\): \\(1-\\exp(-0.75*0.25) = 0.846645\\).\nThe upside of this lottery increases fitness considerably less than the downside reduces it. This arises because of the curvature of the function, in particular, its diminishing marginal fitness returns to effort. This is a very important insight and defines the phenomenon of risk aversion. Risk-aversion in lotteries where the fitness function is a concave function of effort are an application of Jensen’s Inequality, which states that for a concave function, \\(w(x)\\),\n\\[\nw(E(x)) \\geq E(w(x)),\n\\]\nwhere \\(E()\\) indicates mathematical expectation.\nWe can show this graphically. We will draw a chord connecting the upside- and downside-payoffs, the midpoint of which is \\(E(w(x))\\). Note that this is considerably less than \\(w(\\bar{x})\\).\n\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(0,5.1), ylim=c(0,1))\nsegments(0,0,5,fx[1000], lwd=3, col=grey(0.75))\naxis(1, at=c(0.05,x[179],2.5,5), \n     labels=c(expression(x[0]), expression(x[C]),\n               expression(bar(x)), expression(x[1])),\n     tick=FALSE)\nmtext(\"Fitness (w(x))\", side=2,line=2, adj=0.65)\naxis(2, at=0.4882412, labels=\"\", tick=FALSE)\nsegments(2.5,0,2.5,0.4882412,lwd=3, lty=1, col=\"red\") # vertical line at bar(x)\nsegments(2.5,0.4882412,2.5,fx[501], lwd=3, lty=2, col=\"red\")\nlines(x,fx, lwd=3, col=\"black\")\n\n\n\n\n\n\n\n\nA risk-averse decision-maker should be willing to pay for certainty. We can show why this is graphically. Note that the expected fitness of this lottery (i.e., the average of the two possible outcomes) does not, in fact, fall on the fitness curve. We can move horizontally from this point back to the curve and the fitness would not change. If (and this is a big if) we can achieve certainty in our payoff by paying the difference between \\(E(w(x))\\) and what is called the certainty-equivalent return, we should.\n\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(0,5.1), ylim=c(0,1))\nsegments(0,0,5,fx[1000], lwd=3, col=grey(0.75))\naxis(1, at=c(0.05,x[179],2.5,5), \n     labels=c(expression(x[0]), expression(x[C]),\n              expression(bar(x)), expression(x[1])),\n     tick=FALSE)\nmtext(\"Fitness (w(x))\", side=2,line=2, adj=0.65)\naxis(2, at=0.4882412, labels=\"\", tick=FALSE)\nsegments(2.5,0,2.5,0.4882412,lwd=3, lty=1, col=\"red\") # vertical line at bar(x)\nsegments(2.5,0.4882412,x[179],0.4882412,lwd=3, lty=1, col=\"red\") # horizontal line back to utility curve\nsegments(x[179],0.4882412,x[179],0, lwd=3, lty=1, col=\"green\") # vertical line to x_c\nlines(x,fx, lwd=3, col=\"black\")\ntext(0.35, 0.54, expression(pi==bar(x) - x[C]))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#equilibria",
    "href": "interpreting.html#equilibria",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.5 Equilibria",
    "text": "2.5 Equilibria\nIn ecology, evolution, etc., we frequently plot two (or more) sets of rates. For example: birth and death rates in a demographic model or rates of colonization and extinction in a metapopulation model.\nFor example, the classic Levins model for metapopulations\n\\[\n\\dot{n} = cn(1-n) - en,\n\\]\nwhere \\(n\\) is patch occupancy, \\(c\\) is the colonization rate, and \\(e\\) is the extinction rate. The equilibrium for this happens when \\(\\dot{n}=0\\), which is\n\\[\n\\hat{n} = 1 - \\frac{e}{c}.\n\\]\nIf the extinction rate is greater than the colonization rate (\\(e&gt;c\\) ), then, sensibly, the overall population is extinct. Moreover, there will generally always be unoccupied patches at equilibrium.\nA classic example of a graphical representation of such an equilibrium process is the MacArthur-Wilson model, which is similar to the Levins metapopulation model in that it posits the number of species on an island is a dynamic balance between the colonization rate (which declines as a function of the number of resident species) and the extinction rate (which increases as a function of the number of resident species). The equilibrium occurs where the colonization rate just balances out the extinction rate, so that the overall rate of change of species is zero, the definition of an equilibrium.\n\nn &lt;- seq(0,20,,500)\nrate &lt;- 0.2\ncinit &lt;- 55\nplot(n, cinit*exp(-rate*n), type=\"l\", \n     lwd=3, col=\"#0D0887FF\",\n     xlab=\"Number of Species\", \n     ylab=\"Rate\",\n     ylim=c(0,60),\n     xlim=c(-3,23),\n     yaxs=\"i\",\n     axes=FALSE)\nlines(n, exp(rate*n), lwd=3, col=\"#9C179EFF\")\nsegments(log(cinit)/(2*rate),0,log(cinit)/(2*rate),exp(rate*log(cinit)/(2*rate)), lty=2)\naxis(1, at=c(log(cinit)/(2*rate)), labels = c(expression(hat(N))))\nbox()\n\n\n\n\n\n\n\n\n\n2.5.1 Equilibria in Discrete-Time\nRecursions.\nPoverty-trap model. We plot the wealth at time \\(t+1\\) agains the wealth at time \\(t\\). Use a Prelec weighting function to produce the characteristic S-shape of the poverty-trap model. An equilibrium occurs when the the wealth in the next time step is equal to the wealth in the current time step (i.e., there is no change). In this plot, this occurs wherever our curve touches the line of equality, \\(w_{t+1}=w_t\\).\nThe downside of the Prelec function is that we can’t easily solve for an equilibrium analytically, but we can solve it numerically using uniroot().\n\nprelec &lt;- function(p,a,b) (exp(-(-log(p))^a))^b\n## function to solve for interior equilibrium\nfn &lt;- function(p,a,b) (exp(-(-log(p))^a))^b - p\na &lt;- 2\nb &lt;- 1.7\n# we know p=0 and p=1 are solutions so limit to searching an interior interval\npint &lt;- uniroot(fn,interval=c(0.1,0.9),a=a,b=b)$root\np &lt;- seq(0,1,,1000)\nplot(p, prelec(p=p,a=a,b=b), type=\"l\", col=\"blue4\", lwd=2, \n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\", \n     xlab=expression(W[t]), ylab=expression(W[t+1]),\n     xlim=c(-0.05,1.05), ylim=c(-0.05,1.05))\nabline(a=0,b=1,lwd=1, col=grey(0.75))\npoints(c(0,pint,1),c(0,prelec(p=pint,a=a,b=b),1), pch=c(19,1,19), cex=1.5)\n\n\n\n\n\n\n\n\nThere are three equilibria for the poverty-trap model: (1) a stable equilibrium at destitution (\\(w_t=0\\)), (2) an unstable interior equilibrium, and (3) a stable equilibrium at maximum wealth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#indifference-curves",
    "href": "interpreting.html#indifference-curves",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.6 Indifference Curves",
    "text": "2.6 Indifference Curves\nWe encounter indifference curves when we consider the case of multi-species epidemics, as described by Holt et al. (2003). Suppose there is an infectious disease that can infect multiple species. In order to be above the epidemic threshold, there have to be a certain minimum number of susceptible individuals.\nOn one side of the curve – where the minimum conditions for an epidemic are exceeded – an epidemic is possible. On the other side of the curve, no epidemic is possible. Any combination of species numbers along the isoclines satisfy the conditions equally well. This is why we call them “indifference curves.”\nStart with the trivial case where the two species don’t interact at all. There will be an epidemic if there are either enough of species 1 or of species 2. The region where both species are below their respective thresholds lies inside the rectangular isocline\n\nx &lt;- seq(1,10,,500)\nx1 &lt;- seq(0,9,,100)\nplot(x,x, type=\"n\", axes=FALSE, frame=TRUE, xlab=\"Species 1\", ylab=\"Species 2\")\naxis(1,at=c(7), labels=c(expression(hat(S)[1])))\naxis(2,at=c(7), las=2, labels=c(expression(hat(S)[2])))\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     par(\"usr\")[2], par(\"usr\")[4],\n     col = grey(0.95))\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     7, 7, col=\"white\")\n#polygon(c(x1,rev(x1)), c(x1,rev(x1)), col=\"green\", border=FALSE)\nsegments(0,7,7,7, lwd=3, col=\"red\")\nsegments(7,0,7,7, lwd=3, col=\"red\")\ntext(5,5, expression(R[0]&lt;1))\ntext(8,8, expression(R[0]&gt;1))\n\n\n\n\n\n\n\n\nNow consider the slightly more interesting case where hosts of different species can substitute for each other. This means that even if the critical threshold for either of the species is reached, there can still be an epidemic. If the pathogen is not well adapted to a generalist-transmission mode, this effect might be quite small. We can call the epidemic isocline that arises from such conditions weakly-interacting.\n\ng &lt;- seq(0,sqrt(1/5),length=100)\nh &lt;- sqrt(1-(5*g^2))\nplot(g,h, type=\"n\", \n     axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     ylim=c(0,1.1), xlim=c(0,0.5),  \n     xlab=\"Spcies 1\", ylab=\"Species 2\")\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     par(\"usr\")[2], par(\"usr\")[4],\n     col = grey(0.95))\npolygon(c(g[1],g),c(0,h),col=\"white\", border=FALSE)\nlines(g,h,col=\"red\", lwd=3)\naxis(1,at=c(0.4485), labels=c(expression(hat(S)[1])))\naxis(2,at=c(1), las=2, labels=c(expression(hat(S)[2])))\ntext(0.37, 0.8, expression(R[0]&gt;1))\ntext(0.23,0.63, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nI’ve left the non-interacting isocline in this figure to show how, even though species are only interacting weakly, the space in which an epidemic is possible is greater.\nNow consider the case where substitutable.\n\nm &lt;- (0.8432192-0.1235539)/(1.230762-8.257112)\nb &lt;- 0.8432192-(-m*1.230762)\nxint &lt;- -b/m\n\nplot(x, 1/x, type=\"n\", lwd=3, col=\"red\", xlim=c(1.5,9), ylim=c(0.15,1/1.2), axes=FALSE,\n     xlab=\"Species 1\", ylab=\"Species 2\")\nbox()\npolygon(c(seq(1,10,length=100), seq(10,1,length=90)), c(m*seq(1,8.257112,length=100)+1.2*b, rep(8.257112,90)),\n        col=grey(0.95), border=\"red\", lwd=3)\n#segments(1.230762,0.8432192,8.257112,0.1235539, lwd=3, col=\"red\")\naxis(1,at=c(8.7), labels=c(expression(hat(S)[1])))\naxis(2,at=c(0.7423), las=2, labels=c(expression(hat(S)[2])))\ntext(6.5, 0.56, expression(R[0]&gt;1))\ntext(3,0.3, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nA perfectly substitutable curve is linear. This means if you can substitute one individual of species 2 for one individual of species 1 when species 1 is just below its critical threshold and still get an epidemic, you can substitute one for one at any point along the isocline. Now, the slope might not be unity. Maybe you have to substitute two of species 2 for one of species 1. The key is that ratio of substitution remains the same for any mixture of the two species.\nThings get more interesting when having a mixture of the two species makes it more likely that there will be an epidemic when there is a more even mixture of the two species than when the mixture is toward one of the extremes (i.e., mostly species 1 or mostly species 2). We call such an isocline complementary.\n\nplot(x, 1/x, type=\"l\", lwd=3, col=\"red\", xlim=c(1.5,9), ylim=c(0.15,1/1.2), axes=FALSE,\n     xlab=\"Species 1\", ylab=\"Species 2\")\nbox()\npolygon(c(seq(1,10,by=0.1), seq(9,1,by=-0.1)), c(1/seq(1,10,by=0.1), rep(10,81)), col=grey(0.95), border=\"red\")\naxis(1,at=c(8.12), labels=c(expression(hat(S)[1])))\naxis(2,at=c(0.8325), las=2, labels=c(expression(hat(S)[2])))\ntext( 5, 0.32, expression(R[0]&gt;1))\ntext(2.5, 0.24, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nThe convexity of this plot indicates the existence of diminishing marginal effectiveness of each species to maintain the epidemic. To see this, we can look at how the rate of substitution happens at different mixtures of the two species. For example, as you approach the extreme of \\(S_1=0\\), it takes increasingly more of \\(S_2\\) to stay above the epidemic threshold. This is obviously also true as we approach the \\(S_2=0\\) extreme as well, but we’ll focus on the \\(S_1=0\\) extreme here. In the middle of the range, a small change in one can be compensated by a small change in the other, making the epidemic threshold easier to achieve in the middle of the species’ population sizes.\n\nx &lt;- seq(0,12,,1000)\nplot(x, exp(-0.5*x), \n     type=\"n\", \n     axes=FALSE, frame=TRUE, \n     xaxs=\"i\", yaxs=\"i\", \n     xlab=\"Species 1\",\n     ylab=\"Species 2\", \n     xlim=c(2,12), \n     ylim=c(0.002,0.4))\nlines(x,exp(-0.5*x), lwd=3)\nsegments(2.1,exp(-0.5*2.1),2.1,exp(-0.5*2.6), lwd=2, col=\"red\")\nsegments(2.1,exp(-0.5*2.6),2.6,exp(-0.5*2.6), lwd=2, col=\"red\")\nsegments(5,exp(-0.5*5),5,exp(-0.5*5.5), lwd=2, col=\"red\")\nsegments(5,exp(-0.5*5.5),5.5,exp(-0.5*5.5), lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nFor vector-borne pathogens with complex life cycles, passage through an intermediate host is obligate for the perpetuation of the transmission cycle. Frequently, passage through the ultimate host is also obligate. Elimination of either intermediate or ultimate hosts from the community will lead to pathogen extinction. Thus, in the alternating case, a critical threshold exists for one or both species. As long as both host species co-exist above their minimum critical densities, the presence of a mix of both hosts makes pathogen persistence more efficient – this is why the isocline bends inward (i.e., is convex).\n\nx &lt;- seq(0,10,,100)\ny &lt;- 2/x\n\nplot(x,y,type=\"n\", axes=FALSE, frame=TRUE, xlab=\"Species 1\",\n     ylab=\"Species 2\", xlim=c(0,7), ylim=c(0,7))\npolygon(c(x,rev(x)), c(y,rep(8,100)), col=grey(0.95))\nlines(x,y, lwd=3, col=\"red\")\ntext(0.65, 0.61, expression(R[0]&lt;1))\ntext(2.7, 2.1, expression(R[0]&gt;1))\n\n\n\n\n\n\n\n\nSo far, all the interactions between species have made the epidemic more likely (or have been neutral). Sometimes, the presence of a second host species actually makes an outbreak less likely. For example, in zooprophylaxis, a dead-end host protects the host of interest (usually humans) by providing an alternate source of blood meals for biting arthopods such as mosquitoes, ticks, or triatomine bugs. This isocline for the inhibitory interaction has a positive slope: the presence of species 2 means you need more of species 1 to have an epidemic.\n\nx &lt;- seq(1,15,,100)\ny &lt;- -15 + 2*x\n\nx1 &lt;- seq(8,15,,100)\ny1 &lt;- -15 + 2*x1\n\nplot(1:15,1:15,type=\"n\", xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE, frame=TRUE, xlab=\"Species 1\", ylab=\"Species 2\")\naxis(1,at=c(8), labels=c(expression(hat(S)[1])))\n#polygon(c(8,8:15, col=grey(0.95)))\npolygon(c(x1[1],x1,x1[100]), c(1,y1,1), col=grey(0.95), border=FALSE)\ntext(12,4.2, expression(R[0]&gt;1))\ntext(7.5, 5.2, expression(R[0]&lt;1))\nlines(x,y, lwd=3, col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#contour-plots",
    "href": "interpreting.html#contour-plots",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.7 Contour Plots",
    "text": "2.7 Contour Plots\nA contour plot uses an idea very similar to that of an isocline. The most familiar example of a contour plot for most people is probably an elevation map, where the contour line joins points of equal elevation. It can take some practice to learn how to read a contour plot, but it is well worth the effort. Contour plots actually provide better information on spatial relationships in three dimensions than do false 3D plots, even if the latter can look cool.\nThe concept of resilience is central to human ecology and sustainability. A common visual representation of resilience depicts two adjacent basins in a plane. These basins represent attractors for the system. A ball moving along the plane can get pulled into either of them. The catch is that one of these basins represents a good attractor, while the other one is bad. One of these basins is deeper than the other and represents the good attractor. However, some slow-moving factor is forcing the system, making it more likely that it will end up in the bad attractor. As intuitive as the spatial metaphor for resilience may be, it turns out to be quite hard to represent it.\nOne way to represent this figure is using a contour plot. Here we will take advantage of the ggplot aesthetic geom_contour() to render our contours.\n\nrequire(mvtnorm)\nrequire(ggplot2)\n# Create grid\nx &lt;- seq(-4, 4, length.out = 100)\ny &lt;- seq(-4, 4, length.out = 100)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Define parameters for the two basins\nmu1 &lt;- c(-1, 0)  # Center of first basin\nmu2 &lt;- c(2, 0)   # Center of second basin\nsigma1 &lt;- matrix(c(0.5, 0, 0, 0.5), 2, 2)  # Deeper basin\nsigma2 &lt;- matrix(c(1, 0, 0, 0.7), 2, 2)  # Shallower basin\n\n# Calculate heights\nz1 &lt;- -dmvnorm(grid, mu1, sigma1)\nz2 &lt;- -0.7 * dmvnorm(grid, mu2, sigma2)\nz &lt;- matrix(z1 + z2, nrow = length(x))\n## for ggplot\ngrid$z &lt;- z1 + z2\n\n## ggplot topo rendering\nggplot(grid, aes(x = x, y = y)) +\n  geom_contour(aes(z = z), bins = 15, color = \"gray30\") +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\n\nThe left-hand basin is deeper, as indicated by the greater number of contour lines making it up. It’s also steeper, since these lines are closer together. A ball that gets pulled down into that basin should stay there. However, if some slow-moving force like climate change alters the structure of the landscape, a ball in that basin that is subjected to shocks might more easily jump out and land in the less desirable basin to the right.\nWe can also represent this surface using a false 3D image, along with the projected contours, using the plot3D library.\n\nlibrary(plot3D)\n\npersp3D(x, y, z,\n        theta = 90, phi = 30,\n        contour = TRUE,\n        shade = 0.5,\n        colkey = FALSE,\n        expand = 0.5,\n        colvar = z,\n        col = colorRampPalette(c(\"darkblue\", \"blue\", \"lightblue\", \"white\"))(100),\n        axes = FALSE,  # Remove axes\n        box = FALSE,   # Remove box\n        xlab = \"\", ylab = \"\", zlab = \"\") \n\n\n\n\n\n\n\n\nAnother nice example of a contour plot is a little less abstract. In Jones (2009), I introduced an idea called the human demographic space. Essentially, there is a range of values of total fertility rate (TFR), which is the number of live births that a woman would have if she survived to age 50, and life expectancy at birth (\\(e_0\\)), which is the average age of death in a population, that is compatible with the human life cycle. Different combinations of TFR and \\(e_0\\) imply different population growth rates. All things being equal, a population that has a higher TFR will grow faster. However, populations with high TFR often also have low \\(e_0\\), which will lower the growth rate.\nTo calculate the intrinsic rate of increase, \\(r\\), implied by a particular combination of TFR and \\(e_0\\), we use the following approximation:\n\\[\nr \\approx \\frac{\\log(TFR) + \\log(S(a))}{T},\n\\] where \\(S(a)\\) is the survivorship (i.e., the fraction of all ever-born individuals still alive) at age at first reproduction, and \\(T\\) is the mean age of childbearing, also known as the generation time of the population. This approximation comes from the demographic identity \\(R_0 = e^{rT}\\), where \\(R_0\\) is the net reproduction ratio, or the ratio of population size from one generation to the next. Livi-Bacci noted that the net reproduction ratio is well-approximated by the product of TFR and \\(S(a)\\). The rest is just algebra.\nNote that this relationship uses the the fraction surviving until age 20 and not life expectancy at birth. Fortunately, these two are very highly correlated. I use \\(S(a)\\) from the West model life tables of Coale and Demeney, which are indexed by \\(e_0\\). Life tables (i.e., survivorship curves) are given for different levels of life expectancy at birth and we can simply read off the value of \\(S(a)\\) associated with a given value of \\(e_0\\).\n\n# S(20) from CDMLT West\nSa &lt;- c(0.3562541, 0.3969429, 0.4358999, 0.4732345, 0.5090507,\n        0.5434457, 0.5765105, 0.6083286, 0.6389776, 0.6685286, 0.697048,\n        0.724596, 0.7543878, 0.7817066, 0.805704, 0.8290955, 0.8518434,\n        0.8739262, 0.8953284, 0.9160435, 0.935412, 0.952371, 0.9676141,\n        0.980381, 0.9899677)\ntfr &lt;- seq(1,8, by=0.5)\nT &lt;- 27.5 # human generation time\n\n# calculate r from TFR, l(20), and generation time (T)\nf &lt;- function(r,Sa,TFR,T) y &lt;- -r + (log(TFR) + log(Sa))/T\n\ncalcr &lt;- function(S20,tfr, twosex=TRUE){\n  rr &lt;- matrix(0,nr=length(S20),nc=length(tfr))\n  if(twosex) tfr &lt;- tfr/2\n  \n  for(i in 1:length(S20)){\n    for(j in 1:length(tfr)){\n      rr[i,j] &lt;- uniroot(f, c(-0.2,0.1), Sa=S20[i], TFR=tfr[j], T=T)$root\n    }\n  }\n  \n  rr\n}\n\n## calculate the implied growth rates\nrr &lt;- calcr(S20=Sa,tfr=tfr)\n\n# e(0) from CDMLT West\ne0 &lt;- c(18.03431, 20.44308, 22.85202, 25.26012, 27.66740, 30.07391,\n        32.47963, 34.88452, 37.28851, 39.69149, 42.09338, 44.49401, 47.08864,\n        49.54198, 51.79709, 54.08686, 56.40571, 58.75305, 61.1244, 63.51476,\n        65.85406, 68.33622, 70.91915, 73.5699, 76.27995)\n\n## empirical TFR/e0 pairs that span the space\n# data points\nusa &lt;- c(2.05,77)\nven &lt;- c(6.5,65)\nache &lt;- c(8,37.5)\nkung &lt;- c(4,34)\ntaiwan &lt;- c(7.35,29)\n\n\ncontour(tfr,e0,t(rr), \n        lwd=2, col=\"blue4\", \n        xlim=c(2,8), \n        xlab=\"Total Fertility Rate\", \n        ylab=\"Life Expectancy\")\n\ntext(usa[1],usa[2],\"USA\", col=\"red4\", bg=\"red4\")\ntext(ven[1],ven[2], \"Venezuela\", col=\"red4\", bg=\"red4\")\ntext(ache[1],ache[2], \"Ache\", col=\"red4\", bg=\"red4\")\ntext(kung[1],kung[2], \"!Kung\", col=\"red4\", bg=\"red4\")\n\n\n\n\n\n\n\n\nThe contours represent a surface that increases monotonically as it goes up and to the right (i.e., as \\(e_0\\) and TFR increase). The four populations largely span the space and each one is above the \\(r=0\\) contour (though the USA is just barely above). Venezuela (in 1967) was characterized by very high life expectancy and incredibly high fertility, giving it an astounding growth rate of nearly \\(r=0.04\\). This is likely the fastest a sizable human population has ever grown intrinsically. That translates into a doubling every 17 years! See Jones (2009) for more details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#plotting-tricks",
    "href": "interpreting.html#plotting-tricks",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.8 Plotting Tricks",
    "text": "2.8 Plotting Tricks\nIn chapter Chapter 3, I will introduce how to produce scientific figures in R. However, I’ve repeatedly done some things in these notes that merit a brief explanation. Otherwise, there is a risk of things seeming obscure and generally confusing.\nTheoretical plots usually don’t depend on specific values of inputs or functions – you typically care just about the shapes and not the specific values. You are trying to show the general behavior of your system. R is a statistical programming language and, as such, expects you to be plotting data. Presumably, you care about the actual values when data are involved. For our theoretical plots, we usually want to suppress the values on the plot’s axes. This is why nearly all of these figures include the arguments to the plot() command axes=FALSE and frame=TRUE. This suppresses the axes and any ticks and labels indicating specific values on them. We can then add in custom axis labels, such as the critical population size for each species in the multi-species epidemic isoclines using the command axis().\nPerhaps a more mysterious trick I use is to include the arguments xaxs=\"i\" and yaxs=\"i\". This is really the special sauce of a scientific-theory plot in R. Again, R expects data when you call the plot() command. A good aesthetic practice for data plots is to pad the range of the observed data and R does this by default. By forcing the style of the axes to be “internal” (that’s what the “i” stands for), you restrict the axes to the range of your data. This means that \\(y\\)-intercepts actually intercept the \\(y\\) axis, curves that should start at zero actually look like they’re starting at zero, etc.\nWe often want to lay out the axes but not draw a curve quite yet. To do this, we add the argument type=\"n\" to the plot() command. This allows us to build up complex figures with more precision and control. You might notice that we often plot the actual curve we care about last. This is because we want it on top of the various lines we’ve added to indicate interesting bits of the curve (e.g., equilibria and such).\n\n2.8.1 locator\nSometimes you need to find a spot on your figure where you want to add text or draw a segment or an arrow. R has a very handy function that allows you to interactively determine the coordinates of a point on your axes. Use the function locator(n) with a plot rendered in the RStudio window. You can then click your mouse n times on the plot and will get returned a list with the n (x,y)-coordinate pairs. There are a couple figures in chapter Chapter 3 where I use locator() to find coordinates for drawing points or text. This interactive usage is hard to translate into static notes, so I have to hard-code the coordinates in this document.\n\n\n2.8.2 Colors\nYou may have noticed that I use several different ways to specify colors throughout these notes. R has a number of colors built into its base. You can see them all by typing colors() at the prompt. I’m not going to do that here, because there are more than 650 of them and it would be a mess. But we can get a hint.\n\nwhat_are_the_colors &lt;- colors()\nlength(what_are_the_colors)\n\n[1] 657\n\n# the first 10\nwhat_are_the_colors[1:10]\n\n [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n [5] \"antiquewhite2\" \"antiquewhite3\" \"antiquewhite4\" \"aquamarine\"   \n [9] \"aquamarine1\"   \"aquamarine2\"  \n\n\nSometimes I specify colors based on their hexadecimal codes such as #0D0887FF or #9C179EFF. These start with a hash (#). What follows the hash is actually a triplet of codes specifying red, green, and blue components of the color using hexidecimal (i.e., base 16) codes, which go from 00 to FF. I can almost hear your cries as I write this: “but those codes have eight digits following the hash, not six!” Sometimes a hex color code has a fourth byte (as these examples do), which specifies the alpha channel (i.e., degree of transparency) of the color. Note that for the two codes I’ve used here, this last byte is FF, which is the maximum value, so not at all transparent.\nYou can find various web tools for picking colors and generating hex color codes. There is a lot to be said for using hex codes because the code #F0F8FF is a lot less ambiguous than aliceblue.\nIf hexidecimal codes are not your jam, you can also specify a color using RGB (red, green, blue) codes. Once again, an optional fourth value specifies the alpha channel. The one thing that’s tricky about rgb() is that you need to specify what’s known as a radix or what the maximum value is. By convention, this is typically 255 (because of course it is). You could also use a radix of 1 or really anything you like. However, conventions make coding more legible for a wider audience (that may include your future self!). Note that the radix for the hex codes is 16 (or FF is you want to be cute).\nHere we can recreate a figure from earlier, using rgb(), rather than built-in colors, to specify the line colors.\n\ncurve(5*exp(x), 0, 10, lwd=3, col=rgb(255,0,0,255, maxColorValue = 255), xlab=\"x\", ylab=\"y\")\ncurve(1*exp(x), 0, 10, lwd=3, col=rgb(0,0,0,255, maxColorValue = 255), add=TRUE)\n\n\n\n\n\n\n\n\nAnother way to specify colors is by using a color palette. Base R has a few built-in palettes, including rainbow, heat, topo.colors, and terrain.colors. To be honest, these aren’t really the best. We used a color palette above when we compared the curves of linear, quadratic, exponential, and logarithmic functions using the viridisLite package. In that plot, we used a classic palette called plasma. It’s interesting to see what these palette functions actually produce:\n\nrequire(viridisLite)\nplasma(3)\n\n[1] \"#0D0887FF\" \"#CC4678FF\" \"#F0F921FF\"\n\n\nA good color palette is designed to do several things. It should make attractive figures. Basically, you want the colors to be cool and, well, colorful. But you also want them to be perceptually uniform, meaning that values that are close to each other have colors that are close to each other throughout the range of the palette. Ideally, you also want your color choices to be robust for people with color blindness.\nMy personal favorite color-palette package is MetBrewer, which translates the color schemes of various masterworks from the Metropolitan Museum of Art. You can check out the various palettes and the works that inspired them on the MetBrewer GitHub.\n\nlibrary(MetBrewer)\nmet.brewer(\"Johnson\",7)\n\n\n\n\n\n\n\n\nAnd just to remind ourselves what we’re actually doing when we create a palette:\n\ndope_colors &lt;- met.brewer(\"Johnson\",7)\nas.character(dope_colors)\n\n[1] \"#A00E00\" \"#C03800\" \"#DC7400\" \"#F6C200\" \"#529A6F\" \"#066793\" \"#132B69\"\n\n\nNote that these are in the six-digit form (no alpha channel).\n\n\n\n\nGadgil, Madhav, and William H. Bossert. 1970. “Life Historical Consequences of Natural Selection.” The American Naturalist 104 (935): 1–24. http://www.jstor.org/stable/2459070.\n\n\nHolt, R. D., A. P. Dobson, M. Begon, R. G. Bowers, and E. M. Schauber. 2003. “Parasite Establishment in Host Communities.” Ecology Letters 6 (9): 837–42. https://doi.org/10.1046/j.1461-0248.2003.00501.x.\n\n\nJones, J. H. 2009. “The Force of Selection on the Human Life Cycle.” Evolution and Human Behavior 30 (5): 305–14. https://doi.org/10.1016/j.evolhumbehav.2009.01.005.\n\n\nNair, Jayakrishnan, Adam Wierman, and Bert Zwart. 2022. The Fundamentals of Heavy Tails: Properties, Emergence, and Estimation. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge: Cambridge University Press.\n\n\nWeitzman, M. L. 2009. “On Modeling and Interpreting the Economics of Catastrophic Climate Change.” The Review of Economics and Statistics XCI (1): 1–19. https://doi.org/10.1162/rest.91.1.1 .",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "drawing.html#introduction",
    "href": "drawing.html#introduction",
    "title": "2  Theoretical Scientific Figures in R",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nR has powerful graphics capabilities. While we typically use these for plotting data, we can also make publication-quality plots for elucidating theoretical topics as well.\nThese notes are a very tentative start to a much larger body of work. I hope they are nonetheless helpful in their rather incomplete form."
  },
  {
    "objectID": "drawing.html#the-taylor-series-approximation-is-your-friend",
    "href": "drawing.html#the-taylor-series-approximation-is-your-friend",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.2 The Taylor-Series Approximation is Your Friend",
    "text": "3.2 The Taylor-Series Approximation is Your Friend\nYou may be familiar with Taylor polynomials (or series) and how useful they are for applied mathematics and science. A Taylor series allows you to approximate a function \\(f(x)\\) in terms of an infinite sum of its derivatives taken at a single point \\(a\\):\n\\[ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} + \\cdots\n\\]\nWhen we truncate the Taylor series at the first term, we produce a linear approximation of our function. In other words, it’s a tangent line. It turns out that we often want to draw tangents in theoretical figures and just as the Taylor series can help us derive theory, so too can it help us generate plots!\n\n3.2.1 Cobb-Douglas Production Function\nThe Cobb-Douglas production function is a model for production that is the product of two power functions. The classic model combines capital (\\(K\\)) and labor \\(L\\). The inputs are raised to powers \\(\\alpha\\) and \\(\\beta\\).\n\\[ W = K^{\\alpha} L^{\\beta}\n\\]\nIn the Cobb-Douglas form, the exponents also turn out to be the elasticities of production with respect to the inputs. So, suppose that \\(\\alpha=0.25\\), this means that a 1% increase in the capital will increase overall wealth by 0.25%.\nIf \\(\\alpha + \\beta =1\\), then there are constant returns to scale: doubling inputs will double the output. If, on the other hand, \\(\\alpha + \\beta &gt; 1\\), there are increasing returns to scale so that doubling inputs will more than double the output.\nThe optimal balance between capital and labor occurs when a budget line is tangent to this curve.\n\nx &lt;- seq(0,0.8,length=100)\nL &lt;- seq(0,1,length=100)\nK &lt;- (1/L^0.7)^(1/0.3)\n#derivative\nfprime &lt;- -5.6/(0.3^2.4)\n# make a function to calculate K from L\nkf &lt;- function(L) (2/L^0.7)^(1/0.5)\n# pick an arbitrary point for tangency\na &lt;- 0.3\n\n\nplot(L,kf(L), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     xlim=c(0,1), ylim=c(0,50), \n     col=grey(0.65), \n     xlab=\"Labor\", ylab=\"Capital\")\n# tangent\nlines(x, kf(a)+fprime*(x-a), lwd=1, col=\"black\")\n# values on the axes\nsegments(0.3,0,0.3,kf(0.3), col=grey(0.65), lty=3)\nsegments(-0.1,kf(0.3),0.3,kf(0.3), col=grey(0.65), lty=3)\nmtext(expression(hat(L)),1,at=0.3, padj=0.5)\nmtext(expression(hat(K)),2, at=kf(0.3), adj=2, padj=0.5, las=2)\n\n\n\n\n\n\n\n\nGraphics Tip: Note that in labeling the optimum value of capital, we used the argument las=2. This says to print text always perpendicular to the axis and is useful for labeling interesting points on particularly the vertical axis.\nYou will notice that in most of these plots, we suppress the axis labeling. For a theoretical plot, we want to see the shape of the relationships between variables and care less about the specific \\((x,y)\\) values. This means that we typically include in the plot() command the argument axes=FALSE. When we turn off the axes, this also, by default, removes the frame around the axes. We usually want that, so we have to also include the argument frame=TRUE.\nWe can add mathematical, typset material to any text (e.g., axis labels, labels for equilibria and other interesting points, titles, etc.) using expression(). There is a stripped-down markup language for this in R. To learn more, do a help search for plotmath.\n\n\n3.2.2 Marginal Value Theorem\nSomething like the Marginal Value Theorem (MVT), a phrase coined by Charnov (1976), appears in many applications: foraging theory (Charnov 1976), sexual selection (Parker and Stuart 1976), the evolution of virulence (Baalen and Sabelis 1995), and of course, life history theory (Smith and Fretwell 1974). If you pay careful attention, you will notice that it’s always the maximization of some sort of ratio, where the numerator and denominator trade-off. In this case, the MVT solution arises naturally from the quotient rule for differtiation. The MVT states that the optimal value of the ratio fitness measure can be found when a straight line rooted at the origin is tangent to the fitness/constraint function. This the eponymous “marginal value.”\nIn previous examples, we specified the tangent point. Here we solve for the optimal value, which for the marginal value theorem, says that the rate is maximized where a line rooted at the origin is tangent to the utility function. We can mess around with different slopes and try to find something that’s approximately right or we can find the actual solution using the root-finding function in R. I need to acknowledge Mike Price here because he helped me get unstuck as I flailed to get uniroot() to work. I should also note that when I publish notes like this, even if I accompany them with caveats about incompleteness, you are seeing the product of lots of trial and error (lots of error, I assure you). Remember, the struggle is part of the scientific process.\nAnyway, code for the marginal value theorem (in its many guises).\n\nx &lt;- seq(0,20,length=500)\n# utility function fp&gt; 0 fpp &lt; 0\n# turns out that RMarkdown does not handle comments with single quotes \n# fp == deriv of f; fpp == 2nd deriv of f\nf &lt;- function(x) {\n  1 - exp(-0.2*(x-1))\n}\n# derivative of the utility function\nfprime &lt;- function(x) {\n  0.2*exp(-0.2*x)*exp(0.2)\n}\n\n# f + fp*(z-x) = 0\n# z = x -(f/fp)\n# solve for tangency; find the root of this\nxinter &lt;- function(x) {\n  return(x - f(x)/fprime(x))\n}\n\nsoln &lt;- uniroot(xinter,c(0,10))\n\nplot(x,f(x), type=\"l\", lwd=2, xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Time\",\n     ylab=\"Rate of Gain\",\n     ylim=c(0,1))\nlines(x,(f(soln$root)/soln$root)*x,col=\"red\")\nsegments(soln$root,0,soln$root,f(soln$root), lty=2, col=\"red\")\nsegments(0,f(soln$root),soln$root,f(soln$root), lty=2, col=\"red\")\nmtext(expression(hat(t)),1,at=soln$root, padj=1)\n## some non-optimal adaptive functions\nlines(x,(f(2)/2)*x,col=grey(0.85))\nlines(x,(f(1.5)/1.5)*x,col=grey(0.85))\nlines(x,(f(11)/11)*x,col=grey(0.85))\n\n\n\n\n\n\n\n\nMarginal value theorem plot. A line rooted at the origin that is tangent to the gain curve provides the optimal patch-residence time, \\(\\hat{t}\\).\nGraphics Tip: In this case, we had to find the point where a line rooted at the orgin is tangent to the gain curve. We found this using uniroot() which is a one-dimensional optimization routine that searches an interval for the zero of a function. The function returns an list with at least four elements. The one we want is called root, hence the use of soln$root in plotting arguments in the above code.\n\n\n3.2.3 Optimal Age at First Reproduction\nSimple Example of the optimal trade-off between adult reproductive value (\\(E\\)) and juvenile recruitment (\\(S\\)). Charnov (1997) has suggested that fitness is a product, broadly construed, of three things: juvenile recruitment, annual fertility of adults, and adult life expectancy. In turn, these elements can be combined. For example, the product of annual fertility and adult life expectancy can be thought of as adult reproductive value because it is the expected total reproduction over the an individual’s lifespan, conditional on them being recruited into the breeding population. As Charnov notes, these things are likely to trade-off. Moreover, the multiplicative form of fitness makes these trade-offs particularly straightforward to visualize and analyze.\nWhen we plot the allowable combinations of \\(\\log(S)\\) and \\(\\log(E)\\), we get a convex plot of the iso-fitness plot linking the logs of \\(E\\) and \\(S\\), which indicates diminishing marginal returns in both dimensions. The optimal life history is the one for which a line with a slope of -1 is tangent to this iso-fitness constraint curve. Why? The fitness measure (assuming population stationarity) is \\(R_0 = S\\, E\\). Take logs such that \\(\\log(R_0) = \\log E + \\log S\\) and differentiate with respect to age at first reproduction (\\(\\alpha\\); which, in Charnov’s formalism, is the control parameter for the life history):\n\\[\n\\frac{d \\log R_0}{d\\alpha} = \\frac{d \\log E}{d\\alpha} + \\frac{d \\log S}{d\\alpha}.\n\\]\nSet this equal to zero and rearrange. Divide \\(d\\log E/d\\log \\alpha\\) by \\(d\\log S/d\\log \\alpha\\) and we find the optimality criterion:\n\\[\n\\frac{d\\log E}{d \\log S} = -1.\n\\]\n\ng &lt;- seq(0,sqrt(1/5),length=200)\nh &lt;- sqrt(1-(5*g^2))\nhf &lt;- function(g) sqrt(1-(5*g^2))\n## derivative\nfp &lt;- function(g) -5*g/sqrt(1-5*g^2)\n\n# solve for tangency; find the root of this\n# note the sign change\nginter &lt;- function(g) {\n  return(g + hf(g)/fp(g))\n}\n\n## do not search over whole interval \n## because g values &gt; sqrt(5) will give NaNs!\na &lt;- uniroot(ginter,c(0,0.4))$root\n## this simply extends the plotting range \n## so that the tangent line fills the plotting range\ngg &lt;- seq(0,0.5+a,length=500)\n\nplot(g,hf(g), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     ylim=c(0,1.1), xlim=c(0,0.5), \n     xlab=\"log(E)\", \n     ylab=\"log(S)\")\n## first-order Taylor Series approx\nlines(gg, hf(a)+fp(a)*(gg-a), col=\"red\")\nsegments(a,0,a,hf(a), col=grey(0.65), lty=3)\nsegments(-0.1,hf(a),a,hf(a), col=grey(0.65), lty=3)\nmtext(expression(hat(E)),1,at=a, padj=0.5)\nmtext(expression(hat(S)),2, at=hf(a), adj=2, padj=0.5, las=2)\n\n\n\n\n\n\n\n\n\n\n3.2.4 Fitness Sets\nRichard Levins (1962) introduced a the idea of fitness sets as a way to think about evolution in variable environments. This approach was more fully fleshed out in his subsequent monograph (R. Levins 1968). The fundamental idea is to represent the fitness of organisms in the different conditions that make up their variable environments and then find the strategy that maximizes fitness across these environments. The optimum can be a generalized compromise across the different environments or it can be the production of polymorphic specialized phenotypes that better match specific environmental conditions.\nConsider first a population with two phenotypes where the peak fitness in the two environments are quite separated from each other such that the fitness functions do not overlap tremendously. It’s conventional to assume Gaussian distributions of fitnesses with respect to the environment for simplicity, but to make things more interesting, we can use Gamma distributions, which will have more right-skew. The basic idea behind using these peaked functions is that there is an optimum for the environment and that fitness falls off as you move away from this optimum value of the phenotype. A Gaussian distribution just makes quite specific assumptions about how fitness falls off as the phenotype differs from the optimum: it does so symmetrically around the maximum and it declines exponentially in the squared difference from the optimum, while the Gamma distributions will be asymmetric in the way fitness falls off from the peak. The actual form of the fitness function will depend on the particulars of the environment and the phenotypes in question.\n\n## skewed gamma distributions\nx &lt;- seq(0,25,,1000)\nk1 &lt;- 9\ns1 &lt;- 0.5\nk2 &lt;- 7.5\ns2 &lt;- 1\n\nplot(x,dgamma(x,shape=k1,scale=s1), type=\"l\", lwd=2,\n     axes = FALSE, frame=TRUE,\n     xlab=\"Phenotype\", ylab=\"Fitness\")\nlines(x,dgamma(x,shape=k2,scale=s2))\n\n\n\n\n\n\n\n\nIn this figure, we plotted the fitness functions against the environment. We can cut out the middleman, as it were, and simply plot the fitness functions against each other in a manner analogous to phase-plane analysis of, e.g., the Lotka-Volterra predator-prey model. Environment becomes implicit in the plots. What we have done is represent all possible phenotypes in our 2-dimensional fitness space.\nA quick note on convexity is probably warranted here. A space is said to be convex if, for any two points contained within the space, the entirety of the line segment that connects these points is also contained within the space. It’s easy to see that a line segment connecting points in horns of this fitness set would not be entirely contained within the set.\nOur two distributions overlap quite a bit and we will see that they form a convex fitness set. This suggests the geometrical interpretation of convexity, namely, that it implies the ability of a compromise phenotype. To find the optimal (compromise) phenotype, we add the adaptive function for a coarse-grained environment. For a coarse-grained environment, the adaptive function will have a hyperbolic form. Here again, the issue of convexity arises. An adaptive function that takes the hyperbolic form as in figure 3, is also said to be convex. Just as a convex fitness set implies an an optimum phenotype that is a compromise, convexity in the adaptive function suggests that average values have higher fitness than extremes. As the adaptive-function isoclines move from the center to the extremes, the increase in fitness in one dimension must be greater than the reduction of fitness in the other dimension. This is also related to diminishing marginal rate of substitution. Note, for example, as the isocline moves away from its convex center upward in the direction of \\(W_2\\), it takes increasing fitness in the \\(W_2\\) dimension to make up for lost fitness in the \\(W_1\\) direction.\n\n## for the isoclines\nG &lt;- seq(0,0.5,length=100)\nalpha &lt;- 0.5\nbeta &lt;- 0.5\n# simple function to calculate hyperbolic isolclines following Cobb-Douglas form\nkf &lt;- function(G,W,alpha,beta) (W/G^alpha)^(1/beta)\n\n## fitness set\nplot(dgamma(x,shape=k1,scale=s1),dgamma(x,shape=k2,scale=s2), lwd=3,\n     type=\"l\", axes = FALSE, las=1,\n     xlim=c(0,0.4), ylim=c(0,0.25),\n     xlab=expression(W[1]), ylab=expression(W[2]))\nbox()\n## convex adaptive functions\nlines(G,kf(G=G,W=0.05,alpha=0.75,beta=1), lty=2)\nlines(G,kf(G=G,W=0.05,alpha=0.85,beta=1), lty=2)\nlines(G,kf(G=G,W=0.05,alpha=0.63,beta=1), lty=2)\n\n\n\n\n\n\n\n\n\n\n3.2.5 Graphical Newton-Raphson Method\nAnother cool application of tangent lines in plots illustrates the popular and powerful family of optimization algorithms are broadly known as “Newton’s Method’ or the”Newton-Raphson Algorithm.” This algorithm finds roots of functions – that is, points where the function is zero. The basic idea is that we start from an initial guess point on our function. We then draw a line tangent to our function at this point. Finding the \\(x\\)-intercept for the tangent line, we repeat the process only this time drawing our tangent line from the point on the curve corresponding to the \\(x\\)-intercept of our last tangent line. It turns out that, for some initial guess, \\(x_0\\), the value \\(x_1 = x_0 - f(x_0)/f'(x_0)\\) is a better estimate of the root. We can then repeat this process until we are satisfactorily close to the root of the function. We can make a quick graphical demonstration Newton’s method for a very simple function \\(y=x^2-9\\). We start with a guess at \\(x=8\\). The plot shows three iterations (numbered sequentially). We can see that each iteration gets much closer to the root of this equation (at \\(x=3\\)). In fact, it gets so close after three steps that plotting another iteration is indistinguishable from the correct solution (though for a realistic tolerance, it would take a couple more steps to get right to \\(x=3\\)).\n\n## graphical newton-method\n\nx &lt;- seq(0,10, length=100)\ny &lt;- function(x) x^2 - 9\nx1 &lt;- function(a) a-y(a)/(2*a)\n\n# first iteration\na &lt;- 8\nplot(x,y(x),type=\"l\",lwd=3)\nabline(h=0)\npoints(8,55, pch=19, cex=1.5)\ntext(8,59,\"1\")\nlines(x, y(a) + (x-a)*2*a, col=\"red\")\n# second iteration\npoints(x1(a),y(x1(a)), pch=19, cex=1.5)\ntext(x1(a),y(x1(a))+4,\"2\")\na &lt;- x1(a)\nlines(x, y(a) + (x-a)*2*a, col=\"red\")        \n# third iteration\npoints(x1(a),y(x1(a)), pch=19,cex=1.5)\ntext(x1(a),y(x1(a))+4,\"3\")\na &lt;- x1(a)\nlines(x, y(a) + (x-a)*2*a, col=\"red\")  \n\n\n\n\n\n\n\n\nIf we were really feeling ambitious, we could animate this!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#using-expression-to-draw-functional-response-curves",
    "href": "drawing.html#using-expression-to-draw-functional-response-curves",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.3 Using expression() to Draw Functional-Response Curves",
    "text": "3.3 Using expression() to Draw Functional-Response Curves\nThere are several different ways that you can draw a theoretical curve. In the last chapter, I used curve() to draw many of the figures. I find that I have better control over the overall figure, however, if I can access the equation and its evaluation more directly. The quick-and-dirty way to do this is to write an expression(). An expression is an object whose evaluation is delayed until explicitly called for with the function eval(). So you can define your equation of theoretical interest, then enter its inputs, and then evaluate it when it is convenient for you (e.g., when you’re plotting).\nWe can demonstrate this functionality by comparing the Holling family of functional-response curves. These curves model, among other things, the satiation of a predator as prey density increases.\nThe most commonly-used of these functional responses is certainly Holling Type II, which is also known (e.g., in physiology) as the Michaelis–Menten function, which is a simple, if ubiquitous, model of enzyme kinetics.\n\nx &lt;- seq(0,10, length=200)\na &lt;- 0.7\nb &lt;- 1.5\nc &lt;- -1.5\nh2 &lt;- expression(a*x/(1 + a*x))\nh3 &lt;- expression(a*x^2/(b^2+x^2))\nh4 &lt;- expression(a*x^2/(b+c*x+x^2))\n#\nplot(x, eval(h4), type=\"l\", lwd=3, col=\"magenta\",  xaxs=\"i\", yaxs=\"i\", axes=FALSE, xlab=\"Abundance\", ylab=\"Response\", ylim=c(0,1.2))\nlines(x,eval(h3), lwd=3, col=\"cyan\")\nlines(x, eval(h2), lwd=3, col=\"black\")\nlegend(\"topright\", c(\"Type II\", \"Type III\", \"Type IV\"), col=c(\"black\",\"cyan\",\"magenta\"), lwd=3)\nbox()\n\n\n\n\n\n\n\n\nGraphics Tip: We add a legend to a figure, not surprisingly, with the function legend(). The first argument to legend() is its location. You can specify this with x,y-coordinates or, more simply, with a keyword from the list that includes: “bottomright”, “bottom”, “bottomleft”, “left”, “topleft”, “top”, “topright”, “right” and “center”. Sometimes you need to fiddle around with the location of the legend as it might appear quite different under different graphics devices (e.g., in the Plots window of RStudio vs. a .png file vs. a .pdf file).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#fold-catastrophe-model",
    "href": "drawing.html#fold-catastrophe-model",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.4 Fold-Catastrophe Model",
    "text": "3.4 Fold-Catastrophe Model\nA catastrophe is a sudden shift in system state (Zeeman 1976). An interesting form of catastrophe, which Scheffer (2009) discusses in detail, is the fold catastrophe.\nThis is a pretty complicated figure. The solid parts of the curve are stable – when the system state is perturbed when in the vicinity of this part of the attractor, it tends to return, as indicated by the grey arrows pointing back to the attractor. The dashed part of the attractor is unstable – perturbations in this neighborhood tend to move away from the attractor. This graphical representation of the system makes it pretty easy to see how a small perturbation could dramatically change the system if the current combination of conditions and system state place the system on the attractor near the neighborhood where the attractor changes from stable to unstable. The figure illustrates one such scenario. The conditions/system state start at point \\(F1\\). A small forcing perturbs the system off this point across the bifurcation. Further forcing now moves the system way off the current state to some new, far away, stable state. We go from a very high value of the system state to a very low value with only a very small change in conditions. Indeed, in this figure, the conditions remain constant from point \\(F1\\) to the new value indicated by the white point – just a brief perturbation was sufficient to cause the drastic change.\n\nx &lt;- seq(-12,12,length=10000)\ny &lt;- seq(12,10/sqrt(3), length=1000)\n## fold-catastrophe is a cubic\nplot(-x^3+100*x,x,type=\"l\", axes=FALSE, lwd=2, lty=2, \n     xlab=\"Conditions\", ylab=\"System State\")\nbox()\nlines(-y^3+100*y,y, lwd=2)\nlines(y^3-100*y,-y, lwd=2)\n\n# unstable\narrows(-200,-5,-200,-2.75, code=1, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,-1.5,-200, 0.75, code=2, lwd=3, length=0.1, col=grey(0.75))\n#lower stable\narrows(-200,-6,-200, -8.25, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,-11.5,-200, -9.25, code=2, lwd=3, length=0.1, col=grey(0.75))\n#upper stable\narrows(-200,13.5,-200, 11.25, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,8.25,-200, 10.5, code=2, lwd=3, length=0.1, col=grey(0.75))\n# use locator() to find coordinates\npoints(357,7, pch=21, cex=2, lwd=3, bg=grey(0.75))\ntext(376.6749, 7.87279, \"F1\")\npoints(357,-11.452987, pch=21, cex=2, bg=\"white\")\narrows(357,6.5,357,4, lwd=3, length=0.1)\narrows(357,3.5,357,-10.8, code=2, lwd=3, length=0.1, col=grey(0.75))\n\n\n\n\n\n\n\n\nGraphics Tip: For the fold catastrophe, we want the upper and lower arms of the curve to be solid lines, indicating a that the attractor lies in a basin of attraction in these regions, and a dashed line in the middle, indicating that the attractor is unstable there. To do this we plot the whole curve as a dashed line lty=2 and then plot solid lines over this curve in the regions we want it to be solid. Lots of trial-and-error in making such a plot!\n\n3.4.1 Mechanistic Foundation of Fold-Catastrophe\nThe fold-castastrophe may seem like an incredibly specific model. It turns out there are various very natural ways of constructing such an attractor. Here, we discuss the approach of Noy-Meir (1975) for a resource-exploitation case.\n\n# Logistic Recruitment\nlogistic.recruit &lt;- expression(r*N*(1 - (N/K)^theta))\nno &lt;- 1\nr &lt;- 0.45\nK &lt;- 100\ntheta &lt;- 1\nN &lt;- seq(0,K,length=500)\n\n# Holling Type II Functional Response\nh2 &lt;- expression(a*N/(b + a*x))\nx &lt;- N+1\na &lt;- 0.7\nb &lt;- 3\nplot(N,eval(logistic.recruit), type=\"l\", yaxs=\"i\", lwd=3, axes=FALSE, xlab=\"Relative Producer Density\", ylab=\"Relative Productivity\", ylim=c(0,15))\nbox()\nlines(x, 6*eval(h2), lwd=2)\nlines(x, 12*eval(h2), lwd=2)\n\npoints(c(8.702413, 36.521362, 57.497146, 85.464859), c(3.485678, 10.441517, 11.003335,  5.652690), col=c(\"red\",\"red\",\"green\",\"green\"), cex=2, pch=16)\narrows(36.521362-7.5, 10.441517+2, 36.521362-1, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\narrows(36.521362+1, 10.441517+2, 36.521362+7.5, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\n#\narrows(57.497146-7.5, 10.441517+2, 57.497146-1, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(57.497146+1, 10.441517+2, 57.497146+7.5, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\ntext(c(59.13355, 86.65497), c(11.458140,  6.107494), c(\"F2\", \"F1\"))\n\n\n\n\n\n\n\n\nThere are three fixed points where the recruitment curve and the extraction curve intersect. The green points are stable fixed points, whereas the red points are unstable. The third set of points is near zero and I’ve not drawn those just to keep the plot less cluttered. This fixed point is also stable.\nIf we imagine keeping the recruitment curve constant but sweeping extraction curves continuously up through the space (as we have for one big jump in the plot from \\(F1\\) to \\(F2\\)) and tracked the three fixed points along this sweeping, we would have a fold catastrophe.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#cobwebbing",
    "href": "drawing.html#cobwebbing",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.5 Cobwebbing",
    "text": "3.5 Cobwebbing\nWith a discrete-time model in one dimension (e.g., an unstructured population model), we can trace the dynamics as we iterate the model forward using a technique called cobwebbing. Here’s a quick example of a Ricker recruitment model, a density-dependent population model with the feature that it overcompensates when numbers exceed the carrying capacity. When the population of highly reactive (i.e., has strong growth potential), this tendency for overcompensation can lead to some pretty wild dynamics. This plot shows such a case.\n\n## Ricker recruitment function\nricker.recruit &lt;- function(r0,K,N) N*exp(r0*(1-(N/K)))\n## fast growth!\nr0 &lt;- 3\nK &lt;- 50\nN &lt;- 0:150\nn0 &lt;- 25\n## iterate model for 10 time steps\nt &lt;- 10\ny &lt;- rep(0,t)\ny[1] &lt;- ricker.recruit(r0=r0,K=K,N=n0)\nfor(i in 2:t)  y[i] &lt;- ricker.recruit(r0=r0,K=K,N=y[i-1])\n\nplot(N,ricker.recruit(r0=r0,K=K,N=N), type=\"l\", col=\"black\", lwd=3, yaxs=\"i\",\n     ylim=c(0,150),\n     xlab=\"Current Number of Infections\", ylab=\"New Infections\")\nabline(a=0,b=1, lwd=2, col=grey(0.75))\nsegments(n0,0,n0,y[1], col=\"red\")\nsegments(n0,y[1],y[1],y[1], col=\"red\")\nfor(i in 2:(t-2)){\n    segments(y[i],y[i],y[i],y[i+1], col=\"red\") #vertical\n    segments(y[i],y[i+1],y[i+1],y[i+1], col=\"red\") #horiz\n    segments(y[i+1],y[i+1],y[i+1],y[i+2], col=\"red\") #vert\n    segments(y[i+1],y[i+2],y[i+2],y[i+2], col=\"red\") #horiz\n}\n\n\n\n\n\n\n\n## this could very easily be made into a function (and probably should be)\n\nThis population model with highly over-compensatory dynamics will never settle down. It always overshoots or undershoots and so fluctuates wildly. May (1976) notes that we can measure the strength of the response by the slope of the recruitment function at its equilibrium value (i.e., where the grey line of equality intersects with the recruitment function). Using the tools we’ve discussed in these note, we could calculate that slope and draw a tangent line at that point!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#numerical-derivative-for-equilibria-and-tangent-lines",
    "href": "drawing.html#numerical-derivative-for-equilibria-and-tangent-lines",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.6 Numerical Derivative for Equilibria and Tangent Lines",
    "text": "3.6 Numerical Derivative for Equilibria and Tangent Lines\nWe can recreate the figures from May’s classic paper on how simple population models can yield very complex dynamics (May 1976). May investigates the logistic map, a first-order difference equation. The logistic map is essentially a discrete-time density-dependent model.\n\\[\nX_{t+1} = aX_t(1 -X_t),\n\\]\nwhere \\(X_t\\) is the state of the population (e.g., its size) at time \\(t\\) and \\(a\\) is the per-period multiplicative growth rate in the absence of any density effect. May (1976) shows that when \\(a&gt;3\\), this model becomes unstable about its fixed point and when\n\n# logistic map\nlmap &lt;- expression(a*x*(1-x))\nx &lt;- seq(0,1,,1000)\n\n### first plot the unstable recruitment curve\na &lt;- 3.414\nx1 &lt;- eval(lmap)\nplot(x,x1, type=\"l\", lwd=2, \n     xaxs=\"i\", yaxs=\"i\", ylim=c(0,1),\n     xlab=expression(X[t]),ylab=expression(X[t+1]))\n\n## equilibrium for logistic map\nxstar1 &lt;- 1-(1/a)\n\n## numerical derivative\nx1p &lt;- diff(x1)\nxp &lt;- diff(x)\n### the equilibrium x is approximately x[706]\nm1 &lt;- x1p[706]/xp[706]\n\n### use point-slope eq for a line y - y_1 = m(x - x_1)\n### we know the point (xstar,xstar) so solve for eq we can use to draw line\nlines(x[550:850], m1*x[550:850]-m1*xstar1+xstar1,lty=2)\n\n### now plot the stable recruitment curve\na &lt;- 2.707\nx2 &lt;- eval(lmap)\nlines(x,x2, lwd=2, col=grey(0.75))\nxstar2 &lt;- 1-(1/a)\nx2p &lt;- diff(x2)\nxp &lt;- diff(x)\n### the equilibrium x is approximately x[700]\nm2 &lt;- x2p[630]/xp[630]\nlines(x[480:780], m2*x[480:780]-m2*xstar2+xstar2,lty=2)\nabline(a=0,b=1)\nlegend(\"topleft\", c(\"a=2.707\", \"a=3.414\"), col=c(grey(0.75), \"black\"), lwd=2)\n\n\n\n\n\n\n\n\nThe slope on the black curve at the fixed point is steeper than \\(-45^{\\circ}\\) so the fixed point for this higher-growth model is unstable, while the slope at the fixed point for the grey curve is shallower than \\(-45^{\\circ}\\) and is therefore stable.\nNaturally, we could have actually calculated the derivatives of the recruitment function, but this hack actually works pretty well. You just need to make sure that your \\(x\\) values are fine-grained enough that the numerical derivative is approximately right.\n\n3.6.1 Why Period Doubling\nMay (1976) shows how plotting the iterated map can help us understand the phenomenon of period-doubling.\n\nlmap3 &lt;- expression(a*(a*(a*x*(1-x))*(1-(a*x*(1-x))))*(1-(a*(a*x*(1-x))*(1-(a*x*(1-x))))))\na &lt;- 3.7\nx3 &lt;- eval(lmap3)\nplot(x,x3, type=\"l\",lwd=2,\n     xaxs=\"i\", yaxs=\"i\",\n     ylim=c(0,1),\n     xlab=expression(X[t]),ylab=expression(X[t+3]))\na &lt;- 3.9\nlines(x,eval(lmap3), lwd=2, col=grey(0.75))\nlines(x,x)\n\n\n\n\n\n\n\n\nThe black curve is only intersected by the line of equality once, indicating that there is a single period-3 cycle for \\(a=3.7\\). However, when we raise the growth rate to \\(a=3.9\\), the hills and valleys become much steeper and six more period-3 cycles appear!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#bifurcation-diagram",
    "href": "drawing.html#bifurcation-diagram",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.7 Bifurcation Diagram",
    "text": "3.7 Bifurcation Diagram\nMay (1976) logistic map. There is a doubling of the period of the time series at \\(a=3\\). At \\(a=3.57\\), cycles of period \\(2^n\\) begin to appear. At \\(a=3.68\\), period-3 cycles appear, and at \\(a=3.83\\), every integer period is present. We can show this using a bifurcation diagram, which shows the different values of \\(X\\) that the population will cycle between for various values of \\(a\\). For values of \\(a&lt;3\\), the equilibrium is stable, so there is only a line.\n\nn &lt;- 1\nR &lt;- seq(2.5,4,length=1000)\nf &lt;- expression(a*x*(1-x))\ndata &lt;- matrix(0,200,1001)\n\nfor(a in R){\n  x &lt;- runif(1) # random initial condition\n  ## first converge to attractor\n  for(i in 1:200){\n    x &lt;- eval(f)\n  } # collect points on attractor\n  for(i in 1:200){\n    x &lt;- eval(f)\n    data[i,n] &lt;- x\n  }\n  n &lt;- n+1\n}\n\ndata &lt;- data[,1:1000]\nplot(R,data[1,], pch=\".\", xlab=\"a\", ylab=\"X\")\nfor(i in 2:200) points(R,data[i,], pch=\".\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#bistable-attractors",
    "href": "drawing.html#bistable-attractors",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.8 Bistable Attractors",
    "text": "3.8 Bistable Attractors\nBistability means that a dynamic system has two distinct stable states.\nEfferson, Vogt, and Fehr (2020) develop a model of cultural change with heterogeneous preferences. They show that the system is characterized by bistability.\n\n## mixture of 2 logistic curves\nf &lt;- function(a1,b1,c1,a2,b2,c2,x,p){\n  return(x-(p*(c1/(1+exp(-(b1*(x-a1)))))+(1-p)*(0.25+c2/(1+exp(-(b2*(x-a2)))))))\n}\n\na1 &lt;- 1/2\nb1 &lt;- 15\nc1 &lt;- 1\na2 &lt;- 1/2\nb2 &lt;- 7\nc2 &lt;- 1/2\np &lt;- seq(0,1,length=1000)\nlibrary(rootSolve)\nuniroot.all(f,a1=a1,b1=b1,c1=c1,a2=a2,b2=b2,c2=c2,p=0.25,interval=c(0,1))\n\n[1] 0.5000000 0.2479318 0.7520682\n\nAA &lt;- matrix(NA,1000,3)\nfor(i in 1:1000){\n  tmp &lt;- uniroot.all(f,a1=a1,b1=b1,c1=c1,a2=a2,b2=b2,c2=c2,p=p[i],interval=c(0,1))\n  ifelse(length(tmp)==1,AA[i,1] &lt;- tmp, \n         ifelse(length(tmp)==2,AA[i,1:2] &lt;- tmp, AA[i,] &lt;- tmp))\n}\n\n## Attractor\nplot(p[-1000],AA[1:999,3],type=\"l\", lwd=3,\n     xlab=\"Proportion Wearing Masks\", \n     ylab=\"Probability of Mask Adoption\",\n     xlim=c(0,1),ylim=c(0,1))\nlines(p[-1000],AA[1:999,2], lwd=3)\nlines(p[-1000],AA[1:999,1], lty=2, lwd=3, col=\"grey\")\nlines(p[1:44],AA[1:44,1], lwd=3)\narrows(x0=c(0.2,0.4,0.6,0.8,1.0), y0=0.55, x1=c(0.2,0.4,0.6,0.8,1.0), y1=0.65, \n       lwd=3, col=\"red\", length=0.1)\narrows(x0=c(0.2,0.4,0.6,0.8,1.0), y0=0.45, x1=c(0.2,0.4,0.6,0.8,1.0), y1=0.35, \n       lwd=3, col=\"red\", length=0.1)\narrows(x0=0, y0=0.55, x1=0, y1=0.65, \n       lwd=3, col=\"red\", code=1, length=0.1)\narrows(x0=0, y0=0.45, x1=0, y1=0.35, \n       lwd=3, col=\"red\", code=1,length=0.1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#polygons",
    "href": "drawing.html#polygons",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.9 Polygons",
    "text": "3.9 Polygons\nTo create a polygon between two curves, \\(y_1\\) and \\(y_2\\), which are both functions of \\(x\\), you need to pass polygon() a concatenated vector of \\(x\\) and the reverse of \\(x\\) for the first vector, and then a concatenated vector of \\(y_2\\) and the reverse of \\(y_1\\) for the second vector.\nHere, fill in the polygon formed between the convex part of a logistic curve and a straight line that connects its endpoints.\n\nx &lt;- seq(10,30,,500)\nux &lt;- function(a,b,x) 1/(1+exp(-(x-a)/b))\na &lt;- 20\nb &lt;- 1.75\n\nx1 &lt;- seq(10,20,,250)\n# straight line\ny1 &lt;- (x1-10)/20\n# convex part of logistic\ny2 &lt;- ux(a=a,b=b,x=x1)\n\n\nplot(x,ux(a=a,b=b,x=x), type=\"l\", lwd=3, axes=FALSE, frame=TRUE, \n     xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Technological Development\")\nsegments(10,ux(a=a,b=b,x=10),30, ux(a=a,b=b,x=30), lwd=2, col=grey(0.85))\npolygon(c(x1, rev(x1)), c(y2, rev(y1)), col=\"plum\")\n\n\n\n\n\n\n\n\nProbably one of the most common uses for polygons is to fill in the tail (or some other part) of a probability density to show how much relative probability is contained in an interval or a tail. Here we compare the tail probability of a standard normal distribution with a low-df \\(t\\) distribution. We will color in the upper tail above the value of 1.96, the approximate 97.5th quantile of the standard normal distribution and the conventional definition of “statistical significance.”\n\n## normal\nz &lt;- seq(-20, 20, length=2000)\np &lt;- dnorm(z)\nplot(z,p, type=\"l\", lwd=2, xlim=c(-4,4), \n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nz0 &lt;- z[z &gt;= 1.96]    # define region to fill\nz0 &lt;- c(z0[1], z0)\np0 &lt;- p[z &gt;= 1.96]\np0 &lt;- c(0, p0)\npolygon(z0, p0, col=\"grey\")\n\n\n\n\n\n\n\n# integrate to see how much probability mass is in the tail\nintegrate(dnorm, 1.96, Inf)\n\n0.0249979 with absolute error &lt; 1.9e-05\n\n\nNow for the \\(t\\) distribution\n\n## t, df=1\nq &lt;- dt(z,df=1)\nplot(z,q, type=\"l\", lwd=2, xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nt0 &lt;- z[z &gt;= 1.96]    # define region to fill\nt0 &lt;- c(t0[1], t0)\nq0 &lt;- q[z &gt;= 1.96]\nq0 &lt;- c(dt(20,df=1), q0)\npolygon(t0, q0, col=\"grey\")\n\n\n\n\n\n\n\n## integrate to see how much probability mass is in the tail\nintegrate(dt, 1.96, Inf, df=1)\n\n0.1501714 with absolute error &lt; 1.1e-10\n\n\nOoh, pointy. Note that the \\(t\\) distribution decays so slowly that you can see that the polygon has a slope to it even when you extend the range out to 20. This explains why we added dt(20,df=1) as the first element of the q variable for the polygon.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#s-shaped-curves",
    "href": "drawing.html#s-shaped-curves",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.10 S-Shaped Curves",
    "text": "3.10 S-Shaped Curves\nThere are numerous instances where we want to draw S-shaped curves. For example, we might want to show density-dependent population growth or the fraction of a population who have adopted an innovation.\nThere are a number of ways to draw S-shaped curves. The first is to use a logistic function.\nAnother possibility is to use a cumulative distribution function for a normal random variable.\nHere is an example of an adoption curve (E. M. Rogers 2003).\n\n## stylized adoption curve\nx &lt;- seq(-4,4,,500)\nq &lt;- c(0.025, 0.16, 0.5, 0.84)\nqq &lt;- qnorm(q)\ndd &lt;- dnorm(qq)\nzz &lt;- rep(0,4)\n\nplot(x, dnorm(x), type=\"n\", axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Time\", ylab=\"Fraction Adopting\", ylim=c(0,1.05))\naxis(2)\nsegments(qq,zz,qq,dd, lwd=3, col=rgb(1,0,0,0.5))\nlines(x, dnorm(x), lwd=3, col=grey(0.65))\nlines(x,pnorm(x), lwd=3)\nlegend(\"topleft\",c(\"incident\",\"cumulative\"),lwd=3, col=c(grey(0.65),\"black\"))\n\n\n\n\n\n\n\n\n\n3.10.1 Polygyny Threshold Model\nAnother application of S-shaped curves in behavioral ecology is the Polygyny Threshold Model (Orians 1969).\n\n## Polygyny Threshold\nlogisfun &lt;- function(n0=1,K=100,r=0.05,t,delay=0) n0*exp(r*(t-delay))/((1+n0*(exp(r*(t-delay))-1)/K))\n\nt &lt;- seq(0,250,,500)\ndelay &lt;- 50\nn0 &lt;- 1\nK &lt;- 100\nr &lt;- 0.05\n\ny1 &lt;- logisfun(t=t)\ny2 &lt;- logisfun(t=t, delay=50)\n\nplot(t,y1/100, type=\"l\", lwd=3, col=\"magenta\",\n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     ylab=\"Female Reproductive Success\", xlab=\"Resource-Holding Capacity\")\nlines(t,y2/100, col=\"purple\", lwd=3)\nsegments(150,0,150,0.6,lty=3)\nsegments(100,0,100,0.6, lty=3)\nsegments(150,0.6,0,0.6,lty=3)\nmtext(expression(Delta[RHC]),1, at=125, col=\"red\")\naxis(1,at=c(100,150), labels=FALSE, col=\"red\")\ntext(177,0.6, expression(RS[S]==RS[P]), col=\"red\")\nlegend(\"topleft\",c(\"primary\",\"secondary\"), lty=1, lwd=3, col=c(\"magenta\",\"purple\"))\n\n\n\n\n\n\n\n\nThe fitness of the secondary mate is lower for all resource levels except the very highest. If the quality of an already-mated male’s territory is greater than an unmated male’s territory by \\(\\Delta_{RHC}\\), then the fitness of the secondary female is greater than the fitness of a primary female mated to the male with the lower-quality territory. As a result, we expect a polygynous mating.\nFor a quick-and-dirty solution, it is convenient to just represent some equation of interest as an expression(), define some parameters, and then get the function’s values by using eval(). In most cases, it is going to be better to write a function for your equation. In this case, it allowed us to find values for FRS from the two logistic curves associated with specific values of RHC. While it might be a bit more work to write a function, it will probably save effort in the long run. I often prototype plots using expression() and then write a function once I’ve worked out the general features of the plot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#evolutionary-stable-strategies",
    "href": "drawing.html#evolutionary-stable-strategies",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.11 Evolutionary Stable Strategies",
    "text": "3.11 Evolutionary Stable Strategies\n\n3.11.1 Rogers Paradox\nSelection doesn’t necessarily increase mean fitness (A. R. Rogers 1988). Different Rogers.\n\n## Figure 1 from Rogers (1988)\nwi &lt;- function(b=1,c=0.9) 1 + b*(1-c)\nws &lt;- function(b=1,s=0,p,u=0.8) 1 + (b*(1-s)*(1-p)*(1-u))/(1 - p*(1-u))\n\np &lt;- seq(0,1,,100)\n\ncult &lt;- ws(p=p)\nacult &lt;- rep(wi(),100)\nm &lt;- cult*p + acult*(1-p)\n\nplot(p,cult, type=\"l\", lwd=2, axes=FALSE, frame=TRUE,\n     xaxs=\"i\",\n     xlab=\"Frequency of Social Learning (p)\", ylab=\"Fitness\")\nlines(p,acult, col=\"red\",lwd=2)\nlines(p,m, col=grey(0.75), lty=2,lwd=2)\naxis(1)\nlegend(\"topright\", c(\"individual\",\"social\",\"mean\"), \n       col=c(\"red\",\"black\",grey(0.75)), lty=c(1,1,2))\n\n\n\n\n\n\n\n\n\n\n3.11.2 Producer-Scrounger Game\n\np1 &lt;- seq(0,1,,200)\nwthief &lt;- expression(1.5 - 2*p1)\nwfarmer &lt;- expression(1 - 1.2*p1)\n\nplot(p1, eval(wthief), type = \"l\", col=\"red\", lwd=2,\n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Fraction Scroungers\", ylab = \"Fitness\", \n     xlim=c(0,1), ylim=c(0,2))\nlines(p1, eval(wfarmer), lwd=2, col=\"blue\")\nlines(p1,(p1*eval(wthief)+(1-p1)*eval(wfarmer)), lwd=2, lty=2)\naxis(1,at=c(0,1),labels=c(\"0\",\"1\"))\nabline(v=0.625, col=grey(0.75), lwd=1)\narrows(0.4,1.5,0.62,1.5,  col=grey(0.75),lwd=2)\narrows(0.8,1.5,0.63,1.5,  col=grey(0.75),lwd=2)\nlegend(\"topright\", c(\"Scrounger\",\"Producer\",\"Mean\"), \n       col=c(\"red\",\"blue\",\"black\"), lwd=2, lty=c(1,1,2))\naxis(3,at=0.625,labels=c(\"ESS\"))\n\n\n\n\n\n\n\n\nDraw arrows to show the equilibrium and its stability.\nGraphics Tip: Note how we marked the location of the ESS on the top of the plot using axis(3)!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#prey-choice-model",
    "href": "drawing.html#prey-choice-model",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.12 Prey-Choice Model",
    "text": "3.12 Prey-Choice Model\n\nx &lt;- seq(0,20,length=500)\nf &lt;- function(x,b) {\n  1 - exp(-b*(x-1))\n}\n\nf1 &lt;- function(x,a) a/x\n\nplot(x,f(x,b=0.1), type=\"l\", lwd=3, xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE,\n     xlab=\"Item Rank\",\n     ylab=\"Energy Gain\",\n     ylim=c(0,1))\naxis(1)\nbox()\nlines(x,f(x,b=0.05), lwd=3, col=\"red\")\nlines(x,f1(x,a=2), lwd=3, col=\"grey\")\n# found approximate points using locator()\ntext(2.9,0.9648077,expression(E/h))\ntext(18.75,0.8848253,expression(E^g/t))\ntext(18.75,0.6362313,expression(E^b/t))\nsegments(5.5,0,7.4,0, lwd=10, col=\"green\", lend=2) \ntext(6.45,0.04825, \"Fallback Foods\")\n\n\n\n\n\n\n\n\nIn a bad year, diet breadth expands. The extent of this expansion is indicated by the green bar.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#linear-programming",
    "href": "drawing.html#linear-programming",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.13 Linear Programming",
    "text": "3.13 Linear Programming\nFrom Belovsky (1987). Optimal diet for hunter-gatherers based on constraints on feeding time, digestive capacity, energy requirements, and protein needs. Uses linear programming to find the optimal solution. Belovsky shows that, contrary to much thinking about foraging, that hunter-gatherers tend to maximize either total food intake or protein and do not time-minimize.\n\n#### constraint functions\n\n## feeding time constraint\n## 393 &gt;= 0.34*H + 0.42*G\n\ntime.slope &lt;- -0.42/0.34\ntime.int &lt;- 393/0.34\n\n## digestive constraint\n## 700 &gt;= H + 0.67*G\n\ndig.slope &lt;- -0.67\ndig.int &lt;- 700\n\n## engergy constraint\n## 1975 =&lt; 3*H + 3.05*G (or 3.22*G)\n\nenergy.slope &lt;- -3.05/3\nenergy.int &lt;- 1975/3\n\n## protein constraint\n## 60 =&lt; 0.15*H + 0.12*G\n\nprotein.slope &lt;- -0.12/0.15\nprotein.int &lt;- 60/0.15\n\n\n#  polygon top is digestion until it meets time; bottom is energy\nxx &lt;- 0:1000\nddy &lt;- 700 - 0.67*xx\ntty &lt;- 393/.34 - (0.42/0.34)*xx\neey &lt;- 1975/3 - (3.05/3)*xx\n\nmin(which(tty&lt;0))\n\n[1] 937\n\n# 937\ngg &lt;- c(1:937, 937:1)\n\nmin(which(ddy&gt;=tty))\n\n[1] 808\n\n# 808\nhh &lt;- c(ddy[1:808],tty[809:937],eey[937:1])\n\n\nplot(1:1200,1:1200, type=\"n\", yaxs=\"i\", xaxs=\"i\",\n     xlab=\"Gathered Food (g)\", ylab=\"Hunted Food (g)\")\nabline(a=time.int, b=time.slope)\nabline(a=dig.int, b=dig.slope)\nabline(a=energy.int, b=energy.slope)\nabline(a=protein.int, b=protein.slope)\npolygon(gg,hh,col=grey(0.85), border=\"black\")\npoints(807,ddy[808], pch=21, cex=1.5)\nabline(h=0,lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#extreme-value-distribution-for-innovation",
    "href": "drawing.html#extreme-value-distribution-for-innovation",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.14 Extreme-Value Distribution for Innovation",
    "text": "3.14 Extreme-Value Distribution for Innovation\nInnovation happens when the skill of some learner exceeds the current highest level of skill. The distribution for extreme values like this is know, oddly enough, as an extreme value distribution. There are a number of specific flavors of such distributions.\nHenrich (2004) suggests that the specific EVD for the highest skill is a Gumbel distribution (a type of extreme-value distribution). The level of skill that improves culture is \\(z_h\\). The probability that there will be models with skill level \\(z_h\\) or better is small in smaller populations because this value is out in the tail of the distribution.\nTwo parameters: \\(\\alpha\\) (difficulty) and \\(z\\) (skillfulness), which are assumed independent.\n\n# extreme-value distribution package\nlibrary(evd)\nx &lt;- seq(0,100,,500)\n\nplot(x,dgumbel(x,30,10), type=\"l\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=expression(paste(\"Learner's Skill, \", z[i])), \n     ylab=expression(paste(\"Probability of Acquiring \", z[i])))\nlines(x,dgumbel(x,30,15), col=\"magenta\", lwd=3)\nabline(v=60,lty=2)\nmtext(expression(z[h]),1, at=60, padj=1)\n\n\n\n\n\n\n\n\nNote that in the more-innovative population, far more individuals exceed the critical threshold \\(z_h\\), but also way more people are clearly bad at the skill.\nThis result flows precisely from the assumption that learner’s skill follows a Gumbel distribution (i.e., Henrich 2004). It is perfectly fair to ask if this is a reasonable model, but we do know that innovation cultures make a lot of mistakes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#the-right-ibis",
    "href": "drawing.html#the-right-ibis",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.15 The Right Ibis",
    "text": "3.15 The Right Ibis\nLeslie and Winterhalder (2002) describe a bespoke utility function they call the right ibis model. We can code that and then use it to illustrate Tainter’s model of social collapse.\n\nribis &lt;- function(x,m,n,r){\n  if(x&lt;0 | x&gt;=r){\n    ibis &lt;- 0\n  }\n  if(x&gt;=0 & x&lt;m) {\n    ibis &lt;- exp(m^2/((m-n)^2) - (m*x/(m-n)^2)) * x^(m^2/((m-n)^2)) *\n      m^(-m^2/(m-n)^2)\n  }\n  if(x&gt;=m & x&lt;r){\n    ibis &lt;-  1-(x-m)^2/(m-r)^2\n  }\n  return(ibis)\n}\n\nm &lt;- 8\nn &lt;- 2\nr &lt;- 18\nx &lt;- 0:16\n### Use Right Ibis for Tainter plot\n## plot a nice smooth curve\ny &lt;- seq(0,16,length=100)\naaa &lt;- rep(0,100)\nfor(i in 1:100) aaa[i] &lt;- ribis(x=y[i],m=m,n=n,r=r)\n\nmc &lt;- ribis(x=8,m=m,n=n,r=r)\ndb &lt;- ribis(x=15,m=m,n=n,r=r)\ncl &lt;- ribis(x=2.895,m=m,n=n,r=r)\n\nplot(y,aaa,type=\"l\", lwd=2, axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     ylim=c(0,1.1),\n     xlab=\"Complexity\", ylab=\"Benefits to Complexity\")\nsegments(8,mc,0,mc,lty=3)\nsegments(8,0,8,mc,lty=3)\n\nsegments(15,db,0,db,lty=3, col=\"red\")\nsegments(15,0,15,db,lty=3, col=\"red\")\n#segments(2.895,cl,0,cl,lty=3, col=\"red\")\nsegments(2.895,0,2.895,cl,lty=3, col=\"red\")\n#mtext(expression(paste(B, \"*\")),2, at=mc, adj=1, las=2)\nmtext(expression(B[max]),2, at=mc, adj=1, las=2)\nmtext(expression(B[mid]),2, at=db, adj=1, las=2)\nmtext(expression(C[lo]),1, at=2.895, padj=1)\nmtext(expression(C[opt]),1, at=8, padj=1)\nmtext(expression(C[hi]),1, at=15, padj=1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#autocorrelated-time-series",
    "href": "drawing.html#autocorrelated-time-series",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.16 Autocorrelated Time Series",
    "text": "3.16 Autocorrelated Time Series\nSometimes you want to plot a time series for illustrative purposes. More interesting time series, like most actual biophysical series, are likely to show some positive autocorrelation. The AR(1) model is simple model that can produce interesting plots. It is the simplest of the autoregressive models, where the state in the present depends only on the state in the previous time step. A\n\\[\nX_{t+1} = \\varphi X_t + \\varepsilon_t\n\\]\nThe parameter \\(\\varphi\\) is the autocorrelation coefficient and \\(\\varepsilon_t\\) is the shock or “innovation” at time \\(t\\), which is taken to be \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\nWe might use this model to represent volatile income of an individual hunter:\n\nac1fun &lt;- function(xo,alpha,sigma,tmax){\n  x &lt;- rep(0,tmax+1)\n  x[1] &lt;- xo\n  rr &lt;- rnorm(tmax,0,sigma)\n  for(i in 2:(tmax+1)) x[i] &lt;- alpha*x[i-1] + rr[i-1]\n  return(x)\n}\n\n## xo = initial size\n## alpha = autocorrelation\n## sigma = sd of Gaussian noise\n## tmax = max number of time steps\nxo &lt;- 1\nalpha &lt;- 0.5\nsigma &lt;- 1\ntmax &lt;- 100\n\nx1 &lt;- ac1fun(xo=xo,alpha=alpha,sigma=sigma,tmax=tmax)\n\nplot(0:tmax+1, x1, type=\"l\", lwd=2, axes=FALSE, xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Income\")\nabline(h=mean(x1), col=\"red\")\naxis(1,labels=FALSE)\naxis(2,labels=FALSE)\nbox()\n\n\n\n\n\n\n\n## Calculate frequency spectrum\n\nspect &lt;- spectrum(x1, log=\"no\", spans=c(2,2), plot=FALSE)\nspecx &lt;- spect$freq\nspecy &lt;- 2*spect$spec\nttext &lt;- \"Spectrum for Positive Autocorrelation,\"\nplot(specx, specy, type=\"l\", lwd=2,\n     xlab=\"Frequency (1/day)\", ylab=\"Spectral Density\")\ntitle(bquote(.(ttext) ~ alpha==0.5))\n\n\n\n\n\n\n\n\nspectrum calculates the frequency axis in terms of cycles per sampling interval; it makes more sense to convert to cycles per unit time (so divide by the sampling interval). The spectrum is generally far more interpretable if it is smoothed. To do this, we use the argument spans, which specifies the parameter(s) for the what is known as the modified Daniell kernel for smoothing the spectrum. The modified Daniell kernel is essentially just a running average (see code below for a sense of what these parameters do). There is no hard-and-fast rule for how to do this (try a couple different values), but the higher the number of spans, the more smoothing and the lower the peaks of the spectrum will be.\nThe default for spectrum is to calculate the spectrum on a log-scale. Use the argument log=\"no\" to change this default. Note also that the spectrum needs to be multiplied by 2 to make it actually equal to variance!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#how-wavelets-work",
    "href": "drawing.html#how-wavelets-work",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.17 How Wavelets Work",
    "text": "3.17 How Wavelets Work\nWavelets are like spectral analysis, but they work at multiple scales. Suppose we have a series \\(x(t)\\). The wavelet transform involves multiplying the series by the wavelet which has been stretched to various scales spanning the series and summing this product. The scale is determined by the parameter \\(\\tau\\). Note how conceptually similar this is to calculating the covariance of two variables.\n\\[\nW_x(a,\\tau) = \\frac{1}{\\sqrt{a}} \\int_{-\\infty}^{\\infty} x(t)\\psi^* \\left(\\frac{t-\\tau}{a}\\right) dt = \\int_{-\\infty}^{\\infty} x(t)\\psi^*_{a,\\tau}(t) dt\n\\]\n\nlibrary(biwavelet)\nmorlet &lt;- function(x) exp(-x^2/2) * cos(5*x)\nx &lt;- seq(-4,4,length=1000)\ny &lt;- morlet(x)\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\n\n\n\n\n\n\n\n\nNow generate a periodic series and overlay on the mother wavelet.\n\nf &lt;- expression(cos(2*pi*x)*exp(-pi*x^2))\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\nlines(x,eval(f), lwd=2)\n\n\n\n\n\n\n\n\nWe can see that the wavelet captures this periodic variation pretty well. The wavelet transform of the signal (red) shows that most of the function lies above the \\(x=0\\) line and its sum is strongly positive.\n\ndx &lt;- diff(x)\ndx &lt;- c(dx,dx[999])\n\nplot(x, y*eval(f)*dx, type=\"l\", lwd=3, col=\"red\", xlab=\"x\",\n     ylab=expression(paste(\"integrand, \", psi[a,tau](t), x(t))))\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\nWhat happens when the signal is not well matched by the wavelet? In the next figure, the signal in black is largely random with respect to the mother wavelet. When we plot the wavelet transform of this signal (red), there is approximately an equal amount of area above and below the the \\(x=0\\) line and the sum is effectively zero.\n\n# another function\nf1 &lt;- expression(cos(12*pi*x)*exp(-pi*x^2))\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\nlines(x,eval(f1), lwd=2)\n\n\n\n\n\n\n\nplot(x, y*eval(f1)*dx, type=\"l\", lwd=3, col=\"red\", xlab=\"x\",\n     ylab=expression(paste(\"integrand, \", psi[a,tau](t), x(t))))\nabline(h=0, lty=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#to-do",
    "href": "drawing.html#to-do",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.18 To Do",
    "text": "3.18 To Do\n\nSplines\nKinship/genealogy\nPhylogenetic trees\nDAGs/causal diagrams\nShow how cobwebbed Ricker recruitment translates into chaotic time series\n\n\n\n\n\nBaalen, M. van, and M. W. Sabelis. 1995. “The Dynamics of Multiple Infection and the Evolution of Virulence.” American Naturalist 146 (6): 881–910. http://www.jstor.org/stable/2463102.\n\n\nBelovsky, G. E. 1987. “Hunter-Gatherer Foraging: A Linear Programming Approach.” Journal of Anthropological Archaeology 6 (1): 29–76. https://doi.org/10.1016/0278-4165(87)90016-X.\n\n\nCharnov, Eric L. 1976. “Optimal Foraging, the Marginal Value Theorem.” Theoretical Population Biology 9 (2): 129–36. https://doi.org/10.1016/0040-5809(76)90040-X.\n\n\n———. 1997. “Trade-Off-Invariant Rules for Evolutionary Stable Life Histories.” Nature 387 (6631): 393–94. https://doi.org/10.1038/387393a0.\n\n\nEfferson, Charles, Sonja Vogt, and Ernst Fehr. 2020. “The Promise and the Peril of Using Social Influence to Reverse Harmful Traditions.” Nature Human Behaviour 4 (1): 55–68. https://doi.org/10.1038/s41562-019-0768-2.\n\n\nHenrich, Joseph. 2004. “Demography and Cultural Evolution: How Adaptive Cultural Processes Can Produce Maladaptive Losses: The Tasmanian Case.” American Antiquity 69 (2): 197–214. https://doi.org/10.2307/4128416.\n\n\nLeslie, P., and B. Winterhalder. 2002. “Demographic Consequences of Unpredictability in Fertility Outcomes.” American Journal of Human Biology 14 (2): 168–83. https://doi.org/10.1002/ajhb.10044.\n\n\nLevins, R. 1968. Evolution in Changing Environments. Princeton: Princeton University Press.\n\n\nLevins, Richard. 1962. “Theory of Fitness in a Heterogeneous Environment. I. The Fitness Set and Adaptive Function.” The American Naturalist 96 (891): 361–73. http://www.jstor.org/stable/2458725.\n\n\nMay, R. M. 1976. “Simple Mathematical-Models with Very Complicated Dynamics.” Nature 261 (5560): 459–67. https://doi.org/10.1038/261459a0.\n\n\nNoy-Meir, Imanuel. 1975. “Stability of Grazing Systems: An Application of Predator-Prey Graphs.” Journal of Ecology 63 (2): 459–81. https://doi.org/10.2307/2258730.\n\n\nOrians, Gordon H. 1969. “On the Evolution of Mating Systems in Birds and Mammals.” The American Naturalist 103 (934): 589–603. https://doi.org/10.1086/282628.\n\n\nParker, G. A., and R. A. Stuart. 1976. “Animal Behavior as a Strategy Optimizer: Evolution of Resource Assessment Strategies and Optimal Emigration Thresholds.” American Naturalist 110 (976): 1055–76. https://doi.org/10.1086/283126.\n\n\nRogers, A. R. 1988. “Does Biology Constrain Culture?” American Anthropologist 90 (4): 819–31. https://doi.org/10.1525/aa.1988.90.4.02a00030.\n\n\nRogers, E. M. 2003. Diffusion of Innovations. 5th ed. New York: Free Press.\n\n\nScheffer, M. 2009. Critical Transitions in Nature and Society. Princeton: Princeton University Press.\n\n\nSmith, C. C., and S. D. Fretwell. 1974. “The Optimal Balance Between Size and Number of Offspring.” American Naturalist 108: 499–506. https://www.jstor.org/stable/2459681.\n\n\nZeeman, E. C. 1976. “Catastrophe Theory.” Scientific American 234 (4): 65–65 &. https://doi.org/10.1038/scientificamerican0476-65.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "graphs.html#some-definitions",
    "href": "graphs.html#some-definitions",
    "title": "3  Actual Graphs",
    "section": "3.1 Some Definitions",
    "text": "3.1 Some Definitions\nA graph is simply a collection of vertices (or nodes) and edges (or ties). We can denote this \\(\\mathcal{G}(V,E)\\), where \\(V\\) is a the vertex set and \\(E\\) is the edge set. The vertices of the graph represent the actors in the social system. These are usually individual people, but they could be households, geographical localities, institutions, or other social entities. The edges of the graph represent the relations between these entities (e.g., “is friends with” or “has sexual intercourse with” or “sends money to”). These edges can be directed (e.g., “sends money to”) or undirected (e.g., “within 2 meters of”).\nWhen the relations that define the graph are directional, we have a directed graph or digraph.\nGraphs (and digraphs) can be binary (i.e., presence/absence of a relationship) or valued (e.g., “groomed five times in the observation period”, “sent $100”).\nA graph (with no self-loops) with \\(n\\) vertices has \\({n \\choose 2} = n(n-1)/2\\) possible unordered pairs. This number (which can get very big!) is important for defining the density of a graph, i.e., the fraction of all possible relations that actually exist in a network.\nA bipartite graph is a graph where all the nodes of a graph can be partitioned into two sets \\(\\mathcal{V}_1\\) and \\(\\mathcal{V}_2\\) such that for all edges in the graph connects and unordered pair where one vertex comes from \\(\\mathcal{V}_1\\) and the other from \\(\\mathcal{V}_2\\). Often called an “affiliation graph” as bipartite graphs are used to represent people’s affiliations to organizations or events."
  },
  {
    "objectID": "graphs.html#various-ways-to-specify-graphs-in-igraph",
    "href": "graphs.html#various-ways-to-specify-graphs-in-igraph",
    "title": "4  Actual Graphs",
    "section": "4.2 Various Ways to Specify Graphs in igraph",
    "text": "4.2 Various Ways to Specify Graphs in igraph\nThe R package igraph provides tools for the analysis and visualization of networks. The package is actually just a set of R bindings for functions written in C++ that can be used in a variety of environments (e.g., native, R, Python).\n\n4.2.1 Specifying Small Graphs\nWe can create a small, undirected graph of five vertices from a vector of vertex pairs using the function make_graph():\n\nrequire(igraph)\ng &lt;- make_graph( c(1,2, 1,3, 2,3, 2,4, 3,5, 4,5), n=5, dir=FALSE )\nplot(g, vertex.color=\"skyblue2\")\n\n\n\n\n\n\n\n\nThe call to the function make_graph() (which can be shortened to graph()) takes three arguments in this case. First, we enumerate the edges by listing the pairs of vertices which are connected. In this graph, there are six edges. Second, we define the size of our graph. This simple graph has five vertices, so n=5. Third, the default graph type is directed, so to create an undirected graph, we need to specify dir=FALSE. The function graph() creates a graph object which, like any R object, is associated with a number of methods. When we plot a graph object, the plotting method used is plot.igraph(). There are a number of features (or perhaps peculiarities) of the defaults of plot.igraph(). First, is the vertex color. It’s not hideous but it’s not an obvious choice for a default color either. Second, the default font label style is Roman, which can make the labels look cluttered. I typically change to a sens-serif font using the argument vertex.label.family=\"Helvetica\". Third, the layout will not necessarily make sense to you as a human viewer of the graph and will typically change each time you call plot.igraph(). Fortunately, igraph has a number of excellent tools for assisting with graph layout.\nFor small graphs representing the relationships between a few named individuals, we can create a graph using graph_from_literal(). Undirected edges are indicated with one or more dashes -, --, etc. It doesn’t matter how many dashes you use – you can use as many as you want to make your code more readable. The colon operator : links “vertex sets” – i.e., creates ties between all members of two groups of vertices. So, for the Scooby Gang, we could specify the following graph\n\ng &lt;- graph_from_literal(Fred:Daphne:Velma:Shaggy-Fred:Daphne:Velma:Shaggy, Shaggy-Scooby)\nplot(g, vertex.shape=\"none\", vertex.label.color=\"black\")\n\n\n\n\n\n\n\n\nFor directed edges, use -+ where the plus indicates the direction of the arrow, i.e., A --+ B creates a directed edge from A to B. A mutual edge can be created using +-+.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#a-note-on-visualizing-graphs",
    "href": "graphs.html#a-note-on-visualizing-graphs",
    "title": "4  Actual Graphs",
    "section": "4.3 A Note On Visualizing Graphs",
    "text": "4.3 A Note On Visualizing Graphs\nYou will notice that many of the graphs in these notes are a bit cramped. This happens because when I render the Quarto document, R generates fairly small .png files. If you have, for example, vertex labels that really need to be read, it is a good idea to send your plot to a file that uses a vector-based format and potentially make it big. My preference is .pdf, but an argument can be made that .svg is even better. To do this, you just need to wrap your plotting commands in call to .pdf: pdf(file=\"filename.pdf\", height14, width=14) and then don’t forget to close this off (i.e., after all your plotting commands) with dev.off() or you’ll keep sending graphics to your pdf file! The default size for pdf is \\(7 \\times 7\\) (in inches). By specifying the optional arguments height and width, we’ve doubled the size of the plot. This will spread things out quite a bit and you may actually have to increase the size of your vertices, labels, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#special-graphs",
    "href": "graphs.html#special-graphs",
    "title": "4  Actual Graphs",
    "section": "4.4 Special Graphs",
    "text": "4.4 Special Graphs\nA wide variety of special graphs are built into igraph. Note: I really don’t like the current default color in igraph (a kind of sickly orange), so I set the vertex color for every plot – you don’t have to do that\n\n4.4.1 Empty, Full, Ring\n\n# empty graph\ng0 &lt;- make_empty_graph(20)\nplot(g0, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# full graph\ng1 &lt;- make_full_graph(20)\nplot(g1, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# ring\ng2 &lt;- make_ring(20)\nplot(g2, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.4.2 Lattice, Tree, Star\n\n# lattice\ng3 &lt;- make_lattice(dimvector=c(10,10))\nplot(g3, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# tree\ng4 &lt;- make_tree(20, children = 3, mode = \"undirected\")\nplot(g4, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# star\ng5 &lt;- make_star(20, mode=\"undirected\")\nplot(g5, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.4.3 Erdos-Renyi & Power-Law\n\n# Erdos-Renyi Random Graph\ng6 &lt;- sample_gnm(n=100,m=50)\nplot(g6, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n# Power Law\ng7 &lt;- sample_pa(n=100, power=1.5, m=1,  directed=FALSE)\nplot(g7, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#working-with-graphs",
    "href": "graphs.html#working-with-graphs",
    "title": "4  Actual Graphs",
    "section": "4.5 Working with Graphs",
    "text": "4.5 Working with Graphs\n\n4.5.1 Putting Graphs Together\nSometimes you want to plot two (or more) graphs together. The disjoint union operator allows you to merge two graphs with different vertex sets:\n\nplot(g4 %du% g7, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Rewiring and Connected Components\nWe often want to shuffle the edges of our graph around. A common application of this functionality is when we want to randomize the edges of a graph while maintaining the same vertex set and overall number of edges.\nWhen you rewire a graph, there is a chance you will create isolates (i.e., vertices with no incident edges). For visualization purposes, you often want to remove these. You frequently will want to extract the largest connected subcomponent of your graph.\nA subgraph is a graph \\(\\mathcal{G}^{\\prime}\\) where all the vertices and edges are also in graph \\(\\mathcal{G}\\). Subgraphs can be generated by selecting either vertices or the edges from \\(\\mathcal{G}\\). A component is a maximally connected subgraph of a graph (i.e., a path exists between all vertices in the subgraph). The igraph function subcomponent() will find all the subcomponents of your graph and order them in terms of their size. The largest subcomponent will be first, so you will often want to subset your graph (g) using the criterion subcomponent(g,1).\n\ngg &lt;- g4 %du% g7\ngg &lt;- rewire(gg, each_edge(prob = 0.3))\nplot(gg, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n## retain only the connected component\ngg &lt;- induced_subgraph(gg, subcomponent(gg,1))\nplot(gg, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.5.3 Vertex and Edge Attributes\nYou can add arbitrary attributes to both vertices and edges. Generally, you do this to store information for plotting: colors, edge weights, names, etc. Some attributes are automatically created when you construct an graph object (e.g., “name” or “weight” if you load a weighted adjacency matrix)\n\nV(g) accesses vertex attributes\nE(g) accesses edge attributes\n\n\n## look at the structure\ng4\n\nIGRAPH 95612d0 U--- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c)\n+ edges from 95612d0:\n [1] 1-- 2 1-- 3 1-- 4 2-- 5 2-- 6 2-- 7 3-- 8 3-- 9 3--10 4--11 4--12 4--13\n[13] 5--14 5--15 5--16 6--17 6--18 6--19 7--20\n\nV(g4)$name &lt;- LETTERS[1:20]\n## see how it's changed\ng4\n\nIGRAPH 95612d0 UN-- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c), name (v/c)\n+ edges from 95612d0 (vertex names):\n [1] A--B A--C A--D B--E B--F B--G C--H C--I C--J D--K D--L D--M E--N E--O E--P\n[16] F--Q F--R F--S G--T\n\n## see what I did there?\n## hexadecimal color codes\nV(g4)$vertex.color &lt;- \"#4503fc\"\nE(g4)$edge.color &lt;- \"#abed8e\"\ng4\n\nIGRAPH 95612d0 UN-- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c), name (v/c),\n| vertex.color (v/c), edge.color (e/c)\n+ edges from 95612d0 (vertex names):\n [1] A--B A--C A--D B--E B--F B--G C--H C--I C--J D--K D--L D--M E--N E--O E--P\n[16] F--Q F--R F--S G--T\n\nplot(g4, vertex.size=15, vertex.label=NA, vertex.color=V(g4)$vertex.color, \n     vertex.frame.color=V(g4)$vertex.color,\n     edge.color=E(g4)$edge.color, edge.width=3)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#data-formats",
    "href": "graphs.html#data-formats",
    "title": "4  Actual Graphs",
    "section": "4.6 Data Formats",
    "text": "4.6 Data Formats\n\n4.6.1 Adjacency Matrices\nWe can represent the relationships of a social network using a matrix. A matrix is simply a rectangular array of numbers with (n) rows and \\(k\\) columns. It is conventional to denote matrices mathematically using capital letters and boldface, such as \\(\\mathbf{A}\\). We indicate the \\(ij\\)th element (i.e., the element corresponding to row \\(i\\) and column \\(j\\)) of \\(\\mathbf{A}\\) as \\(a_{ij}\\). A sociomatrix or adjacency matrix is a square matrix (i.e., \\(n \\times n\\), where \\(n\\) is the number of vertices in the network). It is typically binary, with \\(a_{ij}=1\\) if individuals \\(i\\) and \\(j\\) share an edge and \\(a_{ij}=0\\) otherwise. Consider a triangle:\n\n# generate a triangle\ng &lt;- graph( c(1,2, 2,3, 1,3), n=3, dir=FALSE)\n\nWarning: `graph()` was deprecated in igraph 2.1.0.\nℹ Please use `make_graph()` instead.\n\n### coordinatess to make the triangle look nice\ntri.coords &lt;- matrix( c(228,416, 436,0, 20,0), nr=3, nc=2, byrow=TRUE)\npar(mfrow=c(1,2))\nplot(g, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\nplot(g, layout=tri.coords, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\nThe sociomatrix corresponding to our triangle is\n\\[\\begin{equation}\n\\mathbf{A} = \\left[ \\begin{array}{cccc}\n0   &  1   & 1 \\\\\n1   &  0   & 1 \\\\\n1   &  1   & 0  \\end{array} \\right].\n\\end{equation}\\]\nBy convention, the diagonal elements of a sociomatrix are all zero (i.e., self-loops are not allowed). Sociomatrix \\(\\mathbf{A}\\) in the equation above is symmetric (\\(a_{ij} = a_{ji}\\)) because the graph is undirected. For a digraph, the upper triangle (i.e., matrix elements above the diagonal) of the sociomatrix will generally be different than the lower triangle.\nMost primatologists/behavioral ecologists probably have experience thinking in terms of adjacency matrices. An example of an adjacency matrix is the pairwise interaction matrices (e.g., agonistic or affiliative interactions) that we construct from behavioral observations.\nA very important potential gotcha: when you read data into R, it will be in the form of a data frame. Converting an adjacency matrix to an igraph graph object requires the data to be in the matrix class. Therefore, you need to coerce the data you read in by wrapping your read.table() in an as.matrix() command.\n\nkids &lt;- as.matrix(\n  read.table(\"data/strayer_strayer1976-fig2.txt\",\n                             header=FALSE)\n  )\nkid.names &lt;- c(\"Ro\",\"Ss\",\"Br\",\"If\",\"Td\",\"Sd\",\"Pe\",\"Ir\",\"Cs\",\"Ka\",\n                \"Ch\",\"Ty\",\"Gl\",\"Sa\", \"Me\",\"Ju\",\"Sh\")\ncolnames(kids) &lt;- kid.names\nrownames(kids) &lt;- kid.names\ng &lt;- graph_from_adjacency_matrix(kids, mode=\"directed\", weighted=TRUE)\nlay &lt;- layout_with_fr(g)\nplot(g, layout=lay, edge.arrow.size=0.5,\n     vertex.color=\"skyblue2\", vertex.label.family=\"Helvetica\", \n     vertex.frame.color=\"skyblue2\")\n\n\n\n\n\n\n\n\nNote that you might want to change some of the graphics parameters depending on the type of display you use. For this document, the figures are constrained to be small, so you don’t want edges – and particularly arrows – to be too thick.\n\n\n4.6.2 Edge Lists\nAdjacency matrices are actually very inefficient. The cost of an adjacency matrix increases as \\(k^2\\). However, most sociomatrices are quite sparse, meaning that most entries in a sociomatrix are zero. We can capitalize on this by using a sparse-matrix representation. In social network analysis, this representation is called an edge list and it is much more efficient than storing relational data in matrix format.\nAn edgelist is simply a two-column matrix in which each row represents a (possibly directed) edge between the vertex listed in first column and the second column.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#community-structure",
    "href": "graphs.html#community-structure",
    "title": "4  Actual Graphs",
    "section": "4.7 Community Structure",
    "text": "4.7 Community Structure\nVarious algorithms for detecting clusters of similar vertices – i.e., “communities.” Use fastgreedy.community() to identify clusters in Kapferer’s tailor shop and color the vertices based on their membership.\n\nA &lt;- as.matrix(\n  read.table(file=\"data/kapferer-tailorshop1.txt\", \n             header=TRUE, row.names=1)\n  )\nG &lt;- graph.adjacency(A, mode=\"undirected\", diag=FALSE)\n\nWarning: `graph.adjacency()` was deprecated in igraph 2.0.0.\nℹ Please use `graph_from_adjacency_matrix()` instead.\n\nfg &lt;- fastgreedy.community(G)\n\nWarning: `fastgreedy.community()` was deprecated in igraph 2.0.0.\nℹ Please use `cluster_fast_greedy()` instead.\n\ncols &lt;- c(\"blue\",\"red\",\"black\",\"magenta\")\nplot(G, vertex.shape=\"none\",\n     vertex.label.cex=0.75, edge.color=grey(0.85), \n     edge.width=1, vertex.label.color=cols[fg$membership],\n     vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n# another approach to visualizing\nplot(fg,G,vertex.label=NA)\n\n\n\n\n\n\n\n\nfastgreedy.community() identified four clusters. These clusters are listed as numbers in fg$membership. We can then use this vector to index vertex colors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#graph-layouts",
    "href": "graphs.html#graph-layouts",
    "title": "4  Actual Graphs",
    "section": "4.8 Graph Layouts",
    "text": "4.8 Graph Layouts\n\n4.8.1 Force-Based Layouts\nThe two most common layouts are Fruchterman-Reingold and Kawai-Kamada.\nSometimes you don’t want a force-based layout. You may have noticed that the lattice we plotted when we introduced make_lattice() was a bit funky. This is because for a force-based layout, vertices on the periphery will have very different forces working on them than those in the center.\n\nTo get a proper lattice layout, specify that you want it on a grid\n\n\nplot(g3, vertex.color=\"skyblue2\", \n     layout=layout_on_grid(g3,10,10), vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.8.2 Making Graph Layouts “By Hand”\nThe layout is of any given plot is random (e.g., plot the same graph repeatedly and you’ll see that the layout changes with each plot). igraph provides a tool for tinkering with the layout called tkplot(). Call tkplot() and it will open an X11 window (on Macs at least). Select and drag the vertices into the layout you want, then use tkplot.getcoords(gid) to get the coordinates (where gid is the graph id returned when calling tkplot() on your graph).\n\n\n\n\ntkplot() window of triangle graph\n\n\n\n\ng &lt;- graph( c(1,2, 2,3, 1,3), n=3, dir=FALSE)\nplot(g, \n     vertex.color=\"skyblue2\", \n     vertex.frame.color=\"skyblue2\", vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n#tkplot(g)\n#tkplot.getcoords(1)\n### do some stuff with tkplot() and get coords which we call tri.coords\n## tkplot(g)\n## tkplot.getcoords(1) ## the plot id may be different depending on how many times you've called tkplot()\n##     [,1] [,2]\n##[1,]  228  416\n##[2,]  436    0\n##[3,]   20    0\ntri.coords &lt;- matrix( c(228,416, 436,0, 20,0), nr=3, nc=2, byrow=TRUE)\npar(mfrow=c(1,2))\nplot(g, vertex.color=\"skyblue2\",\n     vertex.frame.color=\"skyblue2\", \n     vertex.label.family=\"Helvetica\")\nplot(g, layout=tri.coords, \n     vertex.color=\"skyblue2\", \n     vertex.frame.color=\"skyblue2\", vertex.label.family=\"Helvetica\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#plotting-affiliation-graphs",
    "href": "graphs.html#plotting-affiliation-graphs",
    "title": "4  Actual Graphs",
    "section": "4.9 Plotting Affiliation Graphs",
    "text": "4.9 Plotting Affiliation Graphs\n\ndavismat &lt;- as.matrix(\n  read.table(file=\"data/davismat.txt\", \n            row.names=1, header=TRUE)\n  )\nsouthern &lt;- graph_from_incidence_matrix(davismat) \n\nWarning: `graph_from_incidence_matrix()` was deprecated in igraph 1.6.0.\nℹ Please use `graph_from_biadjacency_matrix()` instead.\n\nV(southern)$shape &lt;- c(rep(\"circle\",18), rep(\"square\",14))\nV(southern)$color &lt;- c(rep(\"blue\",18), rep(\"red\", 14))\nplot(southern, layout=layout.bipartite)\n\n\n\n\n\n\n\n## not so beautiful\n## did some tinkering using tkplot()...\nx &lt;- c(rep(23,18), rep(433,14))\ny &lt;- c(44.32432,   0.00000, 132.97297,  77.56757,  22.16216, 110.81081, 155.13514,\n       199.45946, 177.29730, 243.78378, 332.43243, 410.00000, 387.83784, 354.59459,\n       310.27027, 221.62162, 265.94595, 288.10811,   0.00000,  22.16216,  44.32432,\n       66.48649,  88.64865, 132.97297, 166.21622, 199.45946, 277.02703, 365.67568,\n       310.27027, 343.51351, 387.83784, 410.00000)\nsouthern.layout &lt;- cbind(x,y)\nplot(southern, layout=southern.layout, vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\n\nThe incidence matrix is \\(n \\times k\\), where \\(n\\) is the number of actors and \\(k\\) is the number of events\nProject the incidence matrix \\(X\\) into social space, creating a sociomatrix \\(A\\), \\(\\mathbf{A} = \\mathbf{X}\\, \\mathbf{X}^T\\)\nThis transforms the \\(n \\times k\\) into an \\(n \\times n\\) sociomatrix\n\n\n#Sociomatrix\n(f2f &lt;- davismat %*% t(davismat))\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n\ngf2f &lt;- graph_from_adjacency_matrix(f2f, mode=\"undirected\", diag=FALSE, add.rownames=TRUE)\ngf2f &lt;- simplify(gf2f)\nplot(gf2f, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n## who is the most central?\ncb &lt;- betweenness(gf2f)\n#plot(gf2f,vertex.size=cb*10, vertex.color=\"skyblue2\")\nplot(gf2f,vertex.label.cex=1+cb/2, vertex.shape=\"none\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\n\nProject the matrix into event space\n\n\n### this gives you the number of women at each event (diagonal) or mutually at 2 events\n(e2e &lt;- t(davismat) %*% davismat)\n\n    E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14\nE1   3  2  3  2  3  3  2  3  1   0   0   0   0   0\nE2   2  3  3  2  3  3  2  3  2   0   0   0   0   0\nE3   3  3  6  4  6  5  4  5  2   0   0   0   0   0\nE4   2  2  4  4  4  3  3  3  2   0   0   0   0   0\nE5   3  3  6  4  8  6  6  7  3   0   0   0   0   0\nE6   3  3  5  3  6  8  5  7  4   1   1   1   1   1\nE7   2  2  4  3  6  5 10  8  5   3   2   4   2   2\nE8   3  3  5  3  7  7  8 14  9   4   1   5   2   2\nE9   1  2  2  2  3  4  5  9 12   4   3   5   3   3\nE10  0  0  0  0  0  1  3  4  4   5   2   5   3   3\nE11  0  0  0  0  0  1  2  1  3   2   4   2   1   1\nE12  0  0  0  0  0  1  4  5  5   5   2   6   3   3\nE13  0  0  0  0  0  1  2  2  3   3   1   3   3   3\nE14  0  0  0  0  0  1  2  2  3   3   1   3   3   3\n\nge2e &lt;- graph_from_adjacency_matrix(e2e, mode=\"undirected\", diag=FALSE, add.rownames=TRUE)\nge2e &lt;- simplify(ge2e)\nplot(ge2e, vertex.size=20, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "odes.html#lotka-volterra-model",
    "href": "odes.html#lotka-volterra-model",
    "title": "5  ODEs in R",
    "section": "",
    "text": "5.1.1 Rosenzweig-MacArthur\nRosenzweig and MacArthur (1963) include two elements of ecological realism in their extension of the classic Lotka-Volterra model. First, they include density-dependence of the prey population. The growth of the prey population in the absence of the predator is no longer exponential, but is now a function of current size of the population, with a fixed upper limit to total prey population size. Second, the kill rate of predators is no longer linear. Predators have a functional response to prey abundance. In particular, the number of prey harvested by predators saturates, reflecting the eventual satiation of the predators. Here, I present slightly modified code for the Rosenzsweig-MacArthur model presented in Stevens (2009).\n\nrequire(deSolve)\n### Lotka-Volterra with Type II Functional Response\n# Rosenzweig & MacArthur (1963) model\npredpreyRM &lt;- function(t, y, p) {\n  H &lt;- y[1]\n  P &lt;- y[2]\n  with(as.list(p), {\n    dH &lt;- b*H * (1 - alpha*H) - w*P*H/(D+H)\n    dP &lt;- e*w*P*H/(D+H) - s*P\n    return(list(c(dH, dP)))\n  }) \n}\nb &lt;- 0.8\ne &lt;- 0.07\ns &lt;- 0.2\nw &lt;- 5\nD &lt;- 400\nalpha &lt;- 0.001\nH &lt;- 0:(1/alpha)\n\n\nHiso &lt;- expression(b/w * (D + (1 - alpha * D) * H - alpha * H^2))\nHisoStable &lt;- eval(Hiso)\n\np.RM &lt;- c(b = b, alpha = alpha, e = e, s = s, w = w, D = D)\ntmax &lt;- 150\ntimes &lt;- seq(0,tmax,by=0.1)\nRM1 &lt;- as.data.frame(ode(c(900, 120), times, predpreyRM, p.RM))\ncolnames(RM1) &lt;- c(\"time\",\"prey\",\"predator\")\n\nplot(RM1[,\"time\"], RM1[,\"prey\"], type=\"l\", lwd=2, col=\"blue\", xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Population Size\",\n     ylim=c(0,900))\nlines(RM1[,\"time\"], RM1[,\"predator\"], col=\"orange\", lwd=2)\nlegend(\"topright\",c(\"Predator\",\"Prey\"), col=c(\"orange\",\"blue\"),lwd=2)\n\n\n\n\n\n\n\n\nNo more cycles!\nI have created a shiny app that allows you to interact with the Lotka-Volterra model here. Using the model of Hastings and Powell (1991), I’ve made another shiny app that shows how a model with just three trophic levels, parameterized with realistic values for the parameters, can display chaotic dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#sir-model",
    "href": "odes.html#sir-model",
    "title": "5  ODEs in R",
    "section": "5.2 SIR Model",
    "text": "5.2 SIR Model\nWe start with a simple Susceptible-Infeced-Recovered (SIR) epidemic. The SIR epidemic is a system of three coupled ODEs.\n\nSimple model for a closed-population (i.e., no births or deaths)\nNeed to write a function that encodes the system of equations\nThe function takes three arguments t, x, and parms\n\nthese are the time over which the equations are integrated, the state values (i.e., S,I, and R), and the model parameters\n\nThe function starts by renaming the elements of the state vector x as things that make the equations easier to read – e.g., I instead of x[2]\nThe line with(as.list(parms) can take some unpacking\n\nUsing with() means setting up a local scope for variables\nas.list() coerces our vector of parameters into a list\nthese two elements allow us to write the equations in a simple and readable way\nNote we don’t have to say something like parms[\"beta\"] or parms[3] in order to use that parameter in our equation\n\n\n\nrequire(deSolve)\nsir &lt;- function(t,x,parms){\n    S &lt;- x[1]\n    I &lt;- x[2]\n    R &lt;- x[3]\n  with(as.list(parms),\n{\n    dS &lt;- -beta*S*I\n    dI &lt;- beta*S*I - nu*I\n    dR &lt;- nu*I\n    res &lt;- c(dS,dI,dR)\n  list(res)\n})\n}\n\n\nIn order to integrate the equations, use the function lsoda() (which is the solver we use) or ode() (which is a wrapper for different types of solvers including lsoda)\nWe pass the solver the initial state vector, the times, the name of our function that describes the system of equations, and the vector of parameters\nThe solver will return the solutions as a matrix; we coerce this using data.frame() to make it easier to work with, plot, etc.\nOnce we have the data frame, we name the columns to make them easier to refer to\n\n\nN &lt;- 1e4\nparms &lt;- c(N=N,beta=0.0001, nu = 1/2.5)\ntimes &lt;- seq(0,50,0.1)\nx0 &lt;- c(N-1,1,0)\nstateMatrix &lt;- as.data.frame(lsoda(x0,times,sir,parms))\n\ncolnames(stateMatrix) &lt;- c(\"time\",\"S\",\"I\",\"R\")\nplot(stateMatrix[,\"time\"], stateMatrix[,\"S\"], type=\"l\", lwd=2, col=\"blue\",\n     ylim=c(0,1e4),\n     xlab=\"Time\", ylab=\"Population Size\")\nlines(stateMatrix[,\"time\"], stateMatrix[,\"I\"], col=\"red\", lwd=2)\nlines(stateMatrix[,\"time\"], stateMatrix[,\"R\"], col=\"green\", lwd=2)\nlegend(\"right\", c(\"S\",\"I\",\"R\"), col=c(\"blue\",\"red\",\"green\"), lwd=2)\n\n\n\n\n\n\n\n\nPlot the phase portrait.\n\nplot(stateMatrix[,\"S\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Susceptible\", ylab=\"Infected\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#measles-model",
    "href": "odes.html#measles-model",
    "title": "5  ODEs in R",
    "section": "5.3 Measles Model",
    "text": "5.3 Measles Model\n\n\n\nState diagram for the SEIR model.\n\n\n\nSusceptible, Exposed, Infected, Recovered (SEIR) model\nUse parameterization from Ottar Bjornstad (a.k.a., “The Measles Man”)\nOpen population of constant size (birth rate = death rate (\\(\\mu\\)))\nInclude vaccinated fraction \\(p\\)\nModel is a damped oscillator\n\nBased on this parameterization, what is the life expectancy of individuals in the population? How long is the latent period? How long are cases infectious?\n\nseir &lt;- function(t,x,parms){\n    S &lt;- x[1]\n    E &lt;- x[2]\n    I &lt;- x[3]\n    R &lt;- x[4]\n  with(as.list(parms),{\n    dS &lt;- mu*(N*(1-p)-S) - beta*S*I/N\n    dE &lt;- beta*S*I/N - (mu + sigma)*E\n    dI &lt;- sigma*E-(mu+gamma)*I\n    dR &lt;- gamma*I-mu*R+ mu*N*p\n    res &lt;- c(dS,dE,dI,dR)\n    list(res)\n  })\n}\n\ntimes &lt;-  seq(0, 30, by = 1/52)\nparms &lt;-  c(mu = 1/75, N = 1, p = 0, beta = 1250, sigma = 365/7, gamma = 365/7)\nxstart = c(S = 0.06, E = 0, I = 0.001, R = 0)\nstateMatrix &lt;-  as.data.frame(lsoda(xstart, times, seir, parms))\n##\ncolnames(stateMatrix) &lt;- c(\"time\",\"S\",\"E\", \"I\",\"R\")\nplot(stateMatrix[,\"time\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Time\", ylab=\"Fraction Infected\")\n\n\n\n\n\n\n\n\nSpiralize! (i.e., plot the phase portrait)\n\nplot(stateMatrix[,\"S\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Susceptible\", ylab=\"Infected\")\n\n\n\n\n\n\n\n\nCalculate the power spectrum. We’ll trim the plot for all periods greater than 2.5 yrs, since the power is essentially zero for all such periods.\n\nspec &lt;- spectrum(stateMatrix$I, log=\"no\", spans=c(2,2), plot=FALSE)\ndelta &lt;- 1/52\nplot(spec$freq[1:84]/delta, 2*spec$spec[1:84], type=\"l\", lwd=2, col=\"red\", xlab=\"Period\", ylab=\"Spectrum\")\n\n\n\n\n\n\n\nfmax &lt;- which(spec$spec==max(spec$spec))\n1/spec$freq[fmax]\n\n[1] 123.0769",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#lorenz-attractor",
    "href": "odes.html#lorenz-attractor",
    "title": "5  ODEs in R",
    "section": "5.4 Lorenz Attractor",
    "text": "5.4 Lorenz Attractor\nThe Lorenz Attractor is a classic model for dynamical systems and I include it to give you another example of numerically integrating a system of equations in deSolve. It certainly makes for cool phase portraits.\n\nlorenz &lt;- function(t, state, p) {\n    with(as.list(c(state, parms)), {\n\n        dx &lt;- sigma*(y - x)\n        dy &lt;- x*(rho - z) - y\n        dz &lt;- x*y - beta*z\n\n        list(c(dx, dy, dz))\n    })\n}\n\nparms &lt;- c(sigma=10, beta=8/3, rho=28)\ny0 &lt;- c(x = 1, y = 1, z = 1)\ny0p &lt;- y0 + c(1e-6, 0, 0)\ntimes &lt;- seq(0, 100, 0.01)\n\nout &lt;- ode(y = y0, times = times, func = lorenz, parms = parms)\nout2 &lt;- ode(y = y0p, times = times, func = lorenz, parms = parms)\n\nplot(out[,\"x\"], out[,\"y\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\nplot(out[,\"x\"], out[,\"z\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"x\", ylab = \"z\")\n\n\n\n\n\n\n\nplot(out[,\"y\"], out[,\"z\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"y\", ylab = \"z\")\n\n\n\n\n\n\n\n\nEven cooler than the static phase-portraits, we can animate the dynamics of the model on the underlying attractor. We create the animation using the animation package. Here’s an example of its usage:\n\nlibrary(animation)\nsaveVideo({\n    ani.options(interval = 0.05)\n    for (i in seq(2,10000,by=2)) {\n    plot(out[,\"x\"], out[,\"z\"], type = \"l\", col=grey(0.85),\n         xlim = c(-20, 20), ylim = c(0, 50), xlab=\"x\", ylab=\"z\")\n    points(out[i,\"x\"], out[i,\"z\"], pch=19, col=ifelse(out[i,\"x\"]&lt;0,\"red\",\"blue\"))}\n}, video.name = \"lorenz1.mp4\", other.opts = \"-pix_fmt yuv420p -b 300k\")\n\n\nWhen the \\(x\\) and \\(z\\) variables are positively correlated, the point is colored blue. When they are negatively correlated, the point is colored red. If you were to measure this system, whether you saw a positive or a negative correlation between \\(x\\) and \\(z\\) would depend on where in the attractor the system was. Different researchers studying the system at different times could get very different ideas about the system if they didn’t understand the underlying attractor. This is an important lesson from complex systems.\n\n\n\n\nHastings, Alan, and Thomas Powell. 1991. “Chaos in a Three-Species Food Chain.” Ecology 72 (3): 896–903. https://doi.org/10.2307/1940591.\n\n\nRosenzweig, M. L., and R. H. MacArthur. 1963. “Graphical Representation and Stability Conditions of Predator-Prey Interactions.” The American Naturalist 97 (895): 209–23. https://doi.org/10.1086/282272.\n\n\nStevens, M. Henry. 2009. A Primer of Ecology with R. New York: Springer. https://doi.org/10.1007/978-0-387-89882-7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baalen, M. van, and M. W. Sabelis. 1995. “The Dynamics of Multiple\nInfection and the Evolution of Virulence.” American\nNaturalist 146 (6): 881–910. http://www.jstor.org/stable/2463102.\n\n\nBelovsky, G. E. 1987. “Hunter-Gatherer Foraging: A Linear\nProgramming Approach.” Journal of Anthropological\nArchaeology 6 (1): 29–76. https://doi.org/10.1016/0278-4165(87)90016-X.\n\n\nCharnov, Eric L. 1976. “Optimal Foraging, the Marginal Value\nTheorem.” Theoretical Population Biology 9 (2): 129–36.\nhttps://doi.org/10.1016/0040-5809(76)90040-X.\n\n\n———. 1997. “Trade-Off-Invariant Rules for Evolutionary Stable Life\nHistories.” Nature 387 (6631): 393–94. https://doi.org/10.1038/387393a0.\n\n\nEfferson, Charles, Sonja Vogt, and Ernst Fehr. 2020. “The Promise\nand the Peril of Using Social Influence to Reverse Harmful\nTraditions.” Nature Human Behaviour 4 (1): 55–68. https://doi.org/10.1038/s41562-019-0768-2.\n\n\nGadgil, Madhav, and William H. Bossert. 1970. “Life Historical\nConsequences of Natural Selection.” The American\nNaturalist 104 (935): 1–24. http://www.jstor.org/stable/2459070.\n\n\nHastings, Alan, and Thomas Powell. 1991. “Chaos in a Three-Species\nFood Chain.” Ecology 72 (3): 896–903. https://doi.org/10.2307/1940591.\n\n\nHenrich, Joseph. 2004. “Demography and Cultural Evolution: How\nAdaptive Cultural Processes Can Produce Maladaptive Losses: The\nTasmanian Case.” American Antiquity 69 (2):\n197–214. https://doi.org/10.2307/4128416.\n\n\nHolt, R. D., A. P. Dobson, M. Begon, R. G. Bowers, and E. M. Schauber.\n2003. “Parasite Establishment in Host Communities.”\nEcology Letters 6 (9): 837–42. https://doi.org/10.1046/j.1461-0248.2003.00501.x.\n\n\nJones, J. H. 2009. “The Force of Selection on the Human Life\nCycle.” Evolution and Human Behavior 30 (5): 305–14. https://doi.org/10.1016/j.evolhumbehav.2009.01.005.\n\n\nLeslie, P., and B. Winterhalder. 2002. “Demographic Consequences\nof Unpredictability in Fertility Outcomes.” American Journal\nof Human Biology 14 (2): 168–83. https://doi.org/10.1002/ajhb.10044.\n\n\nLevins, R. 1968. Evolution in Changing Environments. Princeton:\nPrinceton University Press.\n\n\nLevins, Richard. 1962. “Theory of Fitness in a Heterogeneous\nEnvironment. I. The Fitness Set and Adaptive\nFunction.” The American Naturalist 96 (891): 361–73. http://www.jstor.org/stable/2458725.\n\n\nMay, R. M. 1976. “Simple Mathematical-Models with Very Complicated\nDynamics.” Nature 261 (5560): 459–67. https://doi.org/10.1038/261459a0.\n\n\nNair, Jayakrishnan, Adam Wierman, and Bert Zwart. 2022. The\nFundamentals of Heavy Tails: Properties, Emergence, and Estimation.\nCambridge Series in Statistical and Probabilistic Mathematics.\nCambridge: Cambridge University Press.\n\n\nNoy-Meir, Imanuel. 1975. “Stability of Grazing Systems: An\nApplication of Predator-Prey Graphs.” Journal of Ecology\n63 (2): 459–81. https://doi.org/10.2307/2258730.\n\n\nOrians, Gordon H. 1969. “On the Evolution of Mating Systems in\nBirds and Mammals.” The American Naturalist 103 (934):\n589–603. https://doi.org/10.1086/282628.\n\n\nParker, G. A., and R. A. Stuart. 1976. “Animal Behavior as a\nStrategy Optimizer: Evolution of Resource Assessment Strategies and\nOptimal Emigration Thresholds.” American Naturalist 110\n(976): 1055–76. https://doi.org/10.1086/283126.\n\n\nRogers, A. R. 1988. “Does Biology Constrain Culture?”\nAmerican Anthropologist 90 (4): 819–31. https://doi.org/10.1525/aa.1988.90.4.02a00030.\n\n\nRogers, E. M. 2003. Diffusion of Innovations. 5th ed. New York:\nFree Press.\n\n\nRosenzweig, M. L., and R. H. MacArthur. 1963. “Graphical\nRepresentation and Stability Conditions of Predator-Prey\nInteractions.” The American Naturalist 97 (895): 209–23.\nhttps://doi.org/10.1086/282272.\n\n\nScheffer, M. 2009. Critical Transitions in Nature and Society.\nPrinceton: Princeton University Press.\n\n\nSmith, C. C., and S. D. Fretwell. 1974. “The Optimal Balance\nBetween Size and Number of Offspring.” American\nNaturalist 108: 499–506. https://www.jstor.org/stable/2459681.\n\n\nStevens, M. Henry. 2009. A Primer of Ecology with\nR. New York: Springer. https://doi.org/10.1007/978-0-387-89882-7.\n\n\nWeitzman, M. L. 2009. “On Modeling and Interpreting the Economics\nof Catastrophic Climate Change.” The Review of Economics and\nStatistics XCI (1): 1–19. https://doi.org/10.1162/rest.91.1.1\n.\n\n\nZeeman, E. C. 1976. “Catastrophe Theory.” Scientific\nAmerican 234 (4): 65–65 &. https://doi.org/10.1038/scientificamerican0476-65.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "gettingR.html",
    "href": "gettingR.html",
    "title": "1  Getting Started in R",
    "section": "",
    "text": "1.1 Setting up R and RStudio\nThese notes will help get you started with R. They are mostly quite non-graphical, focusing instead on the basics of the R language.\nTo get started, you need to do two things:\nR is the software. RStudio is what is known as an Integrated Development Environment (IDE). Basically, an IDE is a way of running base software the provides various tools to make the coding experience easier (e.g., an editor with syntax highlighting, debugger, and interactive graphics facilities).\nHere is a brief video explaining what to do:\nWhat follows are some old notes that are very Base-R focused. The thing is, even though most people use the tidyverse tools these days, it’s still valuable to understand Base-R (since that’s what everything else is built upon!). This is particularly true for the material that forms the bulk of these notes. Making theoretical scientific figures (chapter Chapter 3) is simply easier in Base-R. You typically don’t need to do complex data wrangling to make a theoretical figure, so the powerful data-manipulation tools of dplyr, for instance, are unnecessary. Sometimes, we want to utilize the grid package, which underlies the tidyverse graphics library ggplot2, directly. Again, I find this more straightforward in Base-R.\nThe igraph package for drawing graphs (a.k.a., “networks”) that we discuss in chapter Chapter 4 also runs in Base-R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#setting-up-r-and-rstudio",
    "href": "gettingR.html#setting-up-r-and-rstudio",
    "title": "1  Getting Started in R",
    "section": "",
    "text": "Download R CRAN\nDownload and Install RStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#what-is-r",
    "href": "gettingR.html#what-is-r",
    "title": "1  Getting Started in R",
    "section": "1.2 What Is R?",
    "text": "1.2 What Is R?\n\nR is statistical numerical software\nR is a “dialect” of the S statistical programming language\nR is a system for interactive data analysis\nR is a high-level programming language\nR is free\nR is state-of-the-art in statistical computing. It is what many (most?) research statisticians use in their work",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#why-use-r",
    "href": "gettingR.html#why-use-r",
    "title": "1  Getting Started in R",
    "section": "1.3 Why Use R?",
    "text": "1.3 Why Use R?\n\nR is FREE! That, by itself, is almost enough. No complicated licensing. Broad dissemination of research methodologies and results, etc.\nR is available for a variety of computer platforms (e.g., Linux, MacOS, Windows).\nR is widely used by professional statisticians, social scientists, biologists, demographers, and other scientists. This increases the likelihood that code will exist to do a calculation you might want to do.\nR has remarkable online help lists, tutorials, etc.\nR represents the state-of-the-art in statistical computing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#wouldnt-something-menu-driven-be-easier",
    "href": "gettingR.html#wouldnt-something-menu-driven-be-easier",
    "title": "1  Getting Started in R",
    "section": "1.4 Wouldn’t Something Menu-Driven Be Easier?",
    "text": "1.4 Wouldn’t Something Menu-Driven Be Easier?\n\nFallacious thinking\n\nFor teaching, text-based input is always better\nExample code can be copied and input exactly; you can then tweak it and see what happens, facilitating the learning process\n\nAn example\nWhat follows is a pretty complicated graph of the grooming interactions of a group of rhesus monkeys, Macaca mulatta, observed by Sade (1972)\nWith the code I used to generate this graph, you can recreate the figure exactly. Try it! It doesn’t matter if you have no idea what you’re doing yet. That’s the point.\nThe only thing you need to make this figure is to install and load the library igraph",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#rhesus-money-grooming-network",
    "href": "gettingR.html#rhesus-money-grooming-network",
    "title": "1  Getting Started in R",
    "section": "1.5 Rhesus Money Grooming Network",
    "text": "1.5 Rhesus Money Grooming Network\n\nrequire(igraph)\nrhesus &lt;- read.table(\"./data/sade1.txt\", skip=1, header=FALSE)\nrhesus &lt;- as.matrix(rhesus)\nnms &lt;- c(\"066\", \"R006\", \"CN\", \"ER\", \"CY\", \"EC\", \"EZ\", \"004\", \"065\", \"022\", \"076\", \n         \"AC\", \"EK\", \"DL\", \"KD\", \"KE\")\nsex &lt;- c(rep(\"M\",7), rep(\"F\",9))\ndimnames(rhesus)[[1]] &lt;- nms\ndimnames(rhesus)[[2]] &lt;- nms\ngrhesus &lt;- graph_from_adjacency_matrix(rhesus, weighted=TRUE)\nV(grhesus)$sex &lt;- sex\n\nrhesus.layout &lt;- layout.kamada.kawai(grhesus)\nplot(grhesus, \n     edge.width=log10(E(grhesus)$weight)+1, \n     edge.arrow.width=0.5,\n     vertex.label=V(grhesus)$name,\n     vertex.label.family=\"Helvetica\",\n     vertex.color=as.numeric(V(grhesus)$sex==\"F\")+5, \n     layout=rhesus.layout)\n\n\n\n\nRhesus monkey grooming network (Slade 1972).\n\n\n\n\n\nA note on loading data: the above code loads a data file apparently called \"./data/sade1.txt\"\nWhat does that mean?\nThe one dot followed by a slash, ./, means to go into the sub-directory called data, which is in our current working directory, and read the text file called sade1.txt\nIf the data sub-directory was actually in, say, the same directory where our working directory is located (i.e., they were two sub-directories of the same higher-level folder), we would use two dots, ../, which means to go out one directory in the hierarchy\nThis is actually not R but the underlying OS file system\nLearning about the file/directory structure of your computer is actually an important (and under-appreciated) data-science skill\nCheck out the fantastic MIT course The Missing Semester of Your CS Education for information on various tools that can really improve your workflows and general skill-level",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#a-few-conventions-and-other-helpful-bits",
    "href": "gettingR.html#a-few-conventions-and-other-helpful-bits",
    "title": "1  Getting Started in R",
    "section": "1.6 A Few Conventions and Other Helpful Bits",
    "text": "1.6 A Few Conventions and Other Helpful Bits\n\nThere are some things that you will see over and over in the code embedded in this document\nThe assignment operator &lt;- is used to assign a value to a name.\nThe value is on the right-hand side of the operator and the name is on the left side\nYou can use = for assignment, but I don’t recommend it (it doesn’t work at all levels, makes the code harder to read, etc.)\nDifferent environments make it more or less easy to use &lt;-. In RStudio, hit the option key and the minus sign simultaneously\nComments are marked by #: anything following the hash will be ignored by R\nUse comments liberally to help you (and others) understand your code\nIn these notes, the output that you would see on your own command line will be white following a grey box (the input). Frequently, it will begin with a [1], which indicates the first element of a vector\nSometimes I enclose a command in parentheses; this is simply to force R to echo the output (for pedagogical purposes)\n\n\n# a comment\nx &lt;- c(1,2,3)\n(y &lt;- c(4,5,6))\n\n[1] 4 5 6\n\n\n\nYou will probably want to seek help on functions. At the command line, simply type a question mark followed immediately by the function you want to query, ?function.name\n\nIn Rstudio, you may be given auto-complete suggestions, which you can click on to save typing\n\nWhen you are done with your R session, type q() at the command line\nR will ask you if you want to save your workspace. For now, you probably don’t.\nCheck out the much more comprehensive Introduction to R for all the language details.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#r-as-a-calculator",
    "href": "gettingR.html#r-as-a-calculator",
    "title": "1  Getting Started in R",
    "section": "1.7 R as a Calculator",
    "text": "1.7 R as a Calculator\n\n# addition\n2+2\n\n[1] 4\n\n# multiplication\n2*3\n\n[1] 6\n\na &lt;- 2\nb &lt;- 3\na*b\n\n[1] 6\n\n# division\n2/3\n\n[1] 0.6666667\n\nb/a\n\n[1] 1.5\n\n1/b/a\n\n[1] 0.1666667\n\n# note order of operations!\n1/(b/a)\n\n[1] 0.6666667\n\n# parentheses can override order of operations\n# an exponential\nexp(-2)\n\n[1] 0.1353353\n\n# why we age\nr &lt;- 0.02\nexp(-r*45)\n\n[1] 0.4065697\n\n# something more tricky\nexp(log(2))\n\n[1] 2\n\n# generate 20 normally distributed random numbers\nrnorm(20)\n\n [1]  1.749470956  0.916718564 -0.178373474 -0.327876015  0.192697504\n [6]  0.060440148  1.108932911  0.036229325 -0.967395410 -0.858294855\n[11]  1.070746372 -1.077977391  0.795229962 -1.637697198  0.259469862\n[16] -0.007341621  0.358928489  1.288766001  0.408694783 -0.468418438",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#data-types",
    "href": "gettingR.html#data-types",
    "title": "1  Getting Started in R",
    "section": "1.8 Data Types",
    "text": "1.8 Data Types\n\nNumeric\n\nAll numbers in R are of the form double (i.e., double-precision floating point numbers). This can be a bit confusing for people who are used to languages with integer data types (like, most languages!). Entering something that looks like an integer doesn’t mean it is.\n\n\n\n# it looks like an integer, but don't be fooled!\na &lt;- 2\nis.numeric(a)\n\n[1] TRUE\n\nis.integer(a)\n\n[1] FALSE\n\nis.double(a)\n\n[1] TRUE\n\n\n\nInteger\n\nOK, technically R does have an integer class, but it is used very rarely and many functions will convert integers into doubles anyway. If you really must have an integer (e.g., because you are passing output to external C or FORTRAN code that expects it), add the suffix L to the entered number.\n\n\n\na &lt;- 2L\nis.integer(a)\n\n[1] TRUE\n\n\n\nCharacter\n\nStrings are represented by the character data class.\n\n\n\n(countries &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\n\n[1] \"Uganda\"   \"Tanzania\" \"Kenya\"    \"Rwanda\"  \n\nas.character(1:5)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\n\nFactor\n\nFactors are a data type for encoding categorical data. Notice that factors are printed without the quotes. This is because R stores them as a set of codes. Data of type “factor” are different from data of type “character” (which is what plain text is). Note the difference below between factor and character data. Because factors get used in statistical models, they are actually represented as numbers (the levels) that have associated names. Vectors, on the other hand, are just lists of numbers.\n\n\n\ncountries &lt;- factor(c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\ncountries\n\n[1] Uganda   Tanzania Kenya    Rwanda  \nLevels: Kenya Rwanda Tanzania Uganda\n\n# a trick to get some insight into how factors are handled by R\nunclass(countries)\n\n[1] 4 3 1 2\nattr(,\"levels\")\n[1] \"Kenya\"    \"Rwanda\"   \"Tanzania\" \"Uganda\"  \n\ncountries1 &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\")\ncountries1 == unclass(countries1)\n\n[1] TRUE TRUE TRUE TRUE\n\ncountries == unclass(countries)\n\n[1] FALSE FALSE FALSE FALSE\n\n\n\nLogical\n\nTRUE and FALSE are reserved keywords, while T and F are global constants set to these. These logical variables are essential tools for subsetting data. You also use them extensively in setting optional arguments of functions.\n\n\n\nt.or.f &lt;- c(T,F,F,T,T)\nis.logical(t.or.f)\n\n[1] TRUE\n\naaa &lt;- c(1,2,3,4,5)\n# subset\naaa[t.or.f]\n\n[1] 1 4 5\n\n\n\nList\n\nYou can mix different types of data in a list using the command list(). This is useful when you write your own functions and want to output multiple things. Use the function str() to give you information about a list.\n\n\n\nchild1 &lt;- list(name=\"mary\", child.age=6,\nstatus=\"foster\",mother.alive=F, father.alive=T, parents.ages=c(24,35))\nstr(child1)\n\nList of 6\n $ name        : chr \"mary\"\n $ child.age   : num 6\n $ status      : chr \"foster\"\n $ mother.alive: logi FALSE\n $ father.alive: logi TRUE\n $ parents.ages: num [1:2] 24 35",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#coercion",
    "href": "gettingR.html#coercion",
    "title": "1  Getting Started in R",
    "section": "1.9 Coercion",
    "text": "1.9 Coercion\n\nSometimes you have data in one type but need it in a different type\nR provides a variety of methods to coerce data from one type to another\nThese methods are carried out by functions that begin with as.xxx, where xxx is the data type to which you are coercing\n\n\ncountries &lt;- factor(c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\nas.character(countries)\n\n[1] \"Uganda\"   \"Tanzania\" \"Kenya\"    \"Rwanda\"  \n\nas.numeric(countries)\n\n[1] 4 3 1 2\n\n# werk it backwards\ncountries1 &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\")\nas.factor(countries1)\n\n[1] Uganda   Tanzania Kenya    Rwanda  \nLevels: Kenya Rwanda Tanzania Uganda\n\n# sometimes you want your numbers to actually be strings (e.g., when you make labels or column names)\nas.character(1:5)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n# there actually is an integer class; it just doesn't get used much at all\na &lt;- 2\nis.integer(a)\n\n[1] FALSE\n\nis.integer(as.integer(a))\n\n[1] TRUE\n\n\n\nYou can check the class of an object using functions that begin with is.xxx, where xxx is the data type you are querying (like is.integer() above)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#creating-vectors",
    "href": "gettingR.html#creating-vectors",
    "title": "1  Getting Started in R",
    "section": "1.10 Creating Vectors",
    "text": "1.10 Creating Vectors\n\nA vector is a list of numbers – it turns out everything in R is represented as a vector but that doesn’t affect your life much.\nIn order to create a vector, you use the the function c(), which concatenates a list of items (hence the “c”).\nYou will use this a lot and it’s a super-common mistake to forget the c() when putting together a list of numbers, factors, etc.\nIf you do forget it, you will get a syntax error\nOften we want either regularly spaced vectors or a vector of one value repeated. R has a number of facilities to perform these operations.\n\n\n( manual &lt;- c(1,3,5,7,9))\n\n[1] 1 3 5 7 9\n\n( count &lt;- 1:20 )\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n( ages &lt;- seq(0,85,by=5) )\n\n [1]  0  5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85\n\n( ones &lt;- rep(1,10) )\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n( fourages &lt;- rep(c(1,2),c(5,10)) )\n\n [1] 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n\n( equalspace &lt;- seq(1,5, length=20) )\n\n [1] 1.000000 1.210526 1.421053 1.631579 1.842105 2.052632 2.263158 2.473684\n [9] 2.684211 2.894737 3.105263 3.315789 3.526316 3.736842 3.947368 4.157895\n[17] 4.368421 4.578947 4.789474 5.000000\n\n\n\nYou can use rep() to repeat values.\nSometimes this can be tricky: the second argument tells R how many repetitions.\nThis argument can be a vector and this, along with the possibility of a vector of the items you want repeated too, allows you to create quite complex patterns very easily.\n\n\nrep(2,10)\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\nrep(c(1,2),10)\n\n [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n\nrep(c(1,2), c(5,10))\n\n [1] 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n\nrep(\"R roolz!\", 3)\n\n[1] \"R roolz!\" \"R roolz!\" \"R roolz!\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#creating-matrices",
    "href": "gettingR.html#creating-matrices",
    "title": "1  Getting Started in R",
    "section": "1.11 Creating Matrices",
    "text": "1.11 Creating Matrices\n\nAs we said, a vector is a list of numbers\nA matrix is a rectangular array of numbers – it is a vector of vectors, with the numbers indexed by row and column.\nOne way to create matrices is to “bind” columns together using the commands cbind() or rbind().\n\n\n# age distribution of Gombe chimps in 1980 and 1986\ncx1980 &lt;- c(7, 13, 8, 13, 5, 35, 9)\ncx1988 &lt;- c(9, 11, 15, 8, 9, 38, 0)\n( C &lt;- cbind(cx1980, cx1988) )\n\n     cx1980 cx1988\n[1,]      7      9\n[2,]     13     11\n[3,]      8     15\n[4,]     13      8\n[5,]      5      9\n[6,]     35     38\n[7,]      9      0\n\n# another way\nC &lt;- c(cx1980, cx1988)\n(  C &lt;- matrix(C, nrow=7, ncol=2) )\n\n     [,1] [,2]\n[1,]    7    9\n[2,]   13   11\n[3,]    8   15\n[4,]   13    8\n[5,]    5    9\n[6,]   35   38\n[7,]    9    0\n\n\n\nWhat happens if we try to bind columns of different lengths?\n\n\n# age distribution at Tai; Boesch uses fewer age classes\ncxboesch &lt;- c(18,10,15,30)\n( C &lt;- cbind(C,cxboesch) )\n\nWarning in cbind(C, cxboesch): number of rows of result is not a multiple of\nvector length (arg 2)\n\n\n           cxboesch\n[1,]  7  9       18\n[2,] 13 11       10\n[3,]  8 15       15\n[4,] 13  8       30\n[5,]  5  9       18\n[6,] 35 38       10\n[7,]  9  0       15\n\n\n\nBoth the warning message and the output can seem a little odd to the uninitiated\nR uses a recycling rule for filling out vectors and matrices\nWhen you try to put together things that are neither the same length nor multiples of each other, you get a warning\nWe can use the recycling rule to make a matrix of ones:\n\n\n( X &lt;- matrix(1,nr=3,nc=3) )\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n\n\nNote that using the short version of nrow, nr, is sufficient. This is often true – you can use the minimum name that is unambiguous.\nThe matrix() command requires at least 3 arguments: (1) a vector of numbers that will form the elements of the matrix, (2) the number of rows, and (3) the number of columns.\nFor small matrices, you might want to enter the vectors of values manually\nIf you do this, it’s important to know that R fills matrices column-wise (the standard for FORTRAN and definitely not the way most people actually work!).\nUse the optional argument byrow=TRUE to make R read in the data row-wise\n\n\n# cross-classified data on hair/eye color \nfreq &lt;- c(32,11,10,3,  38,50,25,15,  10,10,7,7,  3,30,5,8)\nhair &lt;- c(\"Black\", \"Brown\", \"Red\", \"Blond\")\neyes &lt;- c(\"Brown\", \"Blue\", \"Hazel\", \"Green\")\nfreqmat &lt;- matrix(freq, nr=4, nc=4, byrow=TRUE)\ndimnames(freqmat)[[1]] &lt;- hair\ndimnames(freqmat)[[2]] &lt;- eyes\nfreqmat\n\n      Brown Blue Hazel Green\nBlack    32   11    10     3\nBrown    38   50    25    15\nRed      10   10     7     7\nBlond     3   30     5     8\n\n# might as well do something with it\nmosaicplot(freqmat)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#data-frames",
    "href": "gettingR.html#data-frames",
    "title": "1  Getting Started in R",
    "section": "1.12 Data Frames",
    "text": "1.12 Data Frames\n\nA data frame is an R object which stores a data matrix. A data frame is essentially a list of variables which are all the same length. A single data frame can hold different types of variables.\nTo access a variable contained in a data frame, use the data frame name followed by the variable name, separated by a dollar sign, $.\n\n\n# five columns of data\nsatu &lt;- c(1,2,3,4,5)\ndua &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\ntiga &lt;- sample(c(TRUE,FALSE),5,replace=TRUE)\nempat &lt;- LETTERS[7:11]\nlima &lt;- rnorm(5)\n# construct a data frame\n(collection &lt;- data.frame(satu,dua,tiga,empat,lima))\n\n  satu dua  tiga empat        lima\n1    1   a FALSE     G -0.24012272\n2    2   b  TRUE     H  1.62867872\n3    3   c FALSE     I  1.34427662\n4    4   d  TRUE     J  0.04494688\n5    5   e FALSE     K  0.11910426\n\n# extract the third variable\ncollection$tiga\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n\n\nby default, data.frame() will produce row numbers (seen to the left of the first column in the data frame collection above)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#directories-and-paths",
    "href": "gettingR.html#directories-and-paths",
    "title": "1  Getting Started in R",
    "section": "1.13 Directories and Paths",
    "text": "1.13 Directories and Paths\n\nR uses a working directory. The default can be set in the Preferences or using an initialization file (i.e., a file that is always read when R starts up).\nIf you read in a file without specifying a path, R will search in the working directory; if there is no file matching the name you provide, you receive an error message\nWe can query the working directory using the command getwd() and we can change it using setwd()\nYou can always load a file by giving either a full or relative path\n\n\ngetwd()\n\n[1] \"/Users/jhj1/Teaching/graphics\"\n\n#setwd(\"/Users/jhj1/Projects/git/AABA2023_Workshop/Markdown\")\n## can't actually change it because it screws up the rendering!\n\n\nSetting the working directory is actually not recommended\nIt is not a good scientific practice that favors replicability/interoperability/etc.\nIt’s generally better to use R Projects in RStudio (as we do in this workshop)\nTo start an R Project, either double-click on the .RProj file in the project’s directory or clicking on the R Project menu button in the upper right corner of your RStudio frame\nTo share the work you have done in an R Project with collaborators, students, or scientists looking to replicate your work, simply share the folder containing the .RProj file\nWhen you quit R, you will be asked if you want to save your R session\nIf a session has previously been saved in your working directory, there will be a copy of the workspace in the R binary format named .RData\nWhen R is started in a particular directory, if there is an .RData file in that directory, it will load automatically\nThis can lead to some surprising behavior if you don’t know that it can happen\nAutomatically saving and loading workspaces is also not recommended\nBest scientific practice involves constructing your workspace using broadly-interoperable data formats (e.g., .csv files) and scripts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#reading-files",
    "href": "gettingR.html#reading-files",
    "title": "1  Getting Started in R",
    "section": "1.14 Reading Files",
    "text": "1.14 Reading Files\n\nThere are a number of ways to read data into R. Probably the easiest and most frequently used involves reading data from plain-text (ASCII) files. These files can be space, tab, or comma delimited.\nYou can create these files in a spreadsheet program like Excel or output them from most other statistical packages.\nYou can read these from a local directory or from an internet source\nR expects delimited files to be “white-space delimited” with values separated by either tabs or spaces and rows separated by carriage returns\nIt’s always a good idea to specify whether or not you have a header (i.e., column names). If you don’t, say header=FALSE; if you do, obviously, say header=TRUE\n\n\n# read a space-delimitted file (a sociomatrix of kids 17 kids aggressive acts toward each other)\n(kids &lt;- read.table(\"./data/strayer_strayer1976-fig2.txt\", header=FALSE))\n\n   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17\n1   0  1  3  4  1  0  0  1  1   0   1   0   7   0   1   0   0\n2   1  0  7  8  2  1  1 12  3   0   1   1   4   1   0   0   2\n3   1  4  0  7  3  2  2  0  1   0   8   1   5   5   0   0   1\n4   3  3  2  0  3  1 13  3  5   1   0   0   8   3   0   2   1\n5   1  0  0  3  0  4  6  0  8   5   1   0   1   3   0   2   1\n6   0  0  0  0  0  0  2  8 11   0   4   0   4   3   0   1   0\n7   1  0  1  9  3  4  0  2  0   0   1   0   7   9   1   1   0\n8   0  0  0  1  1  1  2  0  7   5   1   1   1   0   0   0   0\n9   1  1  1  2  5 11  0  3  0   0   0   0   1   0   0   0   0\n10  0  0  0  0  0  0  1  0  0   0   0  11   0   1   0   1   4\n11  4  0  4  3  3  2  1  0  0   0   0   3  11   5   0   2   2\n12  0  0  0  0  0  0  0  0  0   2   0   0   2   0   8   0   0\n13  0  1  9  3  0  3  6  0  0   0  11   2   0   1   0   7   5\n14  0  1  4  0  1  2  1  0  0   0   1   0   1   0   0   0   0\n15  0  0  0  0  0  0  0  0  0   0   0   3   0   0   0   0   0\n16  0  0  0  0  0  1  0  0  0   0   0   1   1   0   0   0   0\n17  0  0  0  0  0  0  0  0  1   1   0   0   0   0   0   0   0\n\n\n\nIf your file is delimited by something other than spaces, it is a good idea to use a slightly different function, read.delim() and specify exactly what the delimiter is\nFrequently, there will be non-tabular information at the top of a file (e.g., meta-data describing the data set). Use the skip=n option, where n is the number of lines you want skipped.\n\n\nquercus &lt;- read.delim(\"./data/quercus.txt\", skip=24, sep=\"\\t\", header=TRUE)\nhead(quercus)\n\n                    Species   Region Range acorn.size tree.height\n1           Quercus alba L. Atlantic 24196        1.4          27\n2  Quercus bicolor Willd.   Atlantic  7900        3.4          21\n3 Quercus macrocarpa Michx. Atlantic 23038        9.1          25\n4  Quercus prinoides Willd. Atlantic 17042        1.6           3\n5         Quercus Prinus L. Atlantic  7646       10.5          24\n6    Quercus stellata Wang. Atlantic 19938        2.5          17",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#the-workspace",
    "href": "gettingR.html#the-workspace",
    "title": "1  Getting Started in R",
    "section": "1.15 The Workspace",
    "text": "1.15 The Workspace\n\nR handles data in a manner that is different than many statistical packages.\nIn particular, you are not limited to a single rectangular data matrix at a time.\nThe workspace holds all the objects (e.g., data frames, variables, functions) that you have created or read in.\nYou can essentially have as many data frames as your machine’s memory will allow.\nTo find out what lurks in your workspace, use objects() command.\nTo remove an object, use rm().\nIf you really want to clear your whole workspace, you can use the following syntax: rm(list=ls()). Beware, though. Once you do this, you don’t get the data back.\n\n\nobjects()\n\n [1] \"a\"             \"aaa\"           \"ages\"          \"b\"            \n [5] \"C\"             \"child1\"        \"collection\"    \"count\"        \n [9] \"countries\"     \"countries1\"    \"cx1980\"        \"cx1988\"       \n[13] \"cxboesch\"      \"dua\"           \"empat\"         \"equalspace\"   \n[17] \"eyes\"          \"fourages\"      \"freq\"          \"freqmat\"      \n[21] \"grhesus\"       \"hair\"          \"kids\"          \"lima\"         \n[25] \"manual\"        \"nms\"           \"ones\"          \"quercus\"      \n[29] \"r\"             \"rhesus\"        \"rhesus.layout\" \"satu\"         \n[33] \"sex\"           \"t.or.f\"        \"tiga\"          \"x\"            \n[37] \"X\"             \"y\"            \n\nrm(aaa)\nrm(list=ls())\nobjects()\n\ncharacter(0)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#scope",
    "href": "gettingR.html#scope",
    "title": "1  Getting Started in R",
    "section": "1.16 Scope",
    "text": "1.16 Scope\n\nBecause the R workspace can contain many different variables and even multiple data frames, you must be aware of scope\nWhen we extract columns of a data frame (e.g., if we wanted to plot them) we need to use the syntax data.frame$col.name\n\n\n## load it again because we cleared all objects!\nquercus &lt;- read.delim(\"./data/quercus.txt\", skip=24, sep=\"\\t\", header=TRUE)\nplot(quercus$tree.height, quercus$acorn.size, pch=16, col=\"red\", xlab=\"Tree Height (m)\", ylab=\"Acorn Size (cm3)\")\n\n\n\n\n\n\n\n\n\nIt can be a hassle having to type the data frame name (and dollar sign) over and over again\nWith the with() function, we can set up a local scoping rule that allows us to drop the need to type the data frame name (and dollar sign) to access columns of a data frame\n\n\nwith(quercus, plot(tree.height, acorn.size, pch=16, col=\"blue\", xlab=\"Tree Height (m)\", ylab=\"Acorn Size (cm3)\"))\n\n\n\n\n\n\n\n\n\nApparently, there are R users who gladly use with and those who hate its use. I fall into the former category.\nNote that this is a very Base-R perspective. the tidyverse (e.g., ggplot2, etc.) changes many of these issues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#indexing-and-subsetting",
    "href": "gettingR.html#indexing-and-subsetting",
    "title": "1  Getting Started in R",
    "section": "1.17 Indexing and Subsetting",
    "text": "1.17 Indexing and Subsetting\n\nIndex (and access) the elements of a vector using square brackets. myvec[1] takes the first element of a vector called myvec.\nUse the colon (:) operator for sequences. myvec[1:5] takes the first five elements of myvec.\nR is unusual in that it allows negative indexing: myvec[-1] takes all elements of except the first one. To exclude a sequence, you need to place the sequence within parentheses: myvec[-(1:5)].\nVector indices don’t have to be consecutive: myvec[c(2,5,1,11)].\n\n\nmyvec &lt;- c(1,2,3,4,5,6,66,77,7,8,9,10)\nmyvec[1]\n\n[1] 1\n\nmyvec[1:5]\n\n[1] 1 2 3 4 5\n\nmyvec[-1]\n\n [1]  2  3  4  5  6 66 77  7  8  9 10\n\nmyvec[-(1:5)]\n\n[1]  6 66 77  7  8  9 10\n\n# try without the parentheses\n#myvec[-1:5]\nmyvec[c(2,5,1,11)]\n\n[1] 2 5 1 9\n\n\n\nAccess the elements of a data frame using the dollar sign. Subsetting anything other than a data frame uses square brackets.\n\n\ndim(quercus)\n\n[1] 39  5\n\nsize &lt;- quercus$acorn.size\nsize[1:3] #first 3 elements\n\n[1] 1.4 3.4 9.1\n\nsize[17]  #only element 17\n\n[1] 4.8\n\nsize[-39] #all but the last element\n\n [1]  1.4  3.4  9.1  1.6 10.5  2.5  0.9  6.8  1.8  0.3  0.9  0.8  2.0  1.1  0.6\n[16]  1.8  4.8  1.1  3.6  1.1  1.1  3.6  8.1  3.6  1.8  0.4  1.1  1.2  4.1  1.6\n[31]  2.0  5.5  5.9  2.6  6.0  1.0 17.1  0.4\n\nsize[c(3,6,9)] # elements 3,6,9\n\n[1] 9.1 2.5 1.8\n\nsize[quercus$Region==\"California\"] # use a logical test to subset\n\n [1]  4.1  1.6  2.0  5.5  5.9  2.6  6.0  1.0 17.1  0.4  7.1\n\nquercus[3,4] # access an element of an array or data frame by X[row,col]\n\n[1] 9.1\n\nquercus[,\"tree.height\"]\n\n [1] 27.0 21.0 25.0  3.0 24.0 17.0 15.0  0.3 24.0 11.0 15.0 23.0 24.0  3.0 13.0\n[16] 30.0  9.0 27.0  9.0 24.0 23.0 27.0 24.0 23.0 18.0  9.0  9.0  4.0 18.0  6.0\n[31] 17.0 20.0 30.0 23.0 26.0 21.0 15.0  1.0 18.0\n\n\n\nThe comma with nothing in front of it means take every row in the column named \"tree.height\".",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#more-subsetting",
    "href": "gettingR.html#more-subsetting",
    "title": "1  Getting Started in R",
    "section": "1.18 More Subsetting",
    "text": "1.18 More Subsetting\n\nPositive indices include, negative indices exclude elements\n1:3 means a sequence from 1 to 3\nYou can only use a single negative subscript, i.e., you can’t use quercus$acorn.size[-1:3]\nOf course, you can get around this by enclosing the vector in parentheses quercus$acorn.size[-(1:3)]\nThe logical operators are == (equal), != (not equal), and the various greater than/less than symbols: &gt;, &gt;=, &lt;, &lt;=\nFurther logicals are & (and), | (or), ! (not), && (another and), || (another or)\n& and ! work elementwise on vectors: element 1 is compared in the two vectors, then element 2, and so on\n&& and || are tricky. These logical tests evaluate left to right, examining only the first element of each vector (they go until a result is determined for ||).\n\nWhy would you want that?? In general, you don’t. It makes some calculations faster.\n\nWhen you refer to a variable in a data frame, you must specify the data frame name followed a dollar sign and the variable name quercus$acorn.size\nTesting for equality is just a special case of a logical test. We frequently want to identify numbers either above or below some criterion.\n\n\nmyvec &lt;- c(1,2,3,4,5,6,66,77,7,8,9,10)\nmyvec &lt;- myvec[myvec&lt;=10]\nmyvec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:7\n# elements that are greater than 2 but less than 6\n(x&gt;2) & (x&lt;6)\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\nis.even &lt;- rep(c(FALSE, TRUE),5)\n(evens &lt;- myvec[is.even])\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#missing-values",
    "href": "gettingR.html#missing-values",
    "title": "1  Getting Started in R",
    "section": "1.19 Missing Values",
    "text": "1.19 Missing Values\n\nNA is a special code for missing data.\nNA pretty much means “Don’t Know.”\nThe presence of NA values in your data set can lead to some surprising consequences.\nYou can’t test for a NA the way you would test for any other value (i.e., using the == operator) since variable==NA is like asking in English, is the variable equal to some number I don’t know? How could you know that?!\nIt also doesn’t make any sense to add one to something you don’t know what it is – 1+NA is meaningless!\nR therefore provides the function is.na() that allows us to subset using logicals.\n\n\naaa &lt;- c(1,2,3,NA,4,5,6,NA,NA,7,8,9,NA,10)\naaa &lt;- aaa[!is.na(aaa)]\naaa\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nHere we used the not-operator (!) to index everything that is not an NA\nThis is actually probably the most common way of using is.na().\n\n\naaa &lt;- c(1,2,3,NA,4,5,6,NA,NA,7,8,9,NA,10)\nis.na(aaa)\n\n [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13]  TRUE FALSE\n\n!is.na(aaa)\n\n [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[13] FALSE  TRUE\n\n\n\nThere are a couple other special values of objects\nOne is Inf, which means “infinity.”” It may result from dividing zero by zero.\nAnother is NaN, which means “not a number.” You will get this is, e.g., you try to take a logarithm of a negative number.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#summarizing-data",
    "href": "gettingR.html#summarizing-data",
    "title": "1  Getting Started in R",
    "section": "1.20 Summarizing Data",
    "text": "1.20 Summarizing Data\n\nThe function table() is a very useful way of exploring data\n\n\n# generate 100 Poisson random numbers with mean/variance=5\naaa &lt;- rpois(100,5)\ntable(aaa)\n\naaa\n 1  2  3  4  5  6  7  8  9 10 11 12 \n 5  9 14 13 17 16  9 10  3  2  1  1 \n\n\n\ndonner &lt;- read.table(\"./data/donner.dat\", header=TRUE, skip=2)\n# survival=0 == died; male=0 == female\nwith(donner, table(male,survival))\n\n    survival\nmale  0  1\n   0  5 10\n   1 20 10\n\n# table along 3 dimensions\nwith(donner, table(male,survival,age))\n\n, , age = 15\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 0\n\n, , age = 18\n\n    survival\nmale 0 1\n   0 0 0\n   1 0 1\n\n, , age = 20\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 1\n\n, , age = 21\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 0\n\n, , age = 22\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 0\n\n, , age = 23\n\n    survival\nmale 0 1\n   0 0 1\n   1 2 1\n\n, , age = 24\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 0\n\n, , age = 25\n\n    survival\nmale 0 1\n   0 1 1\n   1 5 1\n\n, , age = 28\n\n    survival\nmale 0 1\n   0 0 0\n   1 2 2\n\n, , age = 30\n\n    survival\nmale 0 1\n   0 0 0\n   1 3 1\n\n, , age = 32\n\n    survival\nmale 0 1\n   0 0 2\n   1 0 1\n\n, , age = 35\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 40\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 1\n\n, , age = 45\n\n    survival\nmale 0 1\n   0 2 0\n   1 0 0\n\n, , age = 46\n\n    survival\nmale 0 1\n   0 0 0\n   1 0 1\n\n, , age = 47\n\n    survival\nmale 0 1\n   0 1 0\n   1 0 0\n\n, , age = 50\n\n    survival\nmale 0 1\n   0 1 0\n   1 0 0\n\n, , age = 57\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 60\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 62\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 65\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\ncage &lt;- rep(0,length(donner$age))\n# simplify by defining 2 age classes: over/under 25\ncage[donner$age&lt;=25] &lt;- 1\ncage[donner$age&gt;25] &lt;- 2\ndonner &lt;- data.frame(donner,cage=cage)\nwith(donner, table(male,survival,cage))\n\n, , cage = 1\n\n    survival\nmale  0  1\n   0  1  7\n   1  9  4\n\n, , cage = 2\n\n    survival\nmale  0  1\n   0  4  3\n   1 11  6\n\n\n\nIt’s sometimes useful to sort a vector\n\n\naaa &lt;- rpois(100,5)\nsort(aaa)\n\n  [1]  0  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3\n [26]  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  5  5\n [51]  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6\n [76]  6  6  6  7  7  7  7  7  7  7  7  7  8  8  8  8  9  9  9  9  9 10 11 11 17\n\n# decreasing order\nsort(aaa,decreasing=TRUE)\n\n  [1] 17 11 11 10  9  9  9  9  9  8  8  8  8  7  7  7  7  7  7  7  7  7  6  6  6\n [26]  6  6  6  6  6  6  6  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n [51]  5  5  4  4  4  4  4  4  4  4  4  4  4  4  3  3  3  3  3  3  3  3  3  3  3\n [76]  3  3  3  3  3  3  3  3  3  2  2  2  2  2  2  2  2  2  2  2  1  1  1  1  0\n\n\n\nSorting data frames is a bit more involved, but still straightforward\nuse the function order()\n\n\n# five columns of data again\nsatu &lt;- c(1,2,3,4,5)\ndua &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\ntiga &lt;- sample(c(TRUE,FALSE),5,replace=TRUE)\nempat &lt;- LETTERS[7:11]\nlima &lt;- rnorm(5)\n# construct a data frame\n(collection &lt;- data.frame(satu,dua,tiga,empat,lima))\n\n  satu dua  tiga empat       lima\n1    1   a  TRUE     G -0.5836921\n2    2   b FALSE     H  0.9042409\n3    3   c  TRUE     I -0.1804774\n4    4   d FALSE     J  1.3276193\n5    5   e  TRUE     K  0.3944727\n\no &lt;- order(collection$lima)\ncollection[o,]\n\n  satu dua  tiga empat       lima\n1    1   a  TRUE     G -0.5836921\n3    3   c  TRUE     I -0.1804774\n5    5   e  TRUE     K  0.3944727\n2    2   b FALSE     H  0.9042409\n4    4   d FALSE     J  1.3276193\n\n\n\nthere are definitely better ways to do this using tidy tools like dplyr::arrange()!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#naming-data",
    "href": "gettingR.html#naming-data",
    "title": "1  Getting Started in R",
    "section": "1.21 Naming Data",
    "text": "1.21 Naming Data\n\nThe matrix of aggressive interactions among kids had neither column nor row names\nWe can add the codes used in the Strayer and Strayer (1976) paper\n\n\n## load it again because we cleared all objects!\nkids &lt;- read.table(\"./data/strayer_strayer1976-fig2.txt\", header=FALSE)\nkid.names &lt;- c(\"Ro\",\"Ss\",\"Br\",\"If\",\"Td\",\"Sd\",\"Pe\",\"Ir\",\"Cs\",\"Ka\",\n                \"Ch\",\"Ty\",\"Gl\",\"Sa\", \"Me\",\"Ju\",\"Sh\")\ncolnames(kids) &lt;- kid.names\nrownames(kids) &lt;- kid.names\nkids\n\n   Ro Ss Br If Td Sd Pe Ir Cs Ka Ch Ty Gl Sa Me Ju Sh\nRo  0  1  3  4  1  0  0  1  1  0  1  0  7  0  1  0  0\nSs  1  0  7  8  2  1  1 12  3  0  1  1  4  1  0  0  2\nBr  1  4  0  7  3  2  2  0  1  0  8  1  5  5  0  0  1\nIf  3  3  2  0  3  1 13  3  5  1  0  0  8  3  0  2  1\nTd  1  0  0  3  0  4  6  0  8  5  1  0  1  3  0  2  1\nSd  0  0  0  0  0  0  2  8 11  0  4  0  4  3  0  1  0\nPe  1  0  1  9  3  4  0  2  0  0  1  0  7  9  1  1  0\nIr  0  0  0  1  1  1  2  0  7  5  1  1  1  0  0  0  0\nCs  1  1  1  2  5 11  0  3  0  0  0  0  1  0  0  0  0\nKa  0  0  0  0  0  0  1  0  0  0  0 11  0  1  0  1  4\nCh  4  0  4  3  3  2  1  0  0  0  0  3 11  5  0  2  2\nTy  0  0  0  0  0  0  0  0  0  2  0  0  2  0  8  0  0\nGl  0  1  9  3  0  3  6  0  0  0 11  2  0  1  0  7  5\nSa  0  1  4  0  1  2  1  0  0  0  1  0  1  0  0  0  0\nMe  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0\nJu  0  0  0  0  0  1  0  0  0  0  0  1  1  0  0  0  0\nSh  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0\n\n\n\ncolnames() and rownames() are convenience functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#working-on-lists",
    "href": "gettingR.html#working-on-lists",
    "title": "1  Getting Started in R",
    "section": "1.22 Working on Lists",
    "text": "1.22 Working on Lists\n\napply() applies a function along the margins of a matrix\nlapply() applies a function to a list and generates a list as its output\nsapply() is similar to lapply() but generates a vector as its output\n\n\n# cross-tabulation of sex partners by race/ethnicity from NHSLS\nsextable &lt;- read.csv(\"./data/nhsls_sextable.txt\", header=FALSE)\ndimnames(sextable)[[1]] &lt;- c(\"white\",\"black\",\"hispanic\",\"asian\",\"other\")\ndimnames(sextable)[[2]] &lt;- c(\"white\",\"black\",\"hispanic\",\"asian\",\"other\")\n# take a peek at it\nsextable\n\n         white black hispanic asian other\nwhite     1131    12       16     3    15\nblack        5   268        5     0     0\nhispanic    39     1      115     0     3\nasian       12     0        0    10     4\nother        7     0        1     0    18\n\n# calculate marginals\n(row.sums &lt;- apply(sextable,1,sum))\n\n   white    black hispanic    asian    other \n    1177      278      158       26       26 \n\n(col.sums &lt;- apply(sextable,2,sum))\n\n   white    black hispanic    asian    other \n    1194      281      137       13       40 \n\n# using sapply() gives similar output\nsapply(sextable,sum)\n\n   white    black hispanic    asian    other \n    1194      281      137       13       40 \n\n# create a list -- each element of the list has a different length\naaa &lt;- list(alpha = 1:10, beta = rnorm(50), x = sample(1:100, 100, replace=TRUE))\nlapply(aaa,mean)\n\n$alpha\n[1] 5.5\n\n$beta\n[1] -0.0248892\n\n$x\n[1] 50.63\n\n# more compact as a vector\nsapply(aaa,mean)\n\n     alpha       beta          x \n 5.5000000 -0.0248892 50.6300000 \n\n# compare the output of sapply() to lapply()\nlapply(sextable,sum)\n\n$white\n[1] 1194\n\n$black\n[1] 281\n\n$hispanic\n[1] 137\n\n$asian\n[1] 13\n\n$other\n[1] 40\n\n\n\nThe apply family of functions used to be more widely used and have been largely supplanted by the immensely powerful tools in dplyr and the tidyverse more generally",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#flow-control-if",
    "href": "gettingR.html#flow-control-if",
    "title": "1  Getting Started in R",
    "section": "1.23 Flow Control: if",
    "text": "1.23 Flow Control: if\n\nif allows you to conditionally evaluate expressions.\nThe basic syntax of an if statement is: if(condition) true.branch else false.branch\nThe else part of the statement is optional\n\n\n(coin &lt;- sample(c(\"heads\",\"tails\"),1))\n\n[1] \"tails\"\n\nif(coin==\"tails\") b &lt;- 1 else b &lt;- 0\nb\n\n[1] 1\n\n\n\nSometimes you can use the very efficient ifelse statement\nifelse takes three arguments: (1) the logical test, (2) the result if TRUE, (3) the result if FALSE\n\n\nx &lt;- 4:-2\n# sqrt(x) produces warnings, but using ifelse to check works without producing warings\nsqrt(ifelse(x &gt;= 0, x, NA))\n\n[1] 2.000000 1.732051 1.414214 1.000000 0.000000       NA       NA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#flow-control-for",
    "href": "gettingR.html#flow-control-for",
    "title": "1  Getting Started in R",
    "section": "1.24 Flow Control: for",
    "text": "1.24 Flow Control: for\n\nIf you want to repeat an action over and over again you need a loop\nLoops are mostly generated using for statements\nThe basic syntax of a for loop is: for(item in sequence) statement(s)\n\n\nx &lt;- 1:5\nfor(i in 1:5) print(x[i])\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\nThat’s a pretty silly for loop – there are much more important uses of for loops!\nIf there are multiple statements executed by a for loop, those statements must be enclosed in curly braces, {}\nWe need to be careful with for loops because they can slow code down, particularly when they are nested and the number of iterations is very large.\nVectorizing and using mapping functions like apply and its relatives can greatly speed your code up",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#using-packages",
    "href": "gettingR.html#using-packages",
    "title": "1  Getting Started in R",
    "section": "1.25 Using Packages",
    "text": "1.25 Using Packages\n\nMuch of the functionality of R comes from the many contributed packages\nTo use a package, you must first install it\nThis can be done at the command line using install.packages(package_name)\nIt is often more convenient to use a menu command\nin RStudio this is under Tools&gt;Install Packages...\nOnce a package is installed, you must load it in order to use it\nDo this using the library() command\n\n\nlibrary(igraph)\n# might as well do something with it\n# a small graph\ng &lt;- make_graph( c(1,2, 1,3, 2,3, 3,5), n=5 )\nplot(g)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "drawing.html",
    "href": "drawing.html",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "",
    "text": "3.1 Introduction\nR has powerful graphics capabilities. While we typically use these for plotting data, we can also make publication-quality plots for elucidating theoretical topics as well.\nThese notes are a very tentative start to a much larger body of work. I hope they are nonetheless helpful in their rather incomplete form.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "graphs.html",
    "href": "graphs.html",
    "title": "4  Actual Graphs",
    "section": "",
    "text": "4.1 Some Definitions\nIn this chapter, I will focus on drawing graphs using the R package igraph. I have a more thorough introduction to graphs elsewhere\nA graph is simply a collection of vertices (or nodes) and edges (or ties). We can denote this \\(\\mathcal{G}(V,E)\\), where \\(V\\) is a the vertex set and \\(E\\) is the edge set. The vertices of the graph represent the actors in the social system. These are usually individual people, but they could be households, geographical localities, institutions, or other social entities. The edges of the graph represent the relations between these entities (e.g., “is friends with” or “has sexual intercourse with” or “sends money to”). These edges can be directed (e.g., “sends money to”) or undirected (e.g., “within 2 meters of”).\nWhen the relations that define the graph are directional, we have a directed graph or digraph.\nGraphs (and digraphs) can be binary (i.e., presence/absence of a relationship) or valued (e.g., “groomed five times in the observation period”, “sent $100”).\nA graph (with no self-loops) with \\(n\\) vertices has \\({n \\choose 2} = n(n-1)/2\\) possible unordered pairs. This number (which can get very big!) is important for defining the density of a graph, i.e., the fraction of all possible relations that actually exist in a network.\nA bipartite graph is a graph where all the nodes of a graph can be partitioned into two sets \\(\\mathcal{V}_1\\) and \\(\\mathcal{V}_2\\) such that for all edges in the graph connects and unordered pair where one vertex comes from \\(\\mathcal{V}_1\\) and the other from \\(\\mathcal{V}_2\\). Often called an “affiliation graph” as bipartite graphs are used to represent people’s affiliations to organizations or events.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "odes.html",
    "href": "odes.html",
    "title": "5  ODEs in R",
    "section": "",
    "text": "5.1 Lotka-Volterra Model\nWe can also use R to numerically integrate systems of ordinary differential equations (ODEs). These are commonly used in ecology and epidemiology.\nWe do this using the package deSolve, which has some excellent learning resources to support it (check out the vignettes).\nThe Italian biologist Humberto D’Ancona noted that during the first World War, the composition of fish in the markets around the Adriatic Sea changed substantially. During the war, the percentage of predatory fish for sale in the markets of Trieste, Fiume, and Venice increased. D’Ancona had no explanation for this and approached his father-in-law, the eminent mathematician Vito Volterra, with the riddle. Volterra’s solution forms the foundation for nearly all subsequent theory regarding the interaction of species within communities. The great American biologist and demographer, Alfred Lotka, developed the same framework about the same time and the equations have since been known as the Lotka-Volterra model for predatory/prey dynamics.\nThe classical theory of species interactions is attributable to Alfred Lotka and Vito Volterra involves reducing communities to a single consumer-resource relationship – typically between a primary consumer (i.e., a herbivore) and a secondary consumer (i.e., a carnivore).\nThe assumptions of Lotka-Volterra model include: - in the absence of a predator, the prey population increases exponentially - in the absence of prey, the predator population decays exponentially - per capita rate of kill a linear function of prey density - each kill contributes equally to predator growth\nrequire(deSolve)\nlv &lt;- function(t, x, parms) {\n  with(as.list(parms), {\n    dx1 &lt;- r1*x[1] - c1*x[1]*x[2]\n    dx2 &lt;- -r2*x[2] + c2*x[1]*x[2]\n    results &lt;- c(dx1,dx2)\n    list(results)\n  })\n}\n\nxstart &lt;- c(x1=10,x2=1)\ntimes &lt;- seq(0,100,length=1001)\nparms &lt;- c(r1=0.5,r2=0.5, c1=0.1,c2=0.02)\nout1 &lt;- as.data.frame(ode(xstart,times,lv,parms))\n\nwith(out1, plot(time, x1, type=\"l\", lwd=3, col=\"red\", \n                xlab=\"Time\", ylab=\"Population Size\", xlim=c(0,100), ylim=c(0,90)))\nwith(out1, lines(time, x2, ,lwd=3, col=\"blue\"))\nlegend(\"topleft\",c(\"prey\",\"predator\"), lwd=3, col=c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nwith(out1, plot(x1, x2, type=\"l\", col=\"magenta\", lwd=3, xlab=\"Prey Population Size\", ylab=\"Predator Population Size\",\n                xlim=c(0,90), ylim=c(0,20)))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "interpreting.html",
    "href": "interpreting.html",
    "title": "2  Interpreting Scientific Figures",
    "section": "",
    "text": "2.1 Introduction\nUnderstanding scientific figures is an important part of becoming a scientist or a critical consumer of scientific information. This is a skill that, alas, is generally not taught in most schools. Here, I will try to provide a gentle introduction to reading scientific figures, especially theoretical plots. In chapter Chapter 3, we go into some detail on how to generate scientific plots in R.\nWe use theory in science to bring order to the complexity we observe in the world. Theory generates our hypotheses but it also guides us in what we observe, how we measure it, and what we should find surprising. Surprise is essential for the scientific enterprise because it is the surprise that comes when we observe something novel from a process we thought we understood that generates innovation and explanation.\nA couple starting points. We will use some very basic calculus here: Derivatives, second derivatives, and Taylor series, which give us slopes, curvature, and tangents for making figures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "demog1.html",
    "href": "demog1.html",
    "title": "6  Demography I: Mortality",
    "section": "",
    "text": "6.1 Cohorts and Periods\nDemographic rates typically take the form of so-called event/exposure rates. In essence, we count the number of events of interest (e.g., births, deaths, marriages, bouts of diarrhea, job losses) and divide by the population at risk for the event. The population at risk is a combination of two things: (1) the number of people in the time period and (2) the length of the period. The measure that combines these is known as person-years at risk for the event. Much of demography involves measuring these person-years of risk. This is why it is sometimes said that Epidemiology measures numerators while Demography measures denominators.\nTime enters demography in two distinct ways: (1) through calendar time (which is the same for everyone) and (2) through personal time measured for each person as age. This distinction in time leads to two different ways of conceiving of a population. A cohort is a group of individuals that we follow simultaneously through time. In contrast, a period is a specific historical span of time and the population that occupies that time. The Lexis diagram is a useful graphical device that helps organize thinking about ages, periods, and cohorts. It plots calendar time along the horizontal axis and age along the vertical axis. A person who is born at some time \\(t\\) is plotted with a lifeline that intercepts the horizontal axis at \\(t\\) and then increases with a slope of unity (since for every year that passes, a person is a year older) until death.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#cohorts-and-periods",
    "href": "demog1.html#cohorts-and-periods",
    "title": "6  Demography I: Mortality",
    "section": "",
    "text": "6.1.1 The Lexis diagram.\nWe can use the Epi package to conveniently draw Lexis diagrams.\nI am an honorary Berkeley Demography Nerd and as my homage to the great Ken Wachter, I will use historical data from the UK to illustrate the Lexis diagram. Ken had a habit of using literary and historical examples in his teaching and writing. King Edward III of England reigned from 1312-1377 and had 11 children. We know the dates of birth and death of all but one of these children\nChildren of King Edward III of England (1312-1377)\n\n\n\nName\nBirth\nDeath\n\n\n\n\nEdward, The Black Prince\n1330\n1376\n\n\nIsabel\n1332\n1382\n\n\nJoan\n1335\n1348\n\n\nWilliam of Hatfield\n1336\n?\n\n\nLionel of Antwerp, Duke of Clarence\n1338\n1368\n\n\nJohn of Gaunt, Duke of Lancaster\n1340\n1398\n\n\nEdmund Langley, Duke of York\n1341\n1402\n\n\nBlanche\n1342\n1342\n\n\nMary\n1344\n1362\n\n\nMargaret\n1346\n1361\n\n\nThomas of Woodstock, Duke of Gloucester\n1355\n1397\n\n\n\n\nlibrary(Epi)\nLexis.diagram(date=c(1330,1405), age=c(0,62), int=5,\n              entry.date=c(1330,1332,1335,1336,1338,1340,1341,1342,1344,1346,1355),\n              exit.date=c(1376,1382,1348,1336,1368,1398,1402,1342,1362,1361,1397),\n              entry.age = rep(0,11),\n              fail=c(T,T,T,F,T,T,T,T,T,T,T),\n              lwd.life=2,\n              pch.fail=19,\n              col.fail=c(\"red\",\"black\"),\n              cex.fail=1.1)\n\n\n\n\n\n\n\n\nEach individual is represented by a line with x-origin of their birth-date. The line has a slope of 1 because each year that passes, the individual is one year older. So what’s so profound about plotting a line of age vs. time? They’re literally just a collection of lines with a slope of 1. In fact, in most applications, we supress individual life lines and work with the more abstract space. The Lexis diagram is actually surprisingly helpful for thinking about problems of age vs. period vs. cohort. The so-called age-period-cohort problem is ubiquitous in demography and epidemiology and it’s nice to have a tool to help us understand it. Think about the possibilities for confounding: age effects arise because of intrinsic biological processes like development and aging; periods effects arise because of historical circumstances that affect everyone alive at that moment; cohort effects arise from the particular circumstances experienced by a cohort as they move through time.\nThe half plane that defines the Lexis diagram is called the age-time plane. The Lexis diagram employs some standard demographic conventions, which are useful to state explicitly: A person’s exact age at any given time is the time elapsed since their birth. A person’s age in completed years at any given time is the greatest integer less than their exact age. Age in completed years is also referred to as “age at last birthday.” Time refers to a point in time, while a time period refers to an interval beginning and ending at specified times. Time and time period are analogous to exact age and age group.\nWe can divide up the age-time plane in different ways. Any straight line segment in the age-time plane represents the set of individuals whose life lines intersect this line. A line segment perpendicular to the time axis, intersecting at time \\(t\\), represents the set of all individuals in the population at time \\(t\\). If we did a census at \\(t\\), these are the people we would count, so we can call the individuals intersected by this line the census set of the population at time \\(t\\).\nWe can also draw rectangles, which will intersect with larger groups of individuals. Vertical rectangles capture time periods, while horizontal rectangles capture age groups. Diagonal parallelograms capture cohort experience.\nIn the following Lexis diagram, if we wanted to calculate the mortality rate for the period 1960-1980 (the black rectangle), we’d count up the number of events (black circles) and divide by the total length of the line segments contained in the rectangle.\n\nset.seed(8675309)\nentry.ages &lt;- rep(0,50)\nbirth.dates &lt;- runif(50,min=1900,max=1970)\nexit.ages &lt;- rgamma(50,shape=18,scale=4)\nfails &lt;- rep(F,50)\n#\nLexis.diagram( age=c(0,80),\n              date=c(1950,1990),\n              age.grid=FALSE,\n              date.grid=FALSE,\n              entry.age=entry.ages,\n              birth.date=birth.dates,\n              exit.age=exit.ages,\n              fail=fails,\n              pch.fail=19,\n              cex=0.5,\n              lwd.life=0.25)\nsegments(1960,0,1960,80, lwd=5)\nsegments(1960,80,1980,80, lwd=5)\nsegments(1980,0,1980,80, lwd=5)\nsegments(1960,0,1980,0, lwd=5)\n\n\n\n\n\n\n\n\nIncidentally, that’s not really a practical way to calculate mortality rates; it’s conceptual.\nIf we wanted to examine the mortality of a cohort of people born within ten years of each other, what would the Lexis rectangle look like? Here we follow the people born between 1950-1960 until 1990 (when most of the life lines are right-censored). Our rectangle has turned into a parallelogram.\n\nLexis.diagram( age=c(0,80),\n              date=c(1950,1990),\n              age.grid=FALSE,\n              date.grid=FALSE,\n              entry.age=entry.ages,\n              birth.date=birth.dates,\n              exit.age=exit.ages,\n              fail=fails,\n              pch.fail=19,\n              cex=0.5,\n              lwd.life=0.25)\nsegments(1950,0,1990,40, lwd=5)\nsegments(1960,0,1990,30, lwd=5)\nsegments(1990,30,1990,40, lwd=5)\nsegments(1950,0,1960,0, lwd=5)\n\n\n\n\n\n\n\n\nBecause we haven’t talked about enough geometric shapes yet, here’s another. Turns out, making triangles by dividing parallelograms in half is really useful.\n\nLexis.diagram( age=c(0,5),\n              date=c(2000,2005),\n              int = 1,\n              age.grid=TRUE,\n              date.grid=TRUE)\nrect(2003,0,2004,5, col=rgb(218, 112, 214, 100, maxColorValue = 255))\nxx &lt;- c(2000:2005,2005:2000)\nyy &lt;- c(0:5,4:0,0)\npolygon(xx,yy, col=rgb(0,0,220,120, maxColorValue = 255))\n## lower triangle\nsegments(2003,2,2003,3,lwd=3)\nsegments(2003,3,2004,3,lwd=3)\nsegments(2003,2,2004,3,lwd=3)\n## upper triangle\nsegments(2003,3,2004,4,lwd=3)\nsegments(2004,4,2004,3,lwd=3)\n\n\n\n\n\n\n\n\nIn this Lexis diagram, the orchid vertical rectangle represents the period from 2003-2004. The blue diagonal represents all individuals born in calendar year 2000. Their intersection describes the two Lexis triangles. The lower Lexis triangle is all individuals born in 2000 who are 2 at their last birthday in the year 2003. The upper Lexis triangle is that individuals born in 2000 who are 3 at their last birthday in 2003. Individuals from a particular cohort, \\(t-x\\), are thus split across two different Lexis squares, which combine a a period and age class of the same width. Another way to think about this is that every Lexis square contains events that happened to individuals from the \\(t-x\\) and \\(t-x+1\\) cohorts.\nThe Human Mortality Database makes extensive use of Lexis triangles in its calculations. For example, the cross-classification required to locate a Lexis triangle—you need to know the year of birth, the age at death, and the year of death to place a death in a triangle—provides important quality checks on mortality data. Moreover, by knowing the distribution of deaths in upper vs. lower triangles, we can impute unknown ages at death, as described in the document linked above.\nOne final Lexis variant. This is called a Lexis surface plot. I’ve downloaded the yearly central death rates for the USA from 1933-2023 from HMD. We’ll plot Lexis surfaces for women, men, and the ratio of female-to-male log-mortality. Might need to work a bit on the color scale, but it’s a start.\n\nrequire(MetBrewer)\n\nLoading required package: MetBrewer\n\nusa &lt;- read.table(file=\"./data/usa_mx_1x1.txt\", skip=1, header=TRUE)\nhead(usa)\n\n  Year Age   Female     Male    Total\n1 1933   0 0.054177 0.068175 0.061292\n2 1933   1 0.008866 0.010039 0.009459\n3 1933   2 0.004025 0.004671 0.004351\n4 1933   3 0.002869 0.003333 0.003104\n5 1933   4 0.002230 0.002537 0.002386\n6 1933   5 0.001852 0.002092 0.001975\n\n## 91 years of mortality\nlength(unique(usa$Year))\n\n[1] 91\n\n# initialize 2 matrices to hold the age x year mortality rates\nfmx &lt;- matrix(0,nrow=111,ncol=91)\nmmx &lt;- matrix(0,nrow=111,ncol=91)\n# insert the columns\n\nfor(i in 0:90){\n  fmx[,i+1] &lt;- usa$Female[usa$Year==i+1933]\n}\n\nfor(i in 0:90){\n  mmx[,i+1] &lt;- usa$Male[usa$Year==i+1933]\n}\n\nage &lt;- 0:110\nyear &lt;- 1933:2023\nbounds &lt;- c(min(log10(range(usa$Female))),max(log10(range(usa$Male))))\n\nccc &lt;- met.brewer(\"Johnson\",24)\n\n## need to transpose the matrix to get the dimensions to match\nfilled.contour(x=year, y=age, z=log10(t(fmx)),\n               col=ccc, levels=pretty(bounds,20),\n               xlab=\"Year\", ylab=\"Age\")\ntitle(\"Female\")\n\n\n\n\n\n\n\nfilled.contour(x=year, y=age, z=log10(t(mmx)),\n               col=ccc, levels=pretty(bounds,20),\n               xlab=\"Year\", ylab=\"Age\")\ntitle(\"Male\")\n\n\n\n\n\n\n\n## Ratio of log-Mx\nmux.ratio &lt;- t(fmx)/t(mmx)\nfilled.contour(x=year,y=age,z=log10(mux.ratio),col=topo.colors(25), \n               xlab=\"Year\", ylab=\"Age\")\ntitle(\"Female/Male log-Mortality Ratio\")\n\n\n\n\n\n\n\n\nNeed to think a bit about what this last surface is. Take the ratio of female-to-male mortality and then take the common logarithm of that. If female mortality is less than male mortality, this ratio will be less than one and its logarithm will be negative. The darker the blue, the greater male mortality is relative to female mortality. Yellow means a male mortality advantage, while green means that they are approximately equal.\nCan you see the mortality cross-over? What about the accident hump?\nWhat was happening in the 1960s and 1970s to 60 year-old men? Is that smoking? Is that basically the Don Draper bulge? And what happened to the accident hump in 1997? I actually know the answer to the latter, but I’m still not sure about the former. That said, it’s worth noting that that 1960s bulge happens to be a cohort of men who fought in WWII 20 years earlier, in an era where we pretended that mental health of veterans wasn’t a problem and people regularly self-medicated. Regarding the rise and precipitous drop of male disadvantage across the 1980s-1990s: In 1997, David Ho’s team published a paper in Nature describing their success in suppressing HIV-1 in the cells of infected patients using combination anti-retroviral therapy. This marked the beginning of the end of the AIDS era. The growing excess of male deaths among young adults in the 1980s and early 1990s reflects the fact that HIV/AIDS was primarily a disease of gay men in the United States when it first emergeged.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#anatomy-of-the-human-mortality-curve",
    "href": "demog1.html#anatomy-of-the-human-mortality-curve",
    "title": "6  Demography I: Mortality",
    "section": "6.2 Anatomy of the Human Mortality Curve",
    "text": "6.2 Anatomy of the Human Mortality Curve\nThe mortality curve is a plot of the log of the central mortality rate against age. It has a general bathtub shape. There are marked sex differences (males typically higher at all ages). Mortality is high immediately after birth and declines rapidly. Mortality is lowest during mid-childhood. Males typically show an adolescent accident hump. Mortality increases linearly on a log scale from ages 30 onward. There is frequently a decline in the mortality rate among the oldest old\nThere are seven characteristic features of the human mortality curve:\n\nAn overall “bathtub” shape\nMale mortality generally higher than female mortality at most ages\nAn early peak in mortality rate\nA bottoming-out of mortality in middle childhood\nA nearly linear increase in mortality on a log-scale starting at age 30\nAn eventual decline in mortality (from very high levels) at the oldest ages\nAn “accident hump” in male mortality in late adolescence or early adulthood",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#period-life-table",
    "href": "demog1.html#period-life-table",
    "title": "6  Demography I: Mortality",
    "section": "6.3 Period Life Table",
    "text": "6.3 Period Life Table\nA life table is a summary of the mortality experience of a cohort of people. A cohort life table follows an actual cohort of people through time. Cohort life tables are typically quite limited because to know the full mortality experience of a cohort, you have to follow it through cohort extinction. Since people can live for 100 years or more, this means you typically don’t have cohort life tables for recent populations. It also means that the mortality experience of those who died young often happened in a very different time, where sources of mortality and healthcare might have been very different indeed.\nA period life table summarizes the mortality experience of a synthetic cohort of people, doing the as-if experiment of what this population would look like if it experienced the mortality conditions as they pertain to the present time. It’s very important to always remember that a period life table is actually a fiction. It’s a useful fiction, but a fiction nonetheless. One area where we currently (July 2025) are seeing a lot of confusion about the useful fiction of period demographic measures is not about mortality but fertility (notes forthcoming). There is currently a great deal of consternation about falling birth rates in the US and around the world. Elon Musk thinks that population collapse is the second biggest threat to humanity (after AI, of course). Vice President JD Vance wants “more babies.” Two UT economists have written a book about population collapse called After the Spike. I just read an editorial in the Wall Street Journal about fertility decline this morning. The thing is, the fertility rate of 1.6 births/woman that currently characterizes the US (the lowest ever recorded), and is causing widespread panic, is a period measure. It’s one of these useful fictions. If the population, composed as it is by a mixture of cohorts, is actively changing its behavior—for example, delaying childbearing until later in life—then the period-measure of fertility (the total fertility rate or TFR) will underestimate the actual completed fertility of younger women during a period. This is a widely understood phenomenon among demographers—typically discussed under the moniker Quantum vs. Tempo of reproduction—and has been nicely summarized by a group of outstanding contemporary demographers (Leslie Root, Karen Benjamin Guzzo and Shelly Clark) who specialize in fertility in a recent explainer in The Conversation.\nYou might see the adjective abridged used for life tables, mostly in older literature. This typically means that the ages included in the life table are all five-year classes except for the first two, which are one year (0-1, infants) and four years (1-5, young children). A complete life table would have values for all individual years. Life tables with five-year age classes (called quinquennia) are by far the most common form you will encounter.\nThe period life table is one of the central tools of demography. It follows a hypothetical cohort (i.e., a group of people born at the same moment) through time until every one of its members die. Life tables are conceptually simple but they contain a lot of notation, so strap in. Many of the entries of a life table have both pre- and post-subscripts (typically \\(n\\) and \\(x\\) respectively). Take, for example, the mortality rate between ages \\(x\\) and \\(x + n\\):\n\\[ {_nm_x} = \\frac{ {_nd_x}}{{_nL_x}}. \\]\nThe term in the numerator on the right-hand side of the equality represents the number of deaths between exact ages \\(x\\) and \\(x+n\\), while the denominator is the number of person-years lived between these ages. In general, the post-subscript \\(x\\) indicates the exact age at a person’s last birthday, while the pre-subscript \\(n\\) represents the number of years in the age interval. If there is no pre-subscript, this value is generally taken to be a single year.\nWe typically estimate this mortality rate by dividing the enumerated deaths between ages \\(x\\) and \\(x + n\\) by the mid-population census size between ages \\(x\\) and \\(x + n\\). It is important to note that these are rates and not probabilities and that in order to construct a life table, we have to convert the rates to probabilities.\nDemography necessarily involves a lot of book-keeping. This means that it is fairly notation-heavy. The notation takes a bit of getting used to, but there is nothing really fundamentally difficult about it. The following table summarizes the columns of a life table.\n\n\n\n\n\n\n\nSymbol\nDefinition\n\n\n\n\n\\(x\\)\nExact age at the start of the interval\n\n\n\\(l_x\\)\nNumber alive at age \\(x\\)\n\n\n\\(_nd_x\\)\nNumber dying between ages \\(x\\) and \\(x+n\\)\n\n\n\\(_nq_x\\)\n(\\(= {_n}d_x/l_x\\)) Probability of dying between \\(x\\) and \\(x+n\\)\n\n\n\\(_np_x\\)\n(\\(=1- {_n}d_x/l_x\\)) Probability of surviving between \\(x\\) and \\(x+n\\)\n\n\n\\(_nL_x\\)\nNumber of person-years lived between \\(x\\) and \\(x+n\\)\n\n\n\\(_nT_x\\)\nNumber of person-years that a cohort lives after reaching \\(x\\)\n\n\n\\(e_x\\)\n(\\(=T_x/l_x\\)) Expected length of life after reaching age \\(x\\)\n\n\n\\(_nm_x\\)\n(\\(={_nd}_x/{_nL}_x\\)) Death rate in the age interval \\(x\\) to \\(x+n\\)\n\n\n\\(_na_x\\)\nAverage number of person-years lived by those who die between ages \\(x\\) and \\(x+n\\)\n\n\n\nIn a particular period, vital statistics collected by the state provide the number of deaths by age group \\(_nD_x\\) (note the capital letter) and the corresponding mid-year populations \\(_nK_x\\). From these we can estimate the death rates\n\\[_nM_x = \\frac{_nD_x}{_nK_x} \\]\nWe capitalize this death rate to indicate that it is empirical—it results from the observed deaths divided by the mid-interval population size. This is our approximation of the life-table mortality rate \\(_nm_x\\).\nWe now follow a hypothetical (synthetic) cohort of \\(l_0\\) (the size of which is called the radix of the life table) individuals through life as though they experienced the above mortality rates, which are estimates of\n\\[ _nm_x = \\frac{_nd_x}{_nL_x} \\]\nRecall that the probabilities of dying are:\n\\[ _nq_x = \\frac{_nd_x}{l_x} =  \\frac{_nd_x}{_nL_x}\\frac{_nL_x}{l_x} =\n{_n}m_x \\frac{_nL_x}{l_x} \\]\nThe person-years lived between \\(x\\) and \\(x+n\\) is the \\(l_{x+n}\\) survivors who each live \\(n\\) years plus the years lived by the \\(_nd_x\\) who die in the interval, the average of which is \\(_na_x\\)\n\\[ _nL_x = n\\, l_{x+n} + {_nd_x}\\; {_n}a_x \\]\nRemember that \\(_nd_x = l_x - l_{x+n}\\), or \\(l_{x+n} = l_x - {_nd_x}\\). Substitute this for \\(l_{x+n}\\) above to get\n\\[ l_x = \\frac{_nL_x + {_nd_x}(n - {_n}a_x)}{n}, \\]\nwhich we then substitute this into the formula for \\(_nq_x\\):\n\\[ _nq_x = \\frac{_nd_x}{l_x} = \\frac{n \\cdot  {_nd_x}}{_nL_x + {_nd_x}(n -\n  {_na_x})}. \\]\nFinally, divide both numerator and denominator by \\(_nL_x\\)\n\\[ _nq_x =  \\frac{n \\cdot \\frac{_nd_x}{_nL_x}}{\\frac{_nL_x}{_nL_x} +\n  \\frac{_nd_x}{_nL_x}(n - {_n}a_x)},\n\\]\nSimplify and we are left with\n\\[ _nq_x = \\frac{n \\cdot {_nm_x}}{1 + {_n}m_x(n- {_n}a_x)}. \\]\nThis is known as the Greville equation and it’s how we convert our observed mortality rates into the life-table probabilities (assuming that \\(_nM_x \\approx {_n}m_x\\)).\nTo construct a period life table from estimates of the central mortality rate, we only need the average number of person-years lived by individuals dying in the interval \\(_na_x\\). But wait, how do we know the \\(_na_x\\)? The following relationships are due to Keyfitz:\n\\[ _1a_0 = 0.07 + 1.7\\ {_1m_0}, \\]\n\\[ _4a_1 = 1.587 - 2.816\\ {_1m_0}, \\]\nand\n\\[ _na_x = n/2,~~~~~x \\in 5, 10, 15, \\ldots \\]\nKeyfitz found values of \\(_1a_0\\) and \\(_4a_1\\) through regression of observed values of years lived among those dying in infancy and early childhood on measures of infant mortality. These estimates generally work pretty well, but will be off for very high-mortality populations.\nThe last age interval is usually open. For example 85+, which includes ages 85 through the oldest observed age at death in the population. Since everyone must die,\n\\[ _{\\infty}q_x = 1 \\]\nThis is a logical result and applies regardless of the observed death rate \\(_nm_x\\). The last observed death rate is\n\\[ _{\\infty}m_x =  {_\\infty}d_x/ {_\\infty}L_x \\]\nSince everyone dies, the number of deaths in the final open interval is equal to the number who enter\n\\[  _{\\infty}d_x = l_x. \\]\nIn the construction of a life table, this quantity is already constructed, so we now can write the expression for person-years lived in the open interval:\n\\[ _{\\infty}L_x = l_x/ {_{\\infty}m_x}. \\] So now that you’re totally overwhelmed with notation, let’s actually construct a life table.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#constructing-a-period-life-table",
    "href": "demog1.html#constructing-a-period-life-table",
    "title": "6  Demography I: Mortality",
    "section": "6.4 Constructing a Period Life Table",
    "text": "6.4 Constructing a Period Life Table\nLet’s make a period life table. We’ll start with vectors of reported deaths and mid-interval population sizes. The ages are pretty standard. Because mortality changes very fast early in life, our first age classes are shorter than subsequent ones. So we have ages 0, 1, and 5 and then every five years until the maximum age. For this data set, the maximum age is 85. Of course, it’s an open age class. It really means “85 years old or greater.”\n\nx &lt;- c(0, 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85)\nnDx &lt;- c(15706, 15502, 5813, 2550, 3264, 3162, 3570, 2958, 2958, 2448, 2244, 1836, 2040, 1938,\n         2754, 2792, 2277, 1548, 1640)\nnKx &lt;- c(115272, 443973, 453764, 385103, 299524, 225887, 228872, 178123, 179117, 140309, 126378,  97519,\n         88564,  53735,  45774,  32766,  20115,  10710,   8056)\n nmax &lt;- length(x)\n iwidth &lt;- x[4]-x[3] # width of older age classes\n\n# calculate the central death rate\n nMx &lt;- nDx/nKx\n\nThis turns out to be a high-mortality population, so the Keyfitz appraoch to \\(_na_x\\) will not work. We will instead use an approach attributable to Coale and Demeney. It is based on the same idea—regression of observed person-years lived on infant mortality—it just involves quite different values. In very high-mortality environments, the number of person-years lived by infants or young children dying in the interval will be substantially less than in low-mortality environments. That’s because in high-mortality conditions, you are more likely to die early and thus contribute fewer years to \\(_na_x\\). We’ll actually write a simple function to determine the coefficients to use and calculate our \\(_na_x\\) values for the first two age classes.\n\ncoale &lt;- function(b1,b4,nMx){\n   if(nMx[1]&gt;0.107){\n     b1 &lt;- c(0.350,0)\n     b4 &lt;- c(1.361,0)\n   }\n   nax12 &lt;- c(0,0)\n   nax12[1] &lt;- b1[1] + b1[2] *nMx[1]\n   nax12[2] &lt;- b4[1] + b4[2]* nMx[1]\n   return(nax12)\n}\n\nnax12 &lt;- coale(b1=c(0.053,2.8), b4=c(1.522,1.518), nMx)\n# initialize nax vector\nnax &lt;- NULL\nnax[1] &lt;- nax12[1]\nnax[2] &lt;- nax12[2]\nnax[3:(nmax-1)] &lt;- iwidth/2\nnax[nmax] &lt;- 1/nMx[nmax]\n\n# width of the intervals -- make last interval essentially unbounded\nn &lt;- c(1,4, rep(iwidth, nmax - 3),999)\n\n# Greville equation\n nqx &lt;- (n*nMx) / (1 + (n-nax)*nMx)\n# everyone has to die by the last interval, by definition\n nqx[nmax] &lt;- 1.0\n \n# survivorship lx\nlx &lt;- cumprod(c(1,1-nqx))\n\n# dx \n ndx &lt;- -diff(lx)\n \n# person-years lived by survivors\nlxpn &lt;- lx[-1]\nnLx &lt;- n*lxpn + ndx*nax\nTx &lt;- rev(cumsum(rev(nLx)))\n# life expectancy\nex &lt;- Tx/lx[1:nmax]\n\n## format the life table\nlt &lt;- data.frame(x, nax = round(nax,4), \n                nMx = round(nMx,4), \n                nqx = round(nqx[1:nmax],4), \n                lx = round(lx[1:nmax],4), \n                ndx = round(ndx,4), \n                nLx = round(nLx,4),\n                Tx = round(Tx,2),\n                ex = round(ex,2) )\n\nlt\n\n    x    nax    nMx    nqx     lx    ndx    nLx    Tx    ex\n1   0 0.3500 0.1363 0.1252 1.0000 0.1252 0.9186 38.52 38.52\n2   1 1.3610 0.0349 0.1279 0.8748 0.1119 3.2041 37.60 42.98\n3   5 2.5000 0.0128 0.0621 0.7630 0.0474 3.6964 34.40 45.08\n4  10 2.5000 0.0066 0.0326 0.7156 0.0233 3.5198 30.70 42.90\n5  15 2.5000 0.0109 0.0530 0.6923 0.0367 3.3697 27.18 39.26\n6  20 2.5000 0.0140 0.0676 0.6556 0.0443 3.1671 23.81 36.32\n7  25 2.5000 0.0156 0.0751 0.6112 0.0459 2.9415 20.64 33.77\n8  30 2.5000 0.0166 0.0797 0.5654 0.0451 2.7141 17.70 31.31\n9  35 2.5000 0.0165 0.0793 0.5203 0.0413 2.4983 14.99 28.81\n10 40 2.5000 0.0174 0.0836 0.4790 0.0400 2.2951 12.49 26.07\n11 45 2.5000 0.0178 0.0850 0.4390 0.0373 2.1017 10.19 23.22\n12 50 2.5000 0.0188 0.0899 0.4017 0.0361 1.9181  8.09 20.15\n13 55 2.5000 0.0230 0.1089 0.3656 0.0398 1.7283  6.17 16.89\n14 60 2.5000 0.0361 0.1654 0.3258 0.0539 1.4940  4.45 13.65\n15 65 2.5000 0.0602 0.2615 0.2719 0.0711 1.1816  2.95 10.86\n16 70 2.5000 0.0852 0.3512 0.2008 0.0705 0.8276  1.77  8.82\n17 75 2.5000 0.1132 0.4412 0.1303 0.0575 0.5076  0.94  7.24\n18 80 2.5000 0.1445 0.5309 0.0728 0.0386 0.2674  0.44  5.98\n19 85 4.9122 0.2036 1.0000 0.0342 0.0342 0.1678  0.17  4.91\n\n\nMake some plots.\n\n plot(lt$x, log(lt$nMx), type=\"l\", lwd=2, xlab=\"Age\", ylab=\"log(nMx)\")\n\n\n\n\n\n\n\n plot(lt$x, lt$ndx, type=\"l\", lwd=3, col=\"red\", xlab=\"Age\", ylab=\"Probability of Death\")\n\n\n\n\n\n\n\n\nThat all seems like kind of a lot to remember every time you want to calculate a life table. Fortunately, I have written a package called demogR that provides all these commands as a simple function—and much more! The data that we used in the laborious life-table example are from Madagascar in 1966. There is a data set included in demogR that has vital-rate data from this population as well as Venezuela in 1965, and the United States in 1967. We use the function life.table() to construct the period life table. The function has a lot ways to customize life-table construction. One of these is the type argument. This is how we calculate the first two values of \\(_na_x\\). It takes as its default the Keyfitz method. Because infant mortality is so high in the Madagascar data, we need to use the Coale-Demeny option (type=\"cd\").\n\nlibrary(demogR)\nlibrary(MetBrewer)\ncols &lt;- met.brewer(\"Johnson\",3)\ndata(goodman)\n# Madagascar\nmlt &lt;- with(goodman, life.table(x=age, nDx=mad.nDx, nKx=mad.nKx, type=\"cd\"))\n# Venezuela\nvlt &lt;- with(goodman, life.table(x=age, nDx=ven.nDx, nKx=ven.nKx))\n# USA\nult &lt;- with(goodman, life.table(x=age, nDx=usa.nDx, nKx=usa.nKx))\n\nplot(mlt$x, mlt$lx, type=\"l\", col=cols[1], lwd=2, xlab=\"Age\", ylab=\"Survivorship\")\nlines(vlt$x, vlt$lx, col=cols[2], lwd=2)\nlines(ult$x, ult$lx, col=cols[3], lwd=2)\nlegend(\"topright\", c(\"Madagascar\",\"Venezuela\",\"USA\"), col=cols, lwd=2)\n\n\n\n\n\n\n\n\nWe obviously don’t have enough age categories for Venezuela and the USA! Turns out that Goodman et al. didn’t need the older ages for their analysis (which was awesome, by the way; one of my all-time favorite papers).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#the-mortality-database",
    "href": "demog1.html#the-mortality-database",
    "title": "6  Demography I: Mortality",
    "section": "6.5 The Mortality Database",
    "text": "6.5 The Mortality Database\nThe Human Mortality Database is an online collection of high-quality mortality data, primarily from more developed countries. To download data, it requires you to have an account. There is actually an R package for interfacing the the database (HMDHFDplus), but I won’t use that for pedagogical reasons. You can download \\(_nD_x\\) and \\(_nK_x\\) data or life tables that have already been calculated for you.\nI’ve downloaded the men’s and women’s life tables for 5-year age classes and 10-year intervals from 1751-2020. Let’s compare the mortality curves for the 1810 life table.\n\nsf &lt;- read.table(file=\"./data/sweden_flt_5x10.txt\", skip=1, header=TRUE)\nsm &lt;- read.table(file=\"./data/sweden_mlt_5x10.txt\", skip=1, header=TRUE)\n## pull out 1810 data\n## use a regex, matching \"1810\"\nf1810 &lt;- sf[grep(\"1810\",sf$Year),]\nm1810 &lt;- sm[grep(\"1810\",sm$Year),]\n# fix the ages\nf1810$Age &lt;- c(0,1,seq(5,110,by=5))\nm1810$Age &lt;- c(0,1,seq(5,110,by=5))\n## plot central death rates\n\nplot(f1810$Age, log(f1810$mx), type=\"l\" , lwd=2, col=\"magenta4\", xlab=\"Age\", ylab=\"log(nMx)\")\nlines(m1810$Age, log(m1810$mx), lwd=2, col=\"blue4\")\nlegend(\"topleft\",c(\"Female\",\"Male\"),lwd=2,col=c(\"magenta4\",\"blue4\"))\n\n## annotate: I actually used p &lt;- locator(7) to get the points\nx &lt;- c(34.87,  49.75,  2.5,  49.25,  11.5, 110.27,  19.25)\ny &lt;- c(-1.16, -4.09, -1.5, -3.26, -4.90, -0.57, -4.42)\ntext(x=x,y=y,labels=c(\"(1)\", \"(2)\", \"(3)\", \"(5)\", \"(4)\", \"(6)\", \"(7)\"))\n\n\n\n\n\n\n\n# I got one out of order when I used locator()!\n\nWe can see all the features of the human mortality schedule here:\n\nAn overall “bathtub” shape\nMale mortality generally higher than female mortality at most ages\nAn early peak in mortality rate\nA bottoming-out of mortality in middle childhood\nA nearly linear increase in mortality on a log-scale starting at age 30\nAn eventual decline in mortality (from very high levels) at the oldest ages\nAn “accident hump” in male mortality in late adolescence or early adulthood",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog1.html#survival-analysis",
    "href": "demog1.html#survival-analysis",
    "title": "6  Demography I: Mortality",
    "section": "6.6 Survival Analysis",
    "text": "6.6 Survival Analysis\nThe life table turns out to be a special—high-on-notation—case of a broader form of statistical analysis called survival analysis or event-history analysis. Here are just a few quick notes on this. Plenty more notation, but generally done more with calculus than with discrete categories. Just remember that whenever you see an integral, it’s a (continuous) sum and when you see a derivative, it’s a slope.\nLet \\(T\\) denote the failure time of individuals (or other items) in a population. \\(T\\) is a continuous non-negative random variable with probability density function \\(f(t)\\) and cumulative probability function (cdf) \\(F(T)\\). That is,\n\\[ F(T) = P(t \\leq t) = \\int_0^t f(u) du \\]\nDefine the survivor function as the probability of an individual surviving until time \\(T\\)\n\\[ S(t) = Pr(T &gt; t) = 1 - F(t) \\]\n\\(S(t)\\) is monotone decreasing and \\(S(0)=1\\) (everyone is alive when they are born).\nThe hazard function \\(h(t)\\) is the instantaneous rate of failure at time \\(t\\):\n\\[ h(t) = \\lim_{\\Delta t \\rightarrow 0} \\frac{Pr[ t \\leq T &lt; t +\n    \\Delta t | T \\geq t ]}{\\Delta t} \\]\nNote that this is a rate, not a probability (except in the limit \\(\\Delta t \\rightarrow 0\\)). This means that \\(h(t)\\) is not bounded from above.\nOur three measures of interest are related in the following way:\n\\[h(t) = f(t)/S(t)\\]\nDoes this look like anything we’ve seen already? \\(f(t)\\), \\(S(t)\\), and \\(h(t)\\) all provide identical and complete characterizations of the distribution of \\(T\\).\nSince \\(S(t) = 1 - F(t)\\), \\(f(t) = -d/dt S(t)\\), we have\n\\[ h(t) = - \\frac{d}{dt} \\log [ S(t) ] \\]\nand\n\\[ S(t) = \\exp \\left( - \\int_0^t h(u) du \\right) \\] In other words, the hazard at age \\(t\\) is the minus the slope of the log-survivor function at that age and the survivor function at age \\(t\\) is the exponential of minus the integrated (or cumulative) hazard to that age.\nThe cumulative hazard \\(H(t) = \\int_0^t h(u)du\\) is related to the survivor function as\n\\[ S(t) = \\exp(-H(t)) \\].\nAs a sanity check, note that at moment of (live) birth (i.e., \\(t=0\\)), an individual has not yet experienced any mortality hazard. So what is the value of the survivor function?\nYou have, no doubt, picked up on this already, but corresponding life-table entries for \\(S(t)\\), \\(f(t)\\), and \\(h(t)\\) are \\(l_x\\), \\(_nd_x\\), and \\(_nm_x\\). Sometimes, it’s good to have our intuitions validated.\n\n6.6.1 Some Features\nTime-to-event or failure data are non-negative: \\(T \\geq 0\\). Times can be continuous, i.e., defined on \\((0,\\infty)\\) or can occur at discrete values. Censoring is one of the features that distinguishes survival analysis from other, more conventional statistical models. Censoring means that we observe an individual case in whole or only partially. Consider a censoring variable \\(C\\). If \\(T_i \\leq C_i\\), then event \\(i\\) is observed; otherwise it is censored. What we actually observe then is \\(X_i = \\min(T_i,C_i)\\).\nIt is very important to account for censored observations in our estimates of survival/hazards. Note with Edward III’s children, if we were trying to estimate a mortality rate and excluded William of Hatfield, who died young but at an unknown age, our estimate would be biased. It would be too high. This is because, even if we don’t know the outcome of a censored individual, they contribute to the risk set while they are observed. Eliminating censored observations is a rookie mistake that biases survival estimates. In the case of William of Hatfield, it biased the estimate upward. However, in many empirical cases where you follow a cohort for a fixed amount of time and cease observations before everyone has exited, it will bias survival estimates downward because you eliminate those who live the longest!\nThere are multiple types of censoring. Left censoring occurs when individuals have been exposed to the event of interest before the study started. This is very common in field studies of mortality where individuals are alive when the study starts. Think: Gombe chimpanzees when Jane Goodall arrived in 1960. Right censoring happens (among other reasons) when a study ends and individuals are still alive. There can also be interval censoring where you lose track of some individuals in a study. This is less of an issue for mortality studies, but can matter a lot for recurrent phenomena like fertility or bouts of some illness.\nWe say that censoring is independent or non-informative if \\(C_i\\) is independent of \\(T_i\\). This won’t always be the case: e.g., a patient may drop out of a study because he becomes very sick and can’t easily leave their bed. In this case becoming lost-to-follow-up and death are probably not independent. Censoring that occurs because a study ends and study individuals are still alive is probably approximately independent. Censoring that occurs because individuals are alive when you start your study is probably not totally independent because those observed individuals had to live long enough to be observed in the first place. This is related to a well-known phenomenon in epidemiology known as length-time bias (I swear; don’t ask me why it’s not time-length bias). Length-time bias leads to over-estimation of survival.\n\nrequire(Epi)\nLexis.diagram(date=c(1960,1980), age=c(0,15), int=1,\n              entry.age=c(0,0,0,0,0),\n              exit.age=c(11.1,13,15,12.4,11.8),\n              birth.date=c(1960.17,1962.5,1963.05,1967.13,1969.33),\n              fail=c(TRUE,TRUE,TRUE,TRUE,TRUE),\n              lwd.life=3,\n              pch.fail=19,\n              cex.fail=1.1)\n## some censored observations\nLexis.lines( entry.age=c(0,0,0),\n            exit.age=c(5.2,2.33,3.17),\n            birth.date=c(1961.57, 1962.4, 1966.07),\n            fail=c(FALSE,FALSE,FALSE),\n            lwd.life=3,\n            col.fail=\"red\",\n            pch.fail=19,\n            cex.fail=1.1)\n\n\n\n\n\n\n\n\nWe can see the impact ignoring the censored observations has on estimating the hazard.\n\n## mortality hazard from figure using only complete observations\n5/sum(c(11.1,13,15,12.4,11.8))\n\n[1] 0.07898894\n\n## now include the censored observations in the risk set\n5/sum(c(11.1,13,15,12.4,11.8,5.2,2.33,3.17))\n\n[1] 0.06756757\n\n\n\n\n6.6.2 Kaplan-Meier Estimator\nIf there is no censoring, a good empirical estimate of the survival function, \\(\\tilde{S}(t)\\), is simply the fraction of individuals with event times greater than \\(t\\). If there is censoring, this is not such a good estimator.\nConsider the data aml in the survival package, which describes the survival times of leukemia patients. The data contains one covariate: whether treatment was maintained or not.\n\nlibrary(survival)\naml\n\n   time status             x\n1     9      1    Maintained\n2    13      1    Maintained\n3    13      0    Maintained\n4    18      1    Maintained\n5    23      1    Maintained\n6    28      0    Maintained\n7    31      1    Maintained\n8    34      1    Maintained\n9    45      0    Maintained\n10   48      1    Maintained\n11  161      0    Maintained\n12    5      1 Nonmaintained\n13    5      1 Nonmaintained\n14    8      1 Nonmaintained\n15    8      1 Nonmaintained\n16   12      1 Nonmaintained\n17   16      0 Nonmaintained\n18   23      1 Nonmaintained\n19   27      1 Nonmaintained\n20   30      1 Nonmaintained\n21   33      1 Nonmaintained\n22   43      1 Nonmaintained\n23   45      1 Nonmaintained\n\n\nDivide the time range into discrete chunks \\(1,2,\\ldots,J\\). The Kaplan-Meier estimator of the survival curve at age \\(j\\) is simply the product of of all \\(i&lt;j\\) of the probabilities of surviving each of these. This is given by:\n\\[ \\tilde{S}(t) = \\prod_{i: t_i &lt; t} 1 - \\frac{d_i}{r_i}, \\]\nthe \\(d_i\\) are observed deaths at time \\(t_i\\) and \\(r_i\\) is the number at risk at time \\(t_i\\).\nThe risk set is where the censored individuals contribute to estimation. While they may not contribute any deaths to estimation, they do contribute to the risk pool. Note that if there are no observed deaths in an interval \\(i\\), the probability of surviving the interval is 1! This means that the KM survival curve will be jagged and only go down when there is an observed death.\nThe risk set at age \\(j\\) is just the risk set at age \\(j-1\\) minus any deaths and censoring events at that age.\n\\[\nr_j = r_{j-1} - d_{j-1} - c_{j-1}\n\\]\nThe risk set is also the sum of all future events (deaths and censorings):\n\\[\nr_j = \\sum_{l \\geq j} (c_l+d_l).\n\\]\n\nleuk_surv &lt;- survfit(Surv(time) ~ 1, data=aml)\nsummary(leuk_surv)\n\nCall: survfit(formula = Surv(time) ~ 1, data = aml)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    5     23       2   0.9130  0.0588      0.80485        1.000\n    8     21       2   0.8261  0.0790      0.68484        0.996\n    9     19       1   0.7826  0.0860      0.63096        0.971\n   12     18       1   0.7391  0.0916      0.57980        0.942\n   13     17       2   0.6522  0.0993      0.48389        0.879\n   16     15       1   0.6087  0.1018      0.43862        0.845\n   18     14       1   0.5652  0.1034      0.39496        0.809\n   23     13       2   0.4783  0.1042      0.31209        0.733\n   27     11       1   0.4348  0.1034      0.27284        0.693\n   28     10       1   0.3913  0.1018      0.23504        0.651\n   30      9       1   0.3478  0.0993      0.19876        0.609\n   31      8       1   0.3043  0.0959      0.16407        0.565\n   33      7       1   0.2609  0.0916      0.13112        0.519\n   34      6       1   0.2174  0.0860      0.10011        0.472\n   43      5       1   0.1739  0.0790      0.07137        0.424\n   45      4       2   0.0870  0.0588      0.02313        0.327\n   48      2       1   0.0435  0.0425      0.00639        0.296\n  161      1       1   0.0000     NaN           NA           NA\n\n## now separate the curves by the treatment covariate\nleuk_surv &lt;- survfit(Surv(time) ~ x, data=aml)\nplot(leuk_surv, col=1:2, lwd=2, xaxs=\"i\", xlab=\"Time (months)\", ylab=\"Fraction Surviving\")\n## weird way to do color, right? seriously old-skool\nlegend(\"topright\", c(\"maintained\", \"not maintained\"), lty=1, col=1:2, lwd=2)\n\n\n\n\n\n\n\n\nLooks like you might prefer being maintained!\nLet’s do something a bit more social/behavioral science. This is an example data set from the great text Singer & Willett, Applied Longitudinal Data Analysis. As an aside: this is the book that Ruth Mace suggested I get to learn how to do the sort of demographic event-history analysis she did with data from The Gambia. The data are a subset of data from Capaldi, Crosby, and Stoolmiller (1996), who measured the grade year of first sexual intercourse in a sample of 180 at-risk heterosexual boys. Boys were followed from Grade 7 up to Grade 12 or until they reported experiencing sexual intercourse for the first time. The data includes a couple of covariates: pt is an indicator of whether the boy experienced a “parental transition” (i.e., separation or reunification) in his family in the last year, and pas is a composite measure of parental antisocial behavior.\nWe’ll approximately recreate a figure or two from Singer & Willett. The first will be the baseline hazards of boys who have and have not experienced a parental transition.\n\n# figure 11.1\nurl &lt;- \"https://stats.idre.ucla.edu/stat/examples/alda/firstsex.csv\"\nfirstsex &lt;- read.csv(url,header=TRUE)\n\nts0 &lt;- survfit( Surv(time, 1-censor)~ 1, conf.type=\"none\", subset=(pt==0), data=firstsex)\nts1 &lt;- survfit( Surv(time, 1-censor)~ 1, conf.type=\"none\", subset=(pt==1), data=firstsex)\n\nh0 &lt;- ts0$n.event/ts0$n.risk\nh1 &lt;- ts1$n.event/ts1$n.risk\n\nplot(ts0$time, h0, type=\"l\", lwd=3, col=\"magenta3\",\n     ylab=\"Estimated Hazard probability\", xlab=\"Grade\", \n     ylim=c(0.0, 0.5), xlim=c(7, 12))\nlines(ts1$time, h1, lwd=3, col=\"blue3\")\nlegend(\"topleft\", c(\"pt = TRUE\",\"pt = FALSE\"), lwd=3, col=c(\"blue3\",\"magenta3\"))\n\n\n\n\n\n\n\n\nPretty suggestive evidence that an unstable family life makes boys (at least) more likely to initiate sex (early).\nNow look at the survivor function.\n\nplot(ts0$time, ts0$surv, type=\"l\", lwd=3, col=\"magenta3\",\n     ylab=\"Estimated Survival Function\", xlab=\"Grade\", \n     ylim=c(0.0, 1.0), xlim=c(7, 12))\nlines(ts1$time, ts1$surv, lwd=3, col=\"blue3\")\n## median survival\nabline(h=c(0.5), lty=2)\nlegend(\"topright\", c(\"pt = TRUE\",\"pt = FALSE\"), lwd=3, col=c(\"blue3\",\"magenta3\"))\n\n\n\n\n\n\n\n\nNearly 50% of boys who did not experience a parental transition remained virgins, whereas less than 20% of the boys who did experience a parental transition did.\nHere’s a plot that isn’t in Singer & Willett, but it gives you a sense of the sorts of things you can do. It’s often easier to see differences in either the hazard (as above) or sometimes the cumulative hazard than it is in the survival curves.\n\nplot(ts1, lwd=3, col=\"blue3\", conf.int=0, cumhaz=TRUE, \n     xlab=\"Grade\", ylab=\"Cumulative Hazard\", xlim=c(7,12))\nlines(ts0, lwd=3, col=\"magenta3\", conf.int=0, cumhaz=TRUE)\nlegend(\"topleft\", c(\"pt = TRUE\",\"pt = FALSE\"), lwd=3, col=c(\"blue3\",\"magenta3\"))\n\n\n\n\n\n\n\n\nSurvival analysis is one of the areas where I have literally never seen any tidyverse work. Surely it exists. The survival package, which actually pre-dates the development of R, is pretty old-school.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Demography I: Mortality</span>"
    ]
  },
  {
    "objectID": "demog2.html",
    "href": "demog2.html",
    "title": "7  Demography II: Matrix Population Models",
    "section": "",
    "text": "7.1 Population Growth",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#the-leslie-matrix",
    "href": "demog2.html#the-leslie-matrix",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.3 The Leslie Matrix",
    "text": "7.3 The Leslie Matrix\nThe Leslie matrix is a special matrix for demography and population biology. It is referred to as a Leslie Matrix after its inventor Sir Paul Leslie (Leslie 1945, 1948). A Leslie matrix contains: (1) age-specific fertilities along the first row, (2) age-specific survival probabilities along the subdiagonal, and (3) zeros everywhere else. Here is an example of a \\(5 \\times 5\\) Leslie matrix:\n\\[\n\\mathbf{A} = \\left[ \\begin{array}{ccccc}\n                   0     & F_{2}   & F_{3} & F_{4} & F_{5}\\\\\n                   P_{1} & 0       & 0     & 0     & 0 \\\\\n                   0     & P_2     & 0     & 0     & 0\\\\\n                   0     & 0       & P_3   & 0     & 0 \\\\\n                   0     & 0       & 0     & P_{4} & 0\n\\end{array} \\right]\n\\]\nThe Leslie matrix is a special case of a projection matrix for an age-classified population. With age-structure, the only transitions that can happen are from one age to the next and from adult ages back to the first age class. Can you imagine a projection matrix structured by something other than age?\nA couple points: all the age classes are the same width (cf. an abridged life table). This is also the projection interval. So if your age classes are five years wide, each step of your population projection is five years. If you want to get an annual growth rate, you need to divide by five. If you need to have classes with different durations, you can’t use a Leslie matrix. You need a matrix that models more general stages. The projection interval will remain constant, but you can have different amounts of time spent (sojourn times) in different stages. More on this later.\nLeslie matrices are sparse. Most of their elements (particularly when you have more age classes) are zero. These elements are known as structural zeros. By the definition of our life cycle, they represent impossible transitions. The Leslie matrix contains only two logical transitions: individuals get one age-class older per time step; new individuals can be created from older ones. For example, in an age-structured life cycle, you can’t make a transition from the fourth to the second age class. Element \\(a_{42}\\) is a structural zero. A structural zero contrasts with a transition that is logically possible in our life cycle but just happens to have a transition probability/rate equal to zero.\n\n7.3.1 The Life Cycle Diagram\nIt is useful to think of the matrix entries in a life-cycle manner. The entry \\(a_{ij}\\) is the transition probability of going from age \\(j\\) to age \\(i\\):\n\\[\n    a_{ij} \\equiv a_{i \\leftarrow j}.\n\\]\nSo these rates mean that you end up in the row index and start in the column index. It’s worth noting that this convention is different than the use of matrices in many applications. For example, social mobility matrices, that have historically modeled the occupational status of a son compared to his father, typically move individuals from row to column. Either way works; you just need to know what the convention is in any given field.\nWe can formalize the life-cycle approach by noting the linkages between the projection matrix and the life-cycle graph. A life-cycle graph is a digraph (or directed graph) composed two things: (1) nodes, which represent the states (ages, stages, subgroups, localities, etc.) and (2) edges, which represent transitions between states. The following figure presents a simple age-structured life cycle with five ages and reproduction in age classes 2-5.\n\n\n\nAn age-classified life-cycle graph with five age classes.\n\n\nThere are a number of desirable properties of demographic projection matrices. These properties are required for all the important results that we will discuss to apply. Every demographic matrix is non-negative (all its entries are greater than or equal to zero). In general, we are only interested in non-negative matrices since all survival probabilities and fertility rates must be non-negative. Indeed, any rate of interest in demography must be positive. We have seen that it is important for the elements of a structured population model to come to some sort of stable distribution. However, not all population models do this. Fortunately, the conditions that allow a population to converge to its stable age distribution are simple and the use of the life cycle graph greatly facilitates determining if the conditions do indeed apply. In order for a population to converge on its stable population structure, it must be irreducible. A matrix is irreducible if and only if there is a path between every node and every other node in the life cycle graph. Irreducibility is necessary but not sufficient for stability. The second, sufficient condition is called primitivity. An irreducible non-negative matrix is primitive if all its elements become positive when raised to sufficiently high powers. A matrix is primitive if the greatest common divisor of all loops in the corresponding life-cycle graph is 1.\nHere are two examples of reducible life cycles that will not converge to stable distributions.\n\n\n\nA reducible life cycle, with a post-reproductive stage.\n\n\nThe life-cycle graph makes it easy to determine whether there are any stages that cannot contribute (even indirectly) to another stage. A common case of such a life cycle is one with post-reproductive stages. In this graph, age-class 5 individuals cannot contribute to any of the preceding classes (because they’re post-reproductive!).\nThere are other ways to be reducible. Here is a fanciful example.\n\n\n\nA weird reducible life cycle with who knows what going on!\n\n\nNext, we have an imprimitive life cycle. Reproduction only occurs in the even ages. This causes bulges in the age pyramid that never get evened out. Consequently, the population never converges to a stable age distribution. Be careful with the terminology as it can be confusing: you want your matrix to be irreducible but primitive, not reducible and imprimitive.\n\n\n\nA imprimitive life cycle, where reproduction only happens in ages that are multiples of each other.\n\n\nTwo ways of insuring that a life cycle will be primitive are: (1) having two consecutive age classes in which reproduction occurs or (2) having a life cycle with a self loop.\n\n\n\nThe same life cycle with a self-loop makes it primitive.\n\n\nThis example is a bit of a preview. By putting a self-look on the fourth age class, we’ve turned it from an age class into a stage. The matrix representation of this life cycle will include an element on the diagonal of the matrix in the fourth row and fourth column.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#growth-of-structured-populations",
    "href": "demog2.html#growth-of-structured-populations",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.4 Growth of Structured Populations",
    "text": "7.4 Growth of Structured Populations\nHaving redefined the population model in matrix form, we can write it in a more compact notation of matrix algebra:\n\\[\n\\mathbf{n}(t+1) = \\mathbf{A} \\mathbf{n}(t)\n\\]\nLet’s now assume that there is a solution to the exponential growth model in a structured population. Write the population model as:\n\\[\n\\mathbf{An} = \\lambda \\mathbf{n}\n\\]\nNow solve for \\(\\lambda\\). The rules of linear algebra make this a little trickier than just dividing both sides by \\(\\mathbf{n}\\). Here are the steps we need:\n\\[\n  \\mathbf{An} - \\lambda \\mathbf{n} = 0\n\\]\n\\[\n  \\mathbf{An} - \\lambda \\mathbf{I n} = 0\n\\]\n\\[\n  (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{n} = 0\n\\]\n\\(\\mathbf{I}\\) is an identity matrix of the same rank as \\(\\mathbf{A}\\) (ones along the diagonal, zeros elsewhere).\nIt’s a fact of linear algebra, that the solution to this equation exists only if the determinant of the matrix \\((\\mathbf{An} - \\lambda \\mathbf{I})\\) is zero. For the \\(2 \\times 2\\) case, the determinant is simple. For any \\(2 \\times 2\\) matrix the determinant is given by:\n\\[  \\det \\left[ \\begin{array}{cc}\n    a  & b  \\\\\n    c & d\n  \\end{array} \\right] = ad - bc\n\\]\nDeterminants of matrices of larger rank are, necessarily, more complex.\nSo, using our fact of linear algebra, we can proceed with the calculation:\n\\[ (\\mathbf{A} - \\lambda \\mathbf{I}) = \\left[ \\begin{array}{cc}\n        f_{1}  & f_{2}  \\\\\n        p_{1} & 0\n    \\end{array} \\right] - \\left[ \\begin{array}{cc}\n            \\lambda & 0  \\\\\n        0       & \\lambda\n    \\end{array} \\right] = \\left[ \\begin{array}{cc}\n        f_{1} - \\lambda   & f_{2}  \\\\\n        p_{1} & - \\lambda  \n    \\end{array} \\right]\n\\]\n\\[\n  \\det (\\mathbf{A} - \\lambda \\mathbf{I}) = -(f_{1} - \\lambda)\n    \\lambda - f_{2} p_{1}\n    \\] Multiply out:\n\\[ \\lambda^{2} - f_{1} \\lambda - f_{2}p_{1} = 0  \\]\nUse the quadratic equation to solve for \\(\\lambda\\):\n\\[\n\\frac{-f_{1} \\pm \\sqrt{f_{1}^{2} - 4f_{2}p_{1}}}{2f_{1}}\n\\]\nWe can use a numerical example to make this more concrete. Define:\n\\[\n      \\mathbf{A} = \\left[ \\begin{array}{cc}\n    1.5  & 2  \\\\\n    0.5 & 0\n   \\end{array} \\right]\n\\]\n\\[\n\\det (\\mathbf{A} - \\lambda \\mathbf{I}) = \\left[ \\begin{array}{cc}\n    1.5 - \\lambda   & 2  \\\\\n    0.5 & - \\lambda  \n  \\end{array} \\right]\n\\]\n\\[ \\lambda^{2} - 1.5 \\lambda - 1 = 0 \\]\n\\[ (\\lambda - 2)(\\lambda + 0.5) = 0 \\]\nMatrix \\(\\mathbf{A}\\) has two roots: \\(\\lambda=2\\) and \\(\\lambda=-0.5\\). These roots are known as the eigenvalues of matrix \\(\\mathbf{A}\\). One of these eigenvalues is the growth rate of the population, but which one? For this \\(2 \\times 2\\) example, there are two roots. For larger matrices, there are more. In fact, there are always as many eigenvalues as there are rows (or columns since we are dealing with square matrices) in the matrix. One could imagine this getting a bit hairy. Thankfully, a handy theorem from linear algebra, known as the Perron-Frobenius theorem, tells us that a square, non-negative matrix that is irreducible and primitive will have a single eigenvalue which is positive, real, and strictly greater than all the others. Our example fulfills these criteria, so we may conclude that the growth rate of this population is \\(\\lambda=2\\). This is reassuring since a negative growth rate is an idea that’s a little hard to fathom – at least for the moment. Note that the eigenvalues are the multiplicative growth factors, not instantaneous rates. A declining population will not have a negative multiplicative growth factor! It will have a value of \\(1&gt;\\lambda&gt;0\\). Take the logarithm of that to get the growth rate and that will be negative.\nWe began this chapter with the statement that for an age-structured population to grow geometrically, it must maintain constant ratios between its age classes. There is a special vector that goes hand-in-hand with the eigenvalue called, strangely enough, an eigenvector. Let’s keep up with our example. Remember that the eigenvalues of this model are \\(\\lambda = 2\\) and \\(\\lambda = -0.5\\). That means that we can write our model as:\n\\[\n  \\mathbf{A} = \\left[ \\begin{array}{cc}\n      1.5  & 2  \\\\\n      0.5 & 0\n  \\end{array} \\right] \\left[ \\begin{array}{c}\n    n_{1} \\\\\n    n_{2}\n  \\end{array} \\right] = \\left[ \\begin{array}{c}\n    2n_{1} \\\\\n    2n_{2}\n   \\end{array} \\right]\n\\]\nWe now solve this system to two equations and find that \\(n_{1} = 4n_{2}\\) is a solution. That is, if there are four times the number of stage ones as there are stage twos, the population will grow geometrically.\n\n7.4.1 Projection, The Simplest Form of Analysis\nMost demographers use matrices primarily for projection of populations into the future. Projection is in fact the simplest form of analysis of a population model. When you make a projection, you ask the question: “How big will the population be if the following rates apply for the next \\(t\\) years?” Your projection will only apply to the specific assumptions you build into it, but it may nonetheless yield more general, qualitative insight.\nConsider the following plot of a population that does not start off in its stable age distribution. Even though the rates remain constant, the population oscillates around its general upward trajectory. If we let it run long enough, the oscillations dampen and we see the straight line on semilog axes, indicating geometric increase.\n\npx &lt;- c(.92,.95,.95,.95,.95,.95) # some arbitrary survival probs.\nmx &lt;- c(0,0,.05,.1,.25,.5,1)     # some arbitrary fertilities\nlx &lt;- c(1,px)                    # a quick way to get lx from px\nlx &lt;- cumprod(lx)\nsum(lx*mx)                       # the net reproduction number\n\n[1] 1.410478\n\nk &lt;- length(px)+1                # i.e., 7\nA &lt;- matrix(0, nrow=k, ncol=k)   # make a 7x7 matrix of zeros\nA[row(A) == col(A)+1] &lt;- px      # put px on the subdiag\nA[1,] &lt;- mx                      # put mx on first row\n\n## now project\n# initial population vector\nno &lt;- matrix(c(0,0,0,0,0,1,1),nrow=7)\npop &lt;- no\n# initiate a matrix to hold the projections\nN &lt;- NULL\n# add initial population\nN &lt;- cbind(N,pop)\n# number of time steps to project\ntmax &lt;- 100\n# projection\nfor (i in 1:tmax){\n  pop &lt;- A %*% pop\n  N &lt;- cbind(N,pop)\n}\n\n## plot\nplot(0:100,log(apply(N,2,sum)),type=\"l\",col=\"blue\", lwd=2,\n     xlab=\"Time\", ylab=\"log(Population Size)\")\n\n\n\n\n\n\n\n\nThe population, which starts very far away from a stable age distribution, oscillates for a about 40 time periods before it settles into geometric growth (linear on a log scale).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#left-eigenvectors-of-the-projection-matrix",
    "href": "demog2.html#left-eigenvectors-of-the-projection-matrix",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.6 Left Eigenvectors of the Projection Matrix",
    "text": "7.6 Left Eigenvectors of the Projection Matrix\nIn matrix algebra, multiplication is not commutative, \\(\\mathbf{A B} \\neq \\mathbf{B A}\\). Thus, the left eigenvector of a matrix is distinct from the right eigenvector:\n\\[\n    \\mathbf{v}^{*}\\mathbf{A} = \\lambda\\mathbf{v}^{*},\n\\]\nwhere the asterisk denotes the complex-conjugate transpose.\nDefine \\(\\mathbf{U}\\) as a matrix of eigenvectors where each column \\(i\\) is the \\(i\\)th eigenvector of \\(\\mathbf{A}\\), and \\(\\mathbf{\\Lambda}\\) as a matrix with the eigenvalues of \\(\\mathbf{A}\\) along the diagonal and zeros elsewhere, we have\n\\[\n\\begin{align}\n    \\mathbf{A U}  & =  \\mathbf{U \\Lambda}  \\\\\n    \\mathbf{\\Lambda}& =  \\mathbf{U^{-1} A U} \\\\\n    \\mathbf{U^{-1} A} & =  \\mathbf{\\Lambda U^{-1}}\n\\end{align}\n\\]\nThis is the matrix formula for an eigensystem, suggesting that the rows of \\(\\mathbf{U^{-1}}\\) must be the left eigenvectors of \\(\\mathbf{A}\\).\nThe left eigenvector, corresponding to the dominant eigenvalue, of a demographic projection matrix has a particular interpretation. It represents the age-specific reproductive value of the population. The reproductive value of the \\(i\\)th age class is the expected contribution of individuals age \\(i\\) to the population in the distant future. This is an important concept in evolutionary biology which we take up repeatedly.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#spectral-decomposition-of-the-projection-matrix",
    "href": "demog2.html#spectral-decomposition-of-the-projection-matrix",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.7 Spectral Decomposition of the Projection Matrix",
    "text": "7.7 Spectral Decomposition of the Projection Matrix\nWhat do we mean by reproductive value being the contribution to the population in the distant future? Suppose we are given an initial population vector, \\(\\mathbf{n}(0)\\). We can write \\(\\mathbf{n}(0)\\) as a linear combination of of the right eigenvectors, \\(\\mathbf{u}_i\\) of the projection matrix \\(\\mathbf{A}\\).\n\\[\n\\mathbf{n}(0) = c_1 \\mathbf{u}_1 + c_2 \\mathbf{u}_2 + \\cdots + c_k \\mathbf{u}_k\n\\]\nwhere the \\(c_i\\) are a set of coefficients.\nWe can collect the eigenvectors into a matrix where each column \\(i\\) is simply the \\(i\\)th eigenvecotr and the coefficients into a vector and re-write this equation as:\n\\[\n\\mathbf{n}(0) = \\mathbf{U} \\mathbf{c}.\n\\]\nFrom this, it is clear that \\(\\mathbf{c} = \\mathbf{U}^{-1} \\mathbf{n}(0)\\). \\(\\mathbf{U}^{-1}\\) is just the matrix of left eigenvectors (or their complex-conjugate transpose), so \\(c_i = \\bf{v}_i^* \\mathbf{n}(0)\\).\nProject the initial population vector \\(\\mathbf{n}(0)\\) forward by multiplying it by the projection matrix \\(\\mathbf{A}\\):\n\\[\n\\begin{align}\n  \\mathbf{n}(1) & =  \\mathbf{A} \\mathbf{n}(0) \\\\\n  \\mbox{} & =  \\sum_i c_i \\mathbf{A} \\mathbf{u}_i \\\\\n  \\mbox{} & =  \\sum_i c_i \\lambda_i \\mathbf{u}_i\n\\end{align}\n\\]\nMultiply again!\n\\[\n\\begin{align}\n  \\mathbf{n}(2) & =  \\mathbf{A} \\mathbf{n}(1) \\\\\n  \\mbox{} & =  \\sum_i c_i \\lambda_i \\mathbf{A} \\mathbf{u}_i \\\\\n  \\mbox{} & =  \\sum_i c_i \\lambda_i^2 \\mathbf{u}_i\n\\end{align}\n\\]\nWe could keep going, but at this point it isn’t hard to believe that the following holds:\n\\[\n  \\mathbf{n}(t) = \\sum_i c_i \\lambda_i^t \\mathbf{u}_i\n\\]\nThis is equivalent to:\n\\[\n  \\mathbf{n}(t) = \\sum_i \\lambda_i^t \\mathbf{u}_i \\bf{v}_i^* \\mathbf{n}(0)\n\\]\nThis is known as the Spectral Decomposition of the projection matrix \\(\\mathbf{A}\\)\nIt is instructive to compare this to the solution for population growth in an unstructured (i.e., scalar) population, characterized by a geometric rate of increase \\(a\\) for one time step\n\\[ N(t+1) = a N(t), \\] and for an arbitrary number of steps\n\\[ N(t) = N(0) a^t. \\]\nFor the scalar case, the solution is exponential. For a \\(k\\)-dimensional matrix, this solution means that the population size at time \\(t\\) is a weighted sum of \\(k\\) exponentials. While both depend on the initial conditions, the \\(k\\)-dimensional case weights the initial population vector by the reproductive values of the \\(k\\) classes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#matrix-perturbations",
    "href": "demog2.html#matrix-perturbations",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.8 Matrix Perturbations",
    "text": "7.8 Matrix Perturbations\nThis derivation follows Caswell (2001). As a note of interest, I took the first edition (1989) of Caswell’s book to the field in Borneo with me as a graduate student. I still have that book. It smells like rainforest. I have Hal to thank for much of what I know about matrix population models and population biology more generally. We start from the general matrix population model:\n\\[\n    \\mathbf{A u} = \\lambda \\mathbf{u}\n\\]\nNow we perturb the system, adding a scootch to each of the terms\n\\[\n    (\\mathbf{A} + d\\mathbf{A}) (\\mathbf{u} + d\\mathbf{u}) =\n    (\\lambda + d\\lambda) (\\mathbf{u} + d\\mathbf{u})\n\\]\nMultiply all the products and discard the second-order terms such as \\((d\\mathbf{A})(d\\mathbf{u})\\)—squared scootches are small enough to ignore.\n\\[\n    \\mathbf{A w} + \\mathbf{A}(d\\mathbf{u}) + (d\\mathbf{A})\\mathbf{u}  =\n    \\lambda \\mathbf{u} + \\lambda(d\\mathbf{u}) + (d\\lambda) \\mathbf{u}.\n\\]\nSimplify this to yield\n\\[\n    \\mathbf{A}(d\\mathbf{u}) + (d\\mathbf{A}) \\mathbf{u}  =\n    \\lambda (d\\mathbf{u}) + (d\\lambda) \\mathbf{u}.\n\\]\nMultiply both sides by \\(\\mathbf{v^{*}}\\) to get\n\\[\n    \\mathbf{v^{*}}\\mathbf{A}(d\\mathbf{u}) +\n    \\mathbf{v^{*}}(d\\mathbf{A}) \\mathbf{u} = \\lambda \\mathbf{v^{*}}\n    (d\\mathbf{u}) + \\mathbf{v^{*}} (d\\lambda) \\mathbf{u}.\n\\]\nFrom the definition of a left eigenvector, we know that the first term on the left-hand side is the same as the first term on the right-hand side. Similarly, because the right and left eigenvectors are scaled so that \\(\\langle \\mathbf{w,v} \\rangle = 1\\), the last term simplifies to \\(d\\lambda\\). We are left with\n\\[\n    \\mathbf{v^{*}} d\\mathbf{A} \\mathbf{u} = d\\lambda.\n\\]\nWhen we do a perturbation analysis, we typically only change a single element of \\(\\mathbf{A}\\). Thus the basic formula for the sensitivity of the dominant eigenvalue to a small change in element \\(a_{ij}\\) is\n\\[\n    \\frac{\\partial \\lambda}{\\partial a_{ij}} = v_{i} u_{j}.\n\\]\nIn other words, the sensitivity of fitness to a small change in projection matrix element \\(a_{ij}\\) is simply the \\(i\\)th element of the left eigenvector weighted by the proportion of the stable population in the \\(j\\)th class (assuming vectors have been normed such that \\(\\langle \\mathbf{v,u} \\rangle =1\\)). Keep that in mind.\nEigenvalue Sensitivities are Linear Estimates of the Change in \\(\\lambda_1\\), Given a Perturbation, as illustrated in the following figure. Here, we will recycle and modify our code from discussion of the marginal value theorem in chapter Chapter 3.\n\nx &lt;- seq(1,18,length=500)\n# fitness is an unknown function, but probably fp&gt; 0 fpp &lt; 0\n# fp == deriv of f; fpp == 2nd deriv of f\nf &lt;- function(x) {\n  1 - exp(-0.2*(x-1))\n}\n# derivative of the utility function\nfprime &lt;- function(x) {\n  0.2*exp(-0.2*x)*exp(0.2)\n}\n\n# f + fp*(z-x) = 0\n# z = x -(f/fp)\n# solve for tangency; find the root of this\nxinter &lt;- function(x) {\n  return(x - f(x)/fprime(x))\n}\n\nsoln &lt;- uniroot(xinter,c(0,10))\n#xx &lt;- seq(7,10,length=100)\n\nplot(x,f(x), type=\"l\", lwd=2, xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE, frame=TRUE,\n     xlab=expression(a[ij]),\n     ylab=expression(lambda),\n     xlim=c(0,20), ylim=c(0,1))\nlines(x,(f(soln$root)/soln$root)*x,col=\"red\")\nsegments(soln$root,0,soln$root,f(soln$root), lty=2, col=\"red\")\nsegments(0,f(soln$root),soln$root,f(soln$root), lty=2, col=\"red\")\n\n\n\n\n\n\n\n\nNote that because I was recycling code, I kludged a bit. It’s a schemaic plot, so the values on the axes are meaningless, so I suppressed them using axes=FALSE and frame=TRUE.\nThe black curve is a hypothetical fitness function of the matrix element \\(a_{ij}\\). Presumably, in the absence of any trade-offs, an increase in this matrix element should increase fitness (those elements are survival probabilities and fertility rates, after all). However, we expect diminishing marginal improvement as we increase the value by a lot. Hence, the curve is concave. The red solid line is a tangent to the curve. It is a linear approximation of the slope of the curve at the value of \\(a_{ij}\\). For small perturbations, this should provide a decent measure of how much fitness will change. Note that the black line is, in fact, quite linear for the region around its observed value (dashed red lines), such that it’s hard to tell exactly where the tangent hits without those dashed lines.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#elasticities",
    "href": "demog2.html#elasticities",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.9 Elasticities",
    "text": "7.9 Elasticities\nAnother measure of the change in a matrix given a small change in an underlying element is the eigenvalue elasticity:\n\\[ e_{ij} = \\frac{\\partial \\log \\lambda}{\\partial \\log a_{ij}}. \\]\nElasticities are proportional sensitivities: they measure the linear change on a log scale. Given a 1% change in vital rate \\(a_{ij}\\) what percentage change will accompany \\(\\lambda\\)? This is an elasticity. A very important property of elasticities is that they sum to one: \\(\\sum_{i,j} e_{ij} = 1\\). As such, one can think of the elasticity of \\(\\lambda\\) with respect to \\(a_{ij}\\) as measuring the proportion of the total selection on that trait. This interpretation is somewhat limited because it is conditioned on all other traits remaining constant and on the change being small.\nAnother interesting property of elasticities is that the summed elasticities of all outgoing transitions from a stage will be equal to the summed elasticities of all incoming transitions. In terms of the projection matrix, this means that the elasticities of all elements in column \\(i\\) will be equal to the summed elasticities of row \\(i\\). It also provides us with some intuition about why the elasticities of fertility are so much lower than the elasticities of survival, at least earlier in the reproductive career.\n This fact also provides inuition about why the elasticity of pre-reproductive survival (especially infant survival) is so high.\n\n\n\nThe elasticity of infant survival is equal to the sum of all the fertility elasticities. Moreover, the elasticity of subsequent, pre-reproductive survival is the same as all prior previous survival elasticities in the life cycle.\n\n\n\n7.9.1 Relationship to Hamilton\nThe elasticities of the sub-diagonal elements of the Leslie matrix (i.e., the \\(P_i\\) from the life-cycle graph) are essentially the same thing that Hamilton (1966) calculates by differentiating the discrete-time Euler-Lotka equation. A major advantage of the matrix approach is the ease with which the sensitivities and elasticities can be calculated. Furthermore, the approach is easily extended to other types of structured models (i.e., not just age-structure) and to related measures such as the second derivatives of fitness, which provide an estimate the form of selection (stabilizing, directional).\nConceptually, it is illuminating to think of the force of selection on a given life-cycle transition as being simply a product of the reproductive value of the receiving stage and the fraction of the stable age distribution from the sending stage.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#calculating-sensitivities",
    "href": "demog2.html#calculating-sensitivities",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.10 Calculating Sensitivities",
    "text": "7.10 Calculating Sensitivities\n\n## Matrix A from above\n\n# calculate eigenvalues/eigenvectors\nev &lt;- eigen(A)\n# need to make sure we get the dominant eigenvalue\nlmax &lt;- which(Re(ev$values)==max(Re(ev$values)))\n# take real part (imaginary part will be zero anyway)\nlambda &lt;- Re(ev$values[lmax])\n# right eigenvectors\nU &lt;- ev$vectors\n# dominant right eigenvector\nu &lt;- abs(Re(U[,lmax]))\n# left eigenvectors are the complex conjugate transpose of the inverse of U\nV &lt;- Conj(solve(U))\n# dominant left eigenvector\nv &lt;- abs(Re(V[lmax,]))\n# outer product of v and u\ns &lt;- v%o%u\n# sensitivities of non-existent transitions are zero \ns[A == 0] &lt;- 0\ns\n\n          [,1]     [,2]      [,3]      [,4]      [,5]       [,6]       [,7]\n[1,] 0.0000000 0.000000 0.1283009 0.1152267 0.1034848 0.09293949 0.08346873\n[2,] 0.1888559 0.000000 0.0000000 0.0000000 0.0000000 0.00000000 0.00000000\n[3,] 0.0000000 0.182892 0.0000000 0.0000000 0.0000000 0.00000000 0.00000000\n[4,] 0.0000000 0.000000 0.1761393 0.0000000 0.0000000 0.00000000 0.00000000\n[5,] 0.0000000 0.000000 0.0000000 0.1640102 0.0000000 0.00000000 0.00000000\n[6,] 0.0000000 0.000000 0.0000000 0.0000000 0.1367773 0.00000000 0.00000000\n[7,] 0.0000000 0.000000 0.0000000 0.0000000 0.0000000 0.08786182 0.00000000\n\n\nThat’s cool. A plot would be better.\n\nsurv_sens &lt;- s[row(A)==col(A)+1]\nplot(1:6, surv_sens, type=\"l\", lwd=3, \n     xlim=c(1,7), ylim=c(0.05,0.2), xlab=\"Age\", ylab=\"Sensitivity\")\nlines(4:7,s[1,4:7], lwd=3, col=\"red\")\n\n\n\n\n\n\n\n\nSensitivities of survival are generally higher than those for fertility, except for the last age class. Survival and fertility sensitivities decline with age, but the decline is more pronounced for survival.\nLet’s do something a little more interesting. We’ll calculate mortality and fertility schedules from the three countries in the goodman data in my package demogR. We can then use those data to construct Leslie matrices.\n\nrequire(demogR)\ndata(goodman)\n## Madagascar\nmlt &lt;- with(goodman, life.table(x=age, nDx=mad.nDx, nKx=mad.nKx))\nmmx &lt;- goodman$mad.bx/goodman$mad.nKx\n## Venezuela\nvlt &lt;- with(goodman, life.table(x=age, nDx=ven.nDx, nKx=ven.nKx))\nvmx &lt;- goodman$ven.bx/goodman$ven.nKx\n## USA\nult &lt;- with(goodman, life.table(x=age, nDx=usa.nDx, nKx=usa.nKx))\numx &lt;- goodman$usa.bx/goodman$usa.nKx\n\n## Now make the Leslie matrices\nmad &lt;- leslie.matrix(lx=mlt$nLx, mx=mmx)\nven &lt;- leslie.matrix(lx=vlt$nLx, mx=vmx)\nusa &lt;- leslie.matrix(lx=ult$nLx, mx=umx)\n\n## Calculate everything using eigen.analysis function in demogR\nmea &lt;- eigen.analysis(mad)\nvea &lt;- eigen.analysis(ven)\nuea &lt;- eigen.analysis(usa)\n\n## you can plot leslie.matrix objects in demogR\npar(mfrow=c(1,3))\nplot(mea$sens, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"Madagascar\")\n## ven\nplot(vea$sens, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"Venezuela\")\n## usa\nplot(uea$sens, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"USA\")\n\n\n\n\n\n\n\n\nCompare the sensitivities to the elasticities.\n\n## you can plot leslie.matrix objects in demogR\npar(mfrow=c(1,3))\nplot(mea$elas, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"Madagascar\")\n## ven\nplot(vea$elas, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"Venezuela\")\n## usa\nplot(uea$elas, col=c(\"blue4\",\"magenta4\"), lwd=3)\nlegend(\"topright\",c(\"survival\",\"fertility\"),lwd=3, col=c(\"blue4\",\"magenta4\"))\ntitle(\"USA\")\n\n\n\n\n\n\n\n\nHal Caswell has an open-access text on sensitivity analysis and matrix methods in demography.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#population-growth-with-age-structure",
    "href": "demog2.html#population-growth-with-age-structure",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.2 Population Growth with Age Structure",
    "text": "7.2 Population Growth with Age Structure\nIn this section, we will introduce the idea of population projection matrices. The special case of such matrices for projecting an age-structured population is known as the Leslie matrix, after the British biologist, P.H. Leslie, who first formalized the method in his 1945 paper.\nWe start with a simple example, where we extend the analysis of population growth to a situation where the population is structured, meaning that different groups of individuals within the population (e.g., age classes, sexes, geographic regions) are characterized by different demographic rates. Assume that we have an organism with two age-classes. Then the population growth is now described by the following two equations:\n\\[\n\\begin{align}\n  n_1(t+1) & =  f_{1}n_{1}(t) + f_{2}n_{2}(t) \\\\\n  n_2(t+1) & =  p_{1}n_{1}\n\\end{align}\n\\]\nwhere \\(n_{1}\\) is the number in stage 1, \\(n_{2}\\) is the number in stage 2, \\(f_{1}\\) is the fertility of stage 1 individuals, \\(f_{2}\\) is the fertility of stage 2 individuals, and \\(p_{1}\\) is the survivals of 1’s to age class 2.\nOne question we wish to answer: Is there a unique exponential growth rate for such a population analogous to the unstructured case? Imagine you start with a total population size (\\(N(0) = n_{1}(0) + n_{2}(0)\\)) of 10. What will the population look like in one time step? If there are 10 ones and zero twos, then we have\n\\[\n\\begin{align}\n        n_{1}(1) & = 10f_{1}  \\\\\n        n_{2}(1) & = 10p_{1}\n\\end{align}\n\\]\nHowever, if there are zero ones and 10 twos, then we have a very different situation:\n\\[\n\\begin{align}\n   n_{1}(1) & = 10f_{2} \\\\\n   n_{2}(1) & = 0\n\\end{align}\n\\]\nThese two populations can only increase at the same rate in the degenerate case where \\(f_1=f_2\\) and \\(p_1=0\\). The solution to this apparent paradox is that a structured population will grow geometrically only when the ratios between the different classes of the population remain constant. In the age-structured case, we call this the stable age distribution and in the state-structured case, we call it the stable stage distribution.\nThese two sets of equations can be represented in the compact notation of matrices and the rules for working with them derived from linear algebra.\nFirst, some definitions. A matrix is a rectangular array of numbers. We typically represent matrices by bold face upper-case letters. We indicate an element of a matrix by the lower case letter in plain face, indexed by subscripted row followed by column numbers:\n\\[\n\\mathbf{A} = \\left[ \\begin{array}{cc}\n                     a_{11} & a_{12}  \\\\\n             a_{21} & a_{22}\n    \\end{array} \\right]\n\\] The element \\(a_{12}\\) therefore represents the matrix entry from the first row and second column of matrix \\(\\mathbf{A}\\)\nA vector is simply a list of numbers. Vectors are typically bold face lower-case letters and elements are indexed by their position in the vector with a subscripted number. Vectors can come as either row or column vectors.\n\\[\n\\mathbf{n(t)} = \\left[ \\begin{array}{c}\n                       n_{1} \\\\\n                   n_{2} \\\\\n                   n_{3}\n                       \\end{array} \\right]\n\\]\nA scalar is a single number: \\(\\lambda = 1.05\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#constructing-a-leslie-matrix",
    "href": "demog2.html#constructing-a-leslie-matrix",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.5 Constructing a Leslie Matrix",
    "text": "7.5 Constructing a Leslie Matrix\nThat example is cute, but we want to know how to construct Leslie matrices from actual demographic data.\nNot gonna lie: the accounting to make a Leslie matrix is a bit hairy in its details. Conceptually, however, it is simple. The first row of the matrix contains the fertility elements. The subdiagonal contains the survival elements. How we move from demographic data to matrix elements is a bit tricky, but not actually difficult.\nA good thing to get out of the way deals with age classes. We typically index the matrix entries with \\(i\\) and \\(j\\), such that \\(a_{ij}\\) is the movement from column-index \\(j\\) to row-index \\(i\\). The data that we draw from a life table and tabulated age-specific fertility data are typically indexed with an \\(x\\), such that \\(_nL_x\\) is the person-years lived between \\(x\\) and \\(x+n\\). Let’s just call it like it is and note that the conversion is bound to be awkward. If, as usual, the age at the start of the age class is \\(x\\) and the width of the age interval is \\(n\\), then \\(x = (j-1)\\, n\\). Survival from age-class one to age-class two is thus \\(P_1\\) as the index 1 corresponds to \\((2-1) \\cdot 5 = 5\\). For the fertility elements, \\(F_3\\) is the fertility of 10-14 year olds (hopefully zero).\n\n7.5.1 Subdiagonal\nThe subdiagonal elements of our square matrix are the survival probabilities of moving from the column index to the row index. However, the fact that our age classes are typically greater than one year means we need to think a bit harder about how we calculate these probabilities. Suppose we have five-year age classes (which is common). This means that there are actually five one-year cohorts who contribute to each age class in our projection model. Suppose we want the survival probability moving from the second to the third age class in a matrix with five-year age classes (i.e., \\(a_{32}\\), which moves individuals from age 5 to age 10). In terms of our life-table survivorship values, the size of the youngest cohort in the starting age class will be \\(l_5\\) and the size of the oldest cohort will be \\(l_10\\). The average of these two is \\((l_5 + l_10)/2\\), and the width of the interval is five years, so we have \\(5/2(l_5+l_10)\\). This is the approximation we use in constructing person-years lived for the age class 5-9, \\(_5L_5\\). The same logic applies to the receiving age class, so the transition probability is \\(P_2 = {_5}L_{10}/{_5}L_5\\) (note the subscript of 2 means this is the survival to the second age class in our model, which is divided into five-year intervals; \\(5 \\times 2 = 10\\), the receiving age).\nThe general rule then is simply that the subdiagonal elements of the Leslie matrix are:\n\\[\na_{i+1,i} = \\frac{_nL_{i+1}}{_nL_i},\n\\]\nthe ratio of the person-years lived in the receiving age class to the person-years lived in the sending age class.\n\n\n7.5.2 First Row\nAlas, the fertility values along the first row are even hairier. In a nutshell, fertility rates can change quite rapidly with respect to even modestly-wide age intervals. This means we need to do some averaging across successive age classes. Note that this averaging involves more than just fertility as well, since a certain fraction of women, who may have reproduced, will die before reaching the next age class. We also know that infancy is the most dangerous year in the human life cycle, so we need to account for the fact that a woman may have given birth multiple times, and lost some of those infants, in a single age class. Finally, the Leslie matrix is typically a one-sex model (two-sex models entail more complex stage-based structure), so we need to transform the observed age-specific fertility rates into female-only fertility rates.\nI will not belabor this, while I think about more elegant ways to explain all the steps to constructing the \\(F_i\\) entries. Here’s the general formula, which I’ve tried to write in as compact a form as possible.\n\\[\nF_i = \\frac{_nL_0}{2l_0}(m_i + P_i m_{i+1})\n\\]\nSome things to note: \\(m_i\\) is the maternity rate in age-class \\(i\\). It is the product of the age-specific fertility rate \\(_nF_x\\) and the fraction of births that are female (typically a bit under 50%). As noted above \\(P_i = _nL_{x+1}/{_n}L_x\\). Don’t forget to adjust your fertility rates to represent a single sex or your population growth rates will be way too high!\nRemember that \\(l_0\\) is the radix of the life table. This is the size of the synthetic cohort that defines our period life table. We often take \\(l_0 = 1\\), particularly in biological applications, but demographers conventionally use a radix of \\(l_0=100000\\). \\(_nL_0\\), of course, is the person-years lived in the first age-class.\n\n\n7.5.3 Fun facts about Eigenvalues\nThe Perron-Frobenius Theorem guarantees that one eigenvalue will be real, positive and absolutely greater than all others. This is called the dominant eigenvalue of the projection matrix. The dominant eigenvalue of the projection matrix is the asymptotic growth rate of the population described by that matrix. The dominant eigenvalue of the projection matrix is also the fitness measure of choice for age-structured populations. \\(\\log(\\lambda)/n = r\\). That is, the natural logarithm of the dominant eigenvalue, divided by the length of the projection interval (or age class), gives the annual rate of increase of the population. By calculating the eigenvalues of a projection matrix, you get lots of other important information.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  },
  {
    "objectID": "demog2.html#some-demographic-analysis",
    "href": "demog2.html#some-demographic-analysis",
    "title": "7  Demography II: Matrix Population Models",
    "section": "7.11 Some Demographic Analysis",
    "text": "7.11 Some Demographic Analysis\nThese are four populations that span what I’ve called the human demographic space (Jones 2009): Ache, and !Kung hunter-gatherers, Venezuela from 1965, and the USA from 2002. I show a plot of this space in chapter Chapter 2. The data are somewhat heterogeneous. Venezuela come from tabulated values of deaths, births, and exposed populations listed in the demographic compendium of Keyfitz & Flieger (1971). The USA mortality data come from HMD and the ASFR data come from the US Census Bureau web site (cited in Jones 2009). Both mortality and fertility data for the !Kung and Ache come from the monographs by Howell (1979) and Hill & Hurtado (1994) respectively.\nThe code that follows is necessarily a bit messy because the data are a little different for each population. What I do is get the data necessary to construct Leslie matrices with five-year age classes. I then use smoothing splines to interpolate one-year values. This meant I was able to plot smoother curves in the 2009 paper, among other things. Interpolating with splines is a good trick to have in your toolkit, regardless.\n\nrequire(demogR)\n## load the heterogeneous data\nsource(\"./data/demog_data_4pops.R\")\n# Ache\nache.lt &lt;- life.table(x=ache.age,nDx=ache.nDx,nKx=ache.nKx,type=\"cd\")\nache.lx &lt;- ache.lt[,\"lx\"]\n## fit the splines\nalxspl &lt;- smooth.spline(ache.lt$x,ache.lt$lx)\n## interpolate to one-year values\nalx1 &lt;- predict(alxspl,seq(0,50,by=1))$y\n## calculate px values from lx\napx &lt;- exp(diff(log(alx1)))\n## fit splines to fertility\namxspl &lt;- smooth.spline(seq(10,45,by=5), ache.mx/5)\n## interpolate to one-year values\nmx1 &lt;- predict(amxspl, seq(10,50,by=1))$y\n# replace any negative values with zero\nmx1[mx1&lt;0] &lt;- 0\n## construct 50 x 50 Leslie matrix\n## uses demogR functions leslie.row1 and odiag; also eigen.analysis\nr1 &lt;- leslie.row1(c(rep(0,9),mx1),px=apx,SRB=1)\nales &lt;- odiag(apx[1:46],-1)\nales[1,] &lt;- r1[1:47]\n# analysis\nea.ache &lt;- eigen.analysis(ales)\n# sensitivities\nasf &lt;- ea.ache$sensitivities[1,]\nass &lt;- ea.ache$sensitivities[row(ales) == col(ales)+1]\n# elasticities\naef &lt;- ea.ache$elasticities[1,]\naes &lt;- ea.ache$elasticities[row(ales) == col(ales)+1]\n\n## !Kung\nklxspl &lt;- smooth.spline(kung.age[1:12],kung.lx[1:12])\nklx1 &lt;- predict(klxspl,seq(0,50,by=1))$y\nkpx &lt;- exp(diff(log(klx1)))\n# fertility\nkmxspl &lt;- smooth.spline(seq(15,45,by=5), kung.mx/5)\nkmx1 &lt;- predict(kmxspl, seq(10,50,by=1))$y\nkmx1[kmx1&lt;0] &lt;- 0\n# Leslie matrix\nr1 &lt;- leslie.row1(c(rep(0,9),kmx1),px=kpx,SRB=1)\nkles &lt;- odiag(kpx[1:46],-1)\nkles[1,] &lt;- r1[1:47]\n# analysis\nea.kung &lt;- eigen.analysis(kles)\n# sensitivities\nksf &lt;- ea.kung$sensitivities[1,]\nkss &lt;- ea.kung$sensitivities[row(kles) == col(kles)+1]\n# elasticities\nkef &lt;- ea.kung$elasticities[1,]\nkes &lt;- ea.kung$elasticities[row(kles) == col(kles)+1]\n\n## USA and Venezuela have death, birth, exposure data, so create life tables\n\n## USA\nusa.lt &lt;- life.table(x=usa.age,nDx=usa.nDx,nKx=usa.nKx)\nusa.lx &lt;- usa.lt[,\"lx\"]\nusa.nMx &lt;- usa.lt[,\"nMx\"]\nusa.age &lt;- usa.lt[,\"x\"]\nulxspl &lt;- smooth.spline(usa.age,usa.lx)\nulx1 &lt;- predict(ulxspl,seq(0,50,by=1))$y\nupx &lt;- exp(diff(log(ulx1)))\n\nusa.mx &lt;- usa.mx[4:11]\numxspl &lt;- smooth.spline(seq(15,50,by=5), usa.mx/5)\numx1 &lt;- predict(umxspl, seq(10,50,by=1))$y\numx1[umx1&lt;0] &lt;- 0\nr1 &lt;- leslie.row1(c(rep(0,9),umx1),px=upx,SRB=1)\n\nules &lt;- odiag(upx,-1)\nules[1,] &lt;- r1\n### predicted fertilities for ages greater than 49 are NA\nules &lt;- ules[1:49,1:49]\n# analysis\nea.usa &lt;- eigen.analysis(ules)\n# sensitivities\nuss &lt;- ea.usa$sensitivities[row(ules) == col(ules)+1]\nusf &lt;- ea.usa$sensitivities[1,]\n# elasticities\nues &lt;- ea.usa$elasticities[row(ules) == col(ules)+1]\nuef &lt;- ea.usa$elasticities[1,]\n\n## Venezuela\nven.lt &lt;- life.table(x=ven.age, nDx=ven.nDx, nKx=ven.nKx)\nven.lx &lt;- ven.lt[,\"lx\"]\nven.age &lt;- ven.lt[,\"x\"]\nvlxspl &lt;- smooth.spline(ven.age,ven.lx)\nvlx1 &lt;- predict(vlxspl,seq(0,50,by=1))$y\nvpx &lt;- exp(diff(log(vlx1)))\n\nven.mx &lt;- ven.bx/ven.nKx\nven.mx &lt;- ven.mx[4:12]\nvmxspl &lt;- smooth.spline(seq(10,50,by=5), ven.mx/5)\nvmx1 &lt;- predict(vmxspl, seq(10,50,by=1))$y\nvmx1[vmx1&lt;0] &lt;- 0\nr1 &lt;- leslie.row1(c(rep(0,9),vmx1),px=vpx,SRB=1)\nvles &lt;- odiag(vpx,-1)\nvles[1,] &lt;- r1\nvles &lt;- vles[1:49,1:49]\n#analysis\nea.ven &lt;- eigen.analysis(vles)\n# sensitivities\nvsf &lt;- ea.ven$sensitivities[1,]\nvss &lt;- ea.ven$sensitivities[row(vles) == col(vles)+1]\n# elasticities\nvef &lt;- ea.ven$elasticities[1,]\nves &lt;- ea.ven$elasticities[row(vles) == col(vles)+1]\n\nNow we can construct some plots. First, let’s plot reproductive value and age structure.\n\n## plot reproductive value\nplot(ea.ache$repro.value, type=\"l\", lwd=3, col=\"blue4\", xaxs=\"i\", xlab=\"Age\", ylab=\"Reproductive Value\")\nlines(ea.usa$repro.value, lwd=3, col=\"red4\")\nlines(ea.kung$repro.value, lwd=3, col=\"green4\")\nlines(ea.ven$repro.value, lwd=3, col=\"orange\")\nlegend(\"topright\",c(\"Ache\",\"Venezuela\",\"!Kung\",\"USA\"), lwd=3, col=c(\"blue4\",\"orange\",\"green4\",\"red4\"))\n\n\n\n\n\n\n\n## plot age structure\npar(mfrow=c(2,2))\nbarplot(ea.ache$stable.age, horiz=TRUE, plot=TRUE, col=\"blue4\", border=NA,\n        xlab=\"Fraction\", ylab=\"Age\", axis.lty=1, xlim=c(0, 0.06))\naxis(2,at=seq(0,60,by=5), tick=FALSE, las=1, cex.axis=0.75)\ntitle(\"Ache\")\n\nbarplot(ea.kung$stable.age, horiz=TRUE, plot=TRUE, col=\"green4\", border=NA,\n        xlab=\"Fraction\", ylab=\"Age\", axis.lty=1, xlim=c(0, 0.06))\naxis(2,at=seq(0,60,by=5), tick=FALSE, las=1, cex.axis=0.75)\ntitle(\"!Kung\")\n\nbarplot(ea.ven$stable.age, horiz=TRUE, plot=TRUE, col=\"orange\", border=NA,\n        xlab=\"Fraction\", ylab=\"Age\", axis.lty=1, xlim=c(0, 0.06))\naxis(2,at=seq(0,60,by=5), tick=FALSE, las=1, cex.axis=0.75)\ntitle(\"Venezuela\")\n\nbarplot(ea.usa$stable.age, horiz=TRUE, plot=TRUE, col=\"red4\", border=NA,\n        xlab=\"Fraction\", ylab=\"Age\", axis.lty=1, xlim=c(0, 0.06))\naxis(2,at=seq(0,60,by=5), tick=FALSE, las=1, cex.axis=0.75)\ntitle(\"USA\")\n\n\n\n\n\n\n\n\nNow plot the sensitivities and the elasticities. I commented out the legends because they get too busy with multiple plots in a single figure. It’s good to be able to attach a legend though if you ever need the figure without all the previous context.\n\n# sensitivities\npar(mfrow=c(1,2))\nplot(0:45, ass, type=\"l\", \n     lwd=3, col=\"blue4\",\n     xlab=\"Age\", ylab=\"Survival Sensitivity\",\n     xlim=c(0,48), ylim=c(0,0.046))\nlines(0:45, kss, lwd=3, col=\"green4\")\nlines(0:47, vss, lwd=3, col=\"orange\")\nlines(0:47,uss,lwd=3,col=\"red4\")\n#legend(\"topright\",c(\"Ache\",\"!Kung\",\"Venezuela\",\"USA\"), lwd=3, col=c(\"blue4\",\"green4\",\"orange\",\"red4\"))\n## fertility\nplot(0:46, asf, type=\"l\", \n     lwd=3, col=\"blue4\",\n     xlab=\"Age\", ylab=\"Survival Sensitivity\",\n     xlim=c(0,48), ylim=c(0,0.046))\nlines(0:46, ksf, lwd=3, col=\"green4\")\nlines(0:48, vsf, lwd=3, col=\"orange\")\nlines(0:48,usf,lwd=3,col=\"red4\")\n\n\n\n\n\n\n\n#legend(\"topright\",c(\"Ache\",\"!Kung\",\"Venezuela\",\"USA\"), lwd=3, col=c(\"blue4\",\"green4\",\"orange\",\"red4\"))\n\n# elasticities\npar(mfrow=c(1,2))\nplot(0:45, aes, type=\"l\", \n     lwd=3, col=\"blue4\",\n     xlab=\"Age\", ylab=\"Survival Elasticity\",\n     xlim=c(0,48), ylim=c(0,0.044))\nlines(0:45, kes, lwd=3, col=\"green4\")\nlines(0:47, ves, lwd=3, col=\"orange\")\nlines(0:47,ues,lwd=3,col=\"red4\")\n#legend(\"topright\",c(\"Ache\",\"!Kung\",\"Venezuela\",\"USA\"), lwd=3, col=c(\"blue4\",\"green4\",\"orange\",\"red4\"))\n## fertility\nplot(0:46, aef, type=\"l\", \n     lwd=3, col=\"blue4\",\n     xlab=\"Age\", ylab=\"Survival Elasticity\",\n     xlim=c(0,48), ylim=c(0,0.044))\nlines(0:46, kef, lwd=3, col=\"green4\")\nlines(0:48, vef, lwd=3, col=\"orange\")\nlines(0:48,uef,lwd=3,col=\"red4\")\n\n\n\n\n\n\n\n#legend(\"topright\",c(\"Ache\",\"!Kung\",\"Venezuela\",\"USA\"), lwd=3, col=c(\"blue4\",\"green4\",\"orange\",\"red4\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Demography II: Matrix Population Models</span>"
    ]
  }
]