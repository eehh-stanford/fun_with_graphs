[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fun with Graphs",
    "section": "",
    "text": "Fun with Graphs\nIn the Fall of 1992, I was a first-year Ph.D. student in biological anthropology at Harvard. Mark Leighton was looking for someone to serve as a TF for his class, Primate Evolutionary Ecology. This was an amazing class that brought the theoretical evolutionary ecology of MacArthur and Levins and the behavioral ecology of Krebs, Davies, and Charnov to bear on problems of primate ecology. I didn’t need to teach, but I signed up anyway, hoping to really learn the material to which I had only a superficial introduction during undergraduate tutorials. This turned out to be a pretty fateful decision.\nOn every exam that Mark administered, there was a section cheekily titled Fun with Graphs, where students had to display their graphical reasoning chops. In general, I don’t think the students had all that much fun with this section. When Bill Durham and I first taught our class, Environmental Change and Emerging Infectious Disease, I brought back this tradition and it has been a staple of all my in-person exams for classes I’ve taught at Stanford ever since.\nI worry that the sort of intuitive graphical reasoning that motivated so much of this incredible theory is not developed in students who must expend all their effort studying for tests which will allow them admission to increasingly competitive universities. I also worry that theoretical ecologists (along with formal demographers and mathematical epidemiologists) have not reproduced themselves culturally and this important material is increasingly not taught.\nYou could think of this book as essentially Nonstandard Uses of R. R graphics are clearly designed for plotting data. However, R is a highly versatile and powerful tool for making theoretical and expository figures in science. That’s the vibe. Maybe there will be more…",
    "crumbs": [
      "Fun with Graphs"
    ]
  },
  {
    "objectID": "interpreting.html#introduction",
    "href": "interpreting.html#introduction",
    "title": "1  Interpreting Scientific Figures",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nUnderstanding scientific figures is an important part of becoming a scientist or a critical consumer of scientific information. This is a skill that, alas, is generally not taught in most schools. Here, I will try to provide a gentle introduction to reading scientific figures, especially theoretical plots. I have companion notes that describe how to generate scientific plots in R.\nWe use theory in science to bring order to the complexity we observe in the world. Theory generates our hypotheses but it also guides us in what we observe, how we measure it, and what we should find surprising. Surprise is essential for the scientific enterprise because it is the surprise that comes when we observe something novel from a process we thought we understood that generates innovation and explanation.\nA couple starting points. We will use some very basic calculus here. Derivatives, second derivatives, and Taylor series."
  },
  {
    "objectID": "interpreting.html#lines",
    "href": "interpreting.html#lines",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.2 Lines",
    "text": "2.2 Lines\nPresumably, we all remember the formula for a straight line from high school algebra:\n\\[\ny = mx + b,\n\\]\nwhere \\(m\\) is the slope and \\(b\\) is the \\(y\\)-intercept.\n\nm &lt;- 2\nb &lt;- 1\ncurve(m*x+b, 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\n\nClearly, this is a straight line. What this means is that for whatever \\(x\\)-value you increment, you will increase by a factor of two.\n\n# define a linear function\nlin &lt;- function(x,m=2,b=1) m*x + b\n# draw curve, add increments\ncurve(lin(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,lin(1),3,lin(1), col=\"red\", lty=3)\nsegments(3,lin(1),3,lin(3), col=\"red\", lty=3)\nsegments(4,lin(4),6,lin(4), col=\"red\", lty=3)\nsegments(6,lin(4),6,lin(6), col=\"red\", lty=3)\nsegments(7,lin(7),9,lin(7), col=\"red\", lty=3)\nsegments(9,lin(7),9,lin(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nOf course, we can show this analytically by calculating the derivative. Let \\(f(x) = mx + b\\), then \\(f'(x) = m\\). Not surprising since \\(m\\) is literally the slope that the rate of change in \\(f(x)\\) is always \\(m\\).\nLinear change is a touchstone. We are often interested if something is changing faster or more slowly than linear.\nWe often use linear functions to approximate more complex functions in some restricted range. For example, in the model of optimal virulence, discussed below, we need to draw a tangent line to the function relating transmissibility to disease-induced mortality. This tangent line is a linear approximation of that function in the vicinity of the optimal virulence.\nNote that this is what a derivative is. It’s linear representation of the slope of a function over an infinitesimal change of the input variable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#curves",
    "href": "interpreting.html#curves",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.3 Curves",
    "text": "2.3 Curves\n\n2.3.1 Polynomial Curves\nWhen something is nonlinear, it changes at different rates in different parts of the curve. The simplest extension from a straight line is a polynomial, e.g., a quadratic function.\n\n# quadratic function\nquad &lt;- function(x,m=2,b=1) m*x^2+b\ncurve(quad(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,quad(1),3,quad(1), col=\"red\", lty=3)\nsegments(3,quad(1),3,quad(3), col=\"red\", lty=3)\nsegments(4,quad(4),6,quad(4), col=\"red\", lty=3)\nsegments(6,quad(4),6,quad(6), col=\"red\", lty=3)\nsegments(7,quad(7),9,quad(7), col=\"red\", lty=3)\nsegments(9,quad(7),9,quad(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nThe quadratic curve changes at an increasing rate. For \\(f(x) = mx^2 + b\\), \\(f'(x)=2x\\).\n\n\n2.3.2 Exponential and Logarithmic Curves\nWhen people say that something is growing “exponentially,” what they usually mean is that it’s growing fast. Exponential growth is much more specific than that (and there are, indeed, ways to grow much faster than exponentially!). In continuous time, something grows exponentially if it increases at a constant rate regardless of its size.\nExponential growth has the wild property that the derivative of an exponential is proportional to the exponential itself. For example, if \\(f(x)=e^r\\), then \\(f'(x) = e^r\\). If \\(f(x)=e^{2r}\\), then \\(f'(x) = 2e^{2r}\\), and so on.\n\ncurve(exp(x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,exp(1),3,exp(1), col=\"red\", lty=3)\nsegments(3,exp(1),3,exp(3), col=\"red\", lty=3)\nsegments(4,exp(4),6,exp(4), col=\"red\", lty=3)\nsegments(6,exp(4),6,exp(6), col=\"red\", lty=3)\nsegments(7,exp(7),9,exp(7), col=\"red\", lty=3)\nsegments(9,exp(7),9,exp(9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nLooking at the increments, we quickly discern another important feature of exponential growth: it sneaks up on you! In the early phase of an exponential-growth process, it can be quite difficult to tell it apart from linear growth or even no growth. The red dotted lines showing the growth between \\(x=1\\) and \\(x=3\\) are barely visible.\nBecause of the explosiveness of exponential growth, the initial conditions can matter a lot for outcomes. Compare the following two curves:\n\ncurve(5*exp(x), 0, 10, lwd=3, col=\"red\", xlab=\"x\", ylab=\"y\")\ncurve(1*exp(x), 0, 10, lwd=3, col=\"black\", add=TRUE)\n\n\n\n\n\n\n\n\nWe predict that virulence of a virus, for example, will increase with the size of the infectious innoculum. The intuition behind this prediction is that a larger innoculum provides a larger initial population size that can quickly increase to overwhelm a host’s immunological defenses. The smaller size of the viral population for any given time after infection arising from the smaller innoculum provides a greater likelihood that the host will control the infection quickly and with less tissue damage, etc.\nIt’s also super-important to note that things can also decrease exponentially! Exponential decay is a thing.\n\ncurve(50*exp(-x), 0, 10, lwd=3, xlab=\"x\", ylab=\"y\")\nsegments(1,50*exp(-1),3,50*exp(-1), col=\"red\", lty=3)\nsegments(3,50*exp(-1),3,50*exp(-3), col=\"red\", lty=3)\nsegments(4,50*exp(-4),6,50*exp(-4), col=\"red\", lty=3)\nsegments(6,50*exp(-4),6,50*exp(-6), col=\"red\", lty=3)\nsegments(7,50*exp(-7),9,50*exp(-7), col=\"red\", lty=3)\nsegments(9,50*exp(-7),9,50*exp(-9), col=\"red\", lty=3)\n\n\n\n\n\n\n\n\nCompare them.\n\n# draw exp first to make sure axes fit\nrequire(viridisLite)\nc &lt;- plasma(3)\ncurve(quad(x), 0, 10, lwd=3, col=c[1],\n      xlab=\"x\", ylab=\"y\",\n      xaxs=\"i\", yaxs=\"i\")\ncurve(lin(x), 0, 10, lwd=3, col=\"black\", add=TRUE)\ncurve(exp(x), 0, 10, lwd=3, col=c[2], add=TRUE)\ncurve(log(x), 0.01, 10, lwd=3, col=c[3], add=TRUE)\nlegend(\"topleft\", c(\"linear\",\"quadratic\",\"exponential\",\"logarithmic\"),\n       col=c(\"black\",c), lty=1, lwd=3)\n\n\n\n\n\n\n\n\n\n\n2.3.3 Power Laws\nIt turns out that much of the world – particularly in biology – scales according to a power law. Nearly everything you can imagine measuring about an organism scales with an organism’s body mass and it does so according to a power law. So for some outcome \\(Y\\) (e.g., lifespan, annual fertility, brain mass, metabolic rate, etc.), where we let \\(W\\) indicate body mass, the scaling relationship takes the form\n\\[\nY = A W^a.\n\\]\nIf \\(a&gt;1\\), this curve will be convex (i.e., increasing returns to size), while if \\(0&lt;a&lt;1\\), the curve will be concave. If \\(a=1\\), then we simply have a straight line with slope \\(A\\) and intercept zero. In comparative biology, the case where \\(a=1\\) is known as “isometry” and the case where \\(a \\neq 1\\) is known as “allometry”.\nIf we take logarithms of both sides of the power-law relation, we get a linearized form,\n\\[\n\\log(Y) = \\log(A) + a \\log(W).\n\\]\nPlotting data on double-logarithmic axes can help in diagnosing a power law.\nWhen \\(a&lt;0\\), we have the case of power-law decay. This provides a very interesting case where the decay of some function can be considerably slower than exponential. For example, most of the common probability distributions that we use (e.g., normal, exponential, Poisson, binomial) have “exponential” tails. This means that the probability associated with a particular value decays exponentially as the values move away from the region of highest probability. In contrast, power-law probability distributions can have fat tails, meaning that extreme values are more likely than they would be under a comparable probability distribution with exponential decay.\nThe key difference between a power law and an exponential, which at first glance appear to be quite similar, is that for the power law, the power is constant (\\(x^a\\)) whereas for an exponential, the power is the variable \\(a^x\\) (where we usually use the specific value of \\(a=e\\), where \\(e\\) is the base of the natural logarithm). Note that we’ve already looked at a comparison between exponential growth and power-law growth, when we compared the quadratic (\\(a=2\\)) to the exponential. Let’s look at power-law decay now.\n\ncurve(0.5^x,0, 10, lwd=3, xaxs=\"i\", xlab=\"x\", ylab=\"y\")\ncurve(x^-2, 0, 10, lwd=3, col=\"red\", add=TRUE)\nlegend(\"topright\",c(\"exponential\",\"power\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe power decay starts much higher (it is, in fact, asymptotic to the \\(y\\)-axis) and declines very rapidly at first. However, while the exponential curve will quickly approach the \\(x\\)-axis (to which it is asymptotic), the polynomial power-law curve will approach it very slowly. For the exponential curve, every \\(x\\)-increment of one reduces the value of \\(y\\) by a half. In contrast, for the power-law, every increment contributes a tiny marginal decay as the values of \\(x\\) increase. For the exponential the ratio of subsequent \\(y\\) values is \\(0.5^{x+1}/0.5^{x} = 0.5^1=0.5\\) for all values of \\(x\\). The analogous ratio for the power law changes for different values of \\(x\\). When \\(x\\) is small, the ratio of successive \\(y\\) values is similar to the exponential. For example, when \\(x=2\\), \\((x+1)^{-2}/x^{-2}=0.44\\). However, when \\(x=100\\), the ratio is nearly one (=0.98), meaning that the curve is decreasing very slowly. Successive values of the function are nearly identical, making the ratio close to one.\n\n\n2.3.4 Why We Care About Power Laws\nThe figure above shows how the probability of very high values decays to zero for the expontial function, but decays slowly for the polynomial power law. This turns out to be very important for thinking about the probability of extreme events.\nWe say that a probability distribution has a heavy tail if, for large values of \\(x\\), the log of the probability of \\(x\\) is sublinear (Nair, Wierman, and Zwart 2022). Remember that when we take the logarithm of an exponential, we linearize it, so this is equivalent to saying that a heavy-tailed distribution decays more slowly than exponential. This is exactly what power-law distributions do. Their tails decay slowly – much slower than an exponential.\nWhat this means is that if a random variable is characterized by a power-law distribution, extreme values, while not likely, are also not impossible. Heavy tails make the impossible only unlikely.\nConsider the case of a normal (Gaussian) distribution, which has exponential tails and the related \\(t\\)-distribution, which has heavy tails. The probability density function (pdf) for the normal distribution is:\n\\[\nf(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}},\n\\] where the parameters \\(\\mu\\) and \\(\\sigma^2\\) represent the mean and variance of the distribution.\nThe pdf for the \\(t\\) distribution is:\n\\[\nf(x) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\pi \\nu}\\; \\Gamma\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n\\] where the parameter \\(\\nu\\) represents the degrees of freedom of the distribution. Don’t worry too much about that first fraction in the pdf. That is just a normalizing factor that ensures that the probability integrates to one (as all pdfs must).\nNote that, as with our last section, the pdf for the normal distribution (with exponential tail) has \\(x\\) in the exponent, while for the \\(t\\)-distribution (with polynomial tail), \\(x\\) is being raised to a constant power related to the degrees of freedom.\nWe can compare the two probability density functions:\n\nx &lt;- seq(-10,10,length=1000)\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", ylim=c(0,0.42), xlim=c(-10,10),\n     xlab=\"Outcome (x)\", ylab=\"Probability of Outcome (P(x))\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")\nlegend(\"topright\",c(\"normal\",\"t\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe normal distribution is higher (i.e., has more mass) in the middle of the plot. The \\(t\\) distribution has less of its mass in the central part of the distribution, and spreads out more of its total mass to its tails.\nI should probably note that this is a particularly extreme \\(t\\) distribution, as it only has one degree of freedom (this distribution is also known as a Cauchy distribution). This makes it particularly easy to see the differences between the normal and the \\(t\\) distribution.\nWe can focus on just the right tail. Normal theory tells us that the probability of observing any values greater than four standard deviations aboce the mean is essentially zero. We can calculate the remaining area in the tail above four standard deviations (for the standard normal distribution) by subtracting the cumulative distribution function at \\(x=4\\) from one.\n\n1-pnorm(4)\n\n[1] 3.167124e-05\n\n\nPretty nearly zero.\nWhat about the same calculation for our \\(t\\) distribution with one degree of freedom?\n\n1-pt(4,1)\n\n[1] 0.07797913\n\n\nNearly eight percent of the total probability remains in the tail! That is, you have a 7.8% chance of observing a value of \\(x \\geq 4\\).\nLet’s compare a plot of the tails of the two distributions:\n\nplot(x,dnorm(x), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", ylim=c(0,0.4), xlim=c(0,10),\n     xlab=\"Outcome (x)\", ylab=\"Probability Density of Outcome (f(x))\")\nlines(x,dt(x,df=1), lwd=2, col=\"red\")\nabline(h=0.01,lty=3)\naxis(1)\naxis(2)\nlegend(\"topright\",c(\"normal\",\"t\"), lwd=3, col=c(\"black\",\"red\"))\n\n\n\n\n\n\n\n\nThe horizontal dotted line indicates the value where the probability density is 1%. We can see that the \\(t\\) distribution crosses this thresholds at a value of approximately \\(x=6\\). The term six-sigma event is often used to indicate an outcome that is essentially impossible under our current understanding of a system. This comes from the fact that for a standard normal distribution, the variable \\(x\\) is equivalent to the number of standard deviations from the mean of zero (a standard normal has \\(\\mu=0\\) and \\(\\sigma^2=\\sigma=1\\)). Thus, for a standard normal distribution, an observed value of of \\(x=6\\) is six standard deviations (or \\(6\\sigma\\)) greater than the mean and, based on what we know about the normal distribution, has a probability of essentially zero. Not so for the \\(t\\) distribution! We can see that even extremely high values of \\(x\\) have associated tail probabilities that, while low, are still a long way from zero.\n\n1-pt(6,1)\n\n[1] 0.05256846\n\n1-pt(8,1)\n\n[1] 0.03958342\n\n1-pt(10,1)\n\n[1] 0.03172552\n\n\nThe key question, of course, is whether a \\(t\\) distribution (or other heavy-tailed distribution) represents any phenomena we actually care about scientifically. Turns out, there are lots of things in Nature that follow power laws. One of the more interesting examples where the outcome is actually a \\(t\\) distribution was described by Weitzman (2009). When you have to learn about the mean value of some normally-distributed variable, but have limited opportunities to learn, the distribution of that mean is itself a \\(t\\) distribution. This has huge implications for decision-making that I will discuss elsewhere.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#convexity-and-concavity",
    "href": "interpreting.html#convexity-and-concavity",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.4 Convexity and Concavity",
    "text": "2.4 Convexity and Concavity\nThe derivative of a function provides a measure of how fast a function is changing. The second derivative measures how that rate of change itself is changing. In this sense, it measures the curvature of a function.\nMany theoretical models depend on the curvature of functions to make their predictions. A common assumption employed in many theoretical models is that of concavity. A very common use of concavity in theory is when curve shows diminishing marginal returns. The word “marginal” essentially means the derivative, so diminishing marginal returns means that the derivative of the function is getting smaller for larger values of the input.\nThe classic trade-off model for the evolution of virulence relies on the concavity of transmissibility with respect to disease-induced mortality. If virulence produces decreasing marginal transmissibility with respect to disease-induced mortality, then selection will favor intermediate virulence. Denote virulence by \\(x\\). Both transmission and disease-induced mortality are functions of virulence: \\(\\beta(x)\\) and \\(\\delta(x)\\). The fitness measure for the pathogen is, as usual, \\(R_0\\), which we can write as\n\\[\nR_0 = \\frac{\\beta(x)}{\\mu + \\delta(x)},\n\\]\nwhere \\(\\mu\\) is the disease-independent mortality.\nTo find the optimal value of virulence, differentiate with respect to \\(x\\) and solve for \\(dR_0/dx=0\\). Employing the quotient rule for differentiation and doing a little algebra to tidy up, we get:\n\\[\n\\frac{d \\beta(x)}{d \\delta(x)} = \\frac{\\beta(x^*)}{\\mu + \\delta(x^*)},\n\\]\nwhere \\(x^*\\) indicates the optimal value of virulence.\nThe geometric interpretation of this result is that optimal virulence satisfies the condition that a line, rooted at the origin, is tangent to the curve relating transmissibility to mortality. This result is known as the Marginal Value Theorem in behavioral ecology and, in addition to describing a model for optimal virulence, also predicts the optimal length of time for a foraging bout in a feeding patch or the optimal copula duration when a male has multiple mating opportunities but his sperm can be displaced by subsequent matings.\n\nx &lt;- seq(0,30,length=500)\n# transmissibility function fp&gt; 0 fpp &lt; 0\nf &lt;- function(x) {\n  0.5 - exp(-0.2*(x-7))\n}\n# derivative of the utility function\nfprime &lt;- function(x) {\n  0.2*exp(-0.2*(x-7))\n}\n\n# 1st-degree Taylor series around x: f + fp*(z-x) = 0\n# z = x -(f/fp)\n# solve for tangency; find the root of this\nxinter &lt;- function(x) {\n  return(x - f(x)/fprime(x))\n}\n\nsoln &lt;- uniroot(xinter,c(0,40))\nplot(x,f(x), type=\"l\", lwd=2, xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE,\n     xlab=\"Mortality\",\n     ylab=\"Transmissibility\",\n     ylim=c(0,0.7))\naxis(1,labels=FALSE,tick=FALSE)\naxis(2,labels=FALSE,tick=FALSE)\nbox()\nlines(x,(f(soln$root)/soln$root)*x,col=grey(0.75))\nsegments(soln$root,0,soln$root,f(soln$root), lty=2, col=\"red\")\nsegments(0,f(soln$root),soln$root,f(soln$root), lty=2, col=\"red\")\nmtext(expression(paste(delta,\"*\")),1,at=soln$root, padj=1)\nmtext(expression(paste(beta,\"*\")),2,at=f(soln$root),padj=0.5, adj=1.5, las=2)\nmtext(expression(mu),1,at=5, padj=1)\n\n\n\n\n\n\n\n\nWhat would happen if the function was convex (\\(f''(x)&gt;0\\)), rather than concave? There can be no intermediate optimum for a such a convex function. The optimal virulence is maximum.\nIn one of the most important papers in the field of life history theory, Gadgil and Bossert (1970) noted that the only conditions under which natural selection will favor intermediate reproductive effort are when the fitness gains to effort are concave and, importantly, that the costs of effort are either linear or convex. We can easily visualize why this is the case.\n\nx &lt;- seq(1,11,,110)\ny &lt;- 4*log(x)\ny1 &lt;- 0.1*exp(x/2)\ny2 &lt;- 0.1*exp(x/1.5)\n# maxima\nd1 &lt;- y-y1\nd2 &lt;- y-y2\nmax1 &lt;- x[which(d1==max(d1))]\nmax2 &lt;- x[which(d2==max(d2))]\n\n### concave benefits/concave costs\nplot((x-1)/10,y/11, type=\"l\", lwd=3, \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\",  \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10, y/11 + 0.01*x, lwd=3, col=\"red\")\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\nabline(v=0, col=grey(0.65))\ntitle(\"No Reproduction\")\n\n\n\n\n\n\n\n### concave benefits/convex costs\nplot((x-1)/10,y/11, type=\"l\", lwd=3, \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\", \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10,y1/11, lwd=3, col=\"red\")\nabline(v=max1/11, col=grey(0.65))\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\ntitle(\"Intermediate Reproduction\")\n\n\n\n\n\n\n\n### concave benefits/concave costs, full RE\nplot((x-1)/10,y/11, type=\"l\", lwd=3, col=\"red\", \n     xlab=\"Reproductive Effort\", \n     ylab=\"Cost or Benefit\", \n     xlim=c(0,1), ylim=c(0,1))\nlines((x-1)/10, y/11 + 0.01*x, lwd=3, col=\"black\")\n#legend(0.05,1, c(\"Benefit\",\"Cost\"), lwd=3, lty=1, col=c(\"black\",\"red\"))\nabline(v=1, col=grey(0.65))\ntitle(\"Maximal (Suicidal) Reproduction\")\n\n\n\n\n\n\n\n\nOnly for the concave benefit/linear cost case does the maximum difference between the curves lie in the middle of the plot.\n\n2.4.1 Concavity Introduces Asymmetries\nSuppose you have a curve representing the fitness, \\(w\\), corresponding to a given level of effort, \\(x\\), similar to the Gadgil-Bossert curves discussed above. Further suppose that this curve is concave, showing diminishing marginal returns so that \\(w'(x)&gt;0\\) and \\(w''&lt;0\\).\nStarting at some point on this curve, say at the mean effort \\(\\bar{x}\\), imagine you flip a coin and get decremented a unit’s worth of fitness if it comes up heads and increase a unit’s worth if it comes up tails. This is known as a lottery, a decision in which there is a discrete, variable payoff. We can plot this as follows:\n\n## risk-aversion\nx &lt;- seq(0,5,length=1000)\nr &lt;- 0.75\nfx &lt;- 1-exp(-r*x)\n## for part deux\naaa &lt;- (fx-0.4882412)^2\nwhich(aaa==min(aaa))\n\n[1] 179\n\n#[1] 179\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"Fitness (w(x))\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(-0.1,5.1), ylim=c(0,1))\n#segments(0,0,5,fx[1000], lwd=2, col=grey(0.75))\naxis(1, at=c(0,2.5,5), labels=c(expression(x[0]), expression(bar(x)),\n                                expression(x[1])), tick=FALSE)\nsegments(2.5,0,2.5,0.846645, lwd=3, lty=1, col=grey(0.65))\nsegments(2.5,0.846645,0,0.846645, lwd=3, lty=1, col=\"red\")\narrows(0,0.846645,0,0.01, lwd=3, lty=1, col=\"red\", length=.25,angle=10)\nsegments(2.5,0.846645,5,0.846645, lwd=3, lty=1, col=\"red\")\narrows(5,0.846645,5,fx[1000], lwd=3, lty=1, col=\"red\", length=.25,angle=10)\nlines(x,fx, lwd=3, col=\"black\")\n\n\n\n\n\n\n\n\nWhere do these seemingly very specific numbers that I use to draw the segments and arrows come from? In particular, the value of 0.846645 is simply the value of the utility function at \\(\\bar{x} = 2.5\\): \\(1-\\exp(-0.75*0.25) = 0.846645\\).\nThe upside of this lottery increases fitness considerably less than the downside reduces it. This arises because of the curvature of the function, in particular, its diminishing marginal fitness returns to effort. This is a very important insight and defines the phenomenon of risk aversion. Risk-aversion in lotteries where the fitness function is a concave function of effort are an application of Jensen’s Inequality, which states that for a concave function, \\(w(x)\\),\n\\[\nw(E(x)) \\geq E(w(x)),\n\\]\nwhere \\(E()\\) indicates mathematical expectation.\nWe can show this graphically. We will draw a chord connecting the upside- and downside-payoffs, the midpoint of which is \\(E(w(x))\\). Note that this is considerably less than \\(w(\\bar{x})\\).\n\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(0,5.1), ylim=c(0,1))\nsegments(0,0,5,fx[1000], lwd=3, col=grey(0.75))\naxis(1, at=c(0.05,x[179],2.5,5), \n     labels=c(expression(x[0]), expression(x[C]),\n               expression(bar(x)), expression(x[1])),\n     tick=FALSE)\nmtext(\"Fitness (w(x))\", side=2,line=2, adj=0.65)\naxis(2, at=0.4882412, labels=\"\", tick=FALSE)\nsegments(2.5,0,2.5,0.4882412,lwd=3, lty=1, col=\"red\") # vertical line at bar(x)\nsegments(2.5,0.4882412,2.5,fx[501], lwd=3, lty=2, col=\"red\")\nlines(x,fx, lwd=3, col=\"black\")\n\n\n\n\n\n\n\n\nA risk-averse decision-maker should be willing to pay for certainty. We can show why this is graphically. Note that the expected fitness of this lottery (i.e., the average of the two possible outcomes) does not, in fact, fall on the fitness curve. We can move horizontally from this point back to the curve and the fitness would not change. If (and this is a big if) we can achieve certainty in our payoff by paying the difference between \\(E(w(x))\\) and what is called the certainty-equivalent return, we should.\n\nplot(x,fx, type=\"n\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=\"Parity (x)\", ylab=\"\", \n     xaxs=\"i\", yaxs=\"i\", xlim=c(0,5.1), ylim=c(0,1))\nsegments(0,0,5,fx[1000], lwd=3, col=grey(0.75))\naxis(1, at=c(0.05,x[179],2.5,5), \n     labels=c(expression(x[0]), expression(x[C]),\n              expression(bar(x)), expression(x[1])),\n     tick=FALSE)\nmtext(\"Fitness (w(x))\", side=2,line=2, adj=0.65)\naxis(2, at=0.4882412, labels=\"\", tick=FALSE)\nsegments(2.5,0,2.5,0.4882412,lwd=3, lty=1, col=\"red\") # vertical line at bar(x)\nsegments(2.5,0.4882412,x[179],0.4882412,lwd=3, lty=1, col=\"red\") # horizontal line back to utility curve\nsegments(x[179],0.4882412,x[179],0, lwd=3, lty=1, col=\"green\") # vertical line to x_c\nlines(x,fx, lwd=3, col=\"black\")\ntext(0.35, 0.54, expression(pi==bar(x) - x[C]))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#equilibria",
    "href": "interpreting.html#equilibria",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.5 Equilibria",
    "text": "2.5 Equilibria\nIn ecology, evolution, etc., we frequently plot two (or more) sets of rates. For example: birth and death rates in a demographic model or rates of colonization and extinction in a metapopulation model.\nFor example, the classic Levins model for metapopulations\n\\[\n\\dot{n} = cn(1-n) - en,\n\\]\nwhere \\(n\\) is patch occupancy, \\(c\\) is the colonization rate, and \\(e\\) is the extinction rate. The equilibrium for this happens when \\(\\dot{n}=0\\), which is\n\\[\n\\hat{n} = 1 - \\frac{e}{c}.\n\\]\nIf the extinction rate is greater than the colonization rate (\\(e&gt;c\\) ), then, sensibly, the overall population is extinct. Moreover, there will generally always be unoccupied patches at equilibrium.\nA classic example of a graphical representation of such an equilibrium process is the MacArthur-Wilson model, which is similar to the Levins metapopulation model in that it posits the number of species on an island is a dynamic balance between the colonization rate (which declines as a function of the number of resident species) and the extinction rate (which increases as a function of the number of resident species). The equilibrium occurs where the colonization rate just balances out the extinction rate, so that the overall rate of change of species is zero, the definition of an equilibrium.\n\nn &lt;- seq(0,20,,500)\nrate &lt;- 0.2\ncinit &lt;- 55\nplot(n, cinit*exp(-rate*n), type=\"l\", \n     lwd=3, col=\"#0D0887FF\",\n     xlab=\"Number of Species\", \n     ylab=\"Rate\",\n     ylim=c(0,60),\n     xlim=c(-3,23),\n     yaxs=\"i\",\n     axes=FALSE)\nlines(n, exp(rate*n), lwd=3, col=\"#9C179EFF\")\nsegments(log(cinit)/(2*rate),0,log(cinit)/(2*rate),exp(rate*log(cinit)/(2*rate)), lty=2)\naxis(1, at=c(log(cinit)/(2*rate)), labels = c(expression(hat(N))))\nbox()\n\n\n\n\n\n\n\n\n\n2.5.1 Equilibria in Discrete-Time\nRecursions.\nPoverty-trap model. We plot the wealth at time \\(t+1\\) agains the wealth at time \\(t\\). Use a Prelec weighting function to produce the characteristic S-shape of the poverty-trap model. An equilibrium occurs when the the wealth in the next time step is equal to the wealth in the current time step (i.e., there is no change). In this plot, this occurs wherever our curve touches the line of equality, \\(w_{t+1}=w_t\\).\nThe downside of the Prelec function is that we can’t easily solve for an equilibrium analytically, but we can solve it numerically using uniroot().\n\nprelec &lt;- function(p,a,b) (exp(-(-log(p))^a))^b\n## function to solve for interior equilibrium\nfn &lt;- function(p,a,b) (exp(-(-log(p))^a))^b - p\na &lt;- 2\nb &lt;- 1.7\n# we know p=0 and p=1 are solutions so limit to searching an interior interval\npint &lt;- uniroot(fn,interval=c(0.1,0.9),a=a,b=b)$root\np &lt;- seq(0,1,,1000)\nplot(p, prelec(p=p,a=a,b=b), type=\"l\", col=\"blue4\", lwd=2, \n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\", \n     xlab=expression(W[t]), ylab=expression(W[t+1]),\n     xlim=c(-0.05,1.05), ylim=c(-0.05,1.05))\nabline(a=0,b=1,lwd=1, col=grey(0.75))\npoints(c(0,pint,1),c(0,prelec(p=pint,a=a,b=b),1), pch=c(19,1,19), cex=1.5)\n\n\n\n\n\n\n\n\nThere are three equilibria for the poverty-trap model: (1) a stable equilibrium at destitution (\\(w_t=0\\)), (2) an unstable interior equilibrium, and (3) a stable equilibrium at maximum wealth.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#indifference-curves",
    "href": "interpreting.html#indifference-curves",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.6 Indifference Curves",
    "text": "2.6 Indifference Curves\nWe encounter indifference curves when we consider the case of multi-species epidemics, as described by Holt et al. (2003). Suppose there is an infectious disease that can infect multiple species. In order to be above the epidemic threshold, there have to be a certain minimum number of susceptible individuals.\nOn one side of the curve – where the minimum conditions for an epidemic are exceeded – an epidemic is possible. On the other side of the curve, no epidemic is possible. Any combination of species numbers along the isoclines satisfy the conditions equally well. This is why we call them “indifference curves.”\nStart with the trivial case where the two species don’t interact at all. There will be an epidemic if there are either enough of species 1 or of species 2. The region where both species are below their respective thresholds lies inside the rectangular isocline\n\nx &lt;- seq(1,10,,500)\nx1 &lt;- seq(0,9,,100)\nplot(x,x, type=\"n\", axes=FALSE, frame=TRUE, xlab=\"Species 1\", ylab=\"Species 2\")\naxis(1,at=c(7), labels=c(expression(hat(S)[1])))\naxis(2,at=c(7), las=2, labels=c(expression(hat(S)[2])))\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     par(\"usr\")[2], par(\"usr\")[4],\n     col = grey(0.95))\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     7, 7, col=\"white\")\n#polygon(c(x1,rev(x1)), c(x1,rev(x1)), col=\"green\", border=FALSE)\nsegments(0,7,7,7, lwd=3, col=\"red\")\nsegments(7,0,7,7, lwd=3, col=\"red\")\ntext(5,5, expression(R[0]&lt;1))\ntext(8,8, expression(R[0]&gt;1))\n\n\n\n\n\n\n\n\nNow consider the slightly more interesting case where hosts of different species can substitute for each other. This means that even if the critical threshold for either of the species is reached, there can still be an epidemic. If the pathogen is not well adapted to a generalist-transmission mode, this effect might be quite small. We can call the epidemic isocline that arises from such conditions weakly-interacting.\n\ng &lt;- seq(0,sqrt(1/5),length=100)\nh &lt;- sqrt(1-(5*g^2))\nplot(g,h, type=\"n\", \n     axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     ylim=c(0,1.1), xlim=c(0,0.5),  \n     xlab=\"Spcies 1\", ylab=\"Species 2\")\nrect(par(\"usr\")[1], par(\"usr\")[3],\n     par(\"usr\")[2], par(\"usr\")[4],\n     col = grey(0.95))\npolygon(c(g[1],g),c(0,h),col=\"white\", border=FALSE)\nlines(g,h,col=\"red\", lwd=3)\naxis(1,at=c(0.4485), labels=c(expression(hat(S)[1])))\naxis(2,at=c(1), las=2, labels=c(expression(hat(S)[2])))\ntext(0.37, 0.8, expression(R[0]&gt;1))\ntext(0.23,0.63, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nI’ve left the non-interacting isocline in this figure to show how, even though species are only interacting weakly, the space in which an epidemic is possible is greater.\nNow consider the case where substitutable.\n\nm &lt;- (0.8432192-0.1235539)/(1.230762-8.257112)\nb &lt;- 0.8432192-(-m*1.230762)\nxint &lt;- -b/m\n\nplot(x, 1/x, type=\"n\", lwd=3, col=\"red\", xlim=c(1.5,9), ylim=c(0.15,1/1.2), axes=FALSE,\n     xlab=\"Species 1\", ylab=\"Species 2\")\nbox()\npolygon(c(seq(1,10,length=100), seq(10,1,length=90)), c(m*seq(1,8.257112,length=100)+1.2*b, rep(8.257112,90)),\n        col=grey(0.95), border=\"red\", lwd=3)\n#segments(1.230762,0.8432192,8.257112,0.1235539, lwd=3, col=\"red\")\naxis(1,at=c(8.7), labels=c(expression(hat(S)[1])))\naxis(2,at=c(0.7423), las=2, labels=c(expression(hat(S)[2])))\ntext(6.5, 0.56, expression(R[0]&gt;1))\ntext(3,0.3, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nA perfectly substitutable curve is linear. This means if you can substitute one individual of species 2 for one individual of species 1 when species 1 is just below its critical threshold and still get an epidemic, you can substitute one for one at any point along the isocline. Now, the slope might not be unity. Maybe you have to substitute two of species 2 for one of species 1. The key is that ratio of substitution remains the same for any mixture of the two species.\nThings get more interesting when having a mixture of the two species makes it more likely that there will be an epidemic when there is a more even mixture of the two species than when the mixture is toward one of the extremes (i.e., mostly species 1 or mostly species 2). We call such an isocline complementary.\n\nplot(x, 1/x, type=\"l\", lwd=3, col=\"red\", xlim=c(1.5,9), ylim=c(0.15,1/1.2), axes=FALSE,\n     xlab=\"Species 1\", ylab=\"Species 2\")\nbox()\npolygon(c(seq(1,10,by=0.1), seq(9,1,by=-0.1)), c(1/seq(1,10,by=0.1), rep(10,81)), col=grey(0.95), border=\"red\")\naxis(1,at=c(8.12), labels=c(expression(hat(S)[1])))\naxis(2,at=c(0.8325), las=2, labels=c(expression(hat(S)[2])))\ntext( 5, 0.32, expression(R[0]&gt;1))\ntext(2.5, 0.24, expression(R[0]&lt;1))\n\n\n\n\n\n\n\n\nThe convexity of this plot indicates the existence of diminishing marginal effectiveness of each species to maintain the epidemic. To see this, we can look at how the rate of substitution happens at different mixtures of the two species. For example, as you approach the extreme of \\(S_1=0\\), it takes increasingly more of \\(S_2\\) to stay above the epidemic threshold. This is obviously also true as we approach the \\(S_2=0\\) extreme as well, but we’ll focus on the \\(S_1=0\\) extreme here. In the middle of the range, a small change in one can be compensated by a small change in the other, making the epidemic threshold easier to achieve in the middle of the species’ population sizes.\n\nx &lt;- seq(0,12,,1000)\nplot(x, exp(-0.5*x), \n     type=\"n\", \n     axes=FALSE, frame=TRUE, \n     xaxs=\"i\", yaxs=\"i\", \n     xlab=\"Species 1\",\n     ylab=\"Species 2\", \n     xlim=c(2,12), \n     ylim=c(0.002,0.4))\nlines(x,exp(-0.5*x), lwd=3)\nsegments(2.1,exp(-0.5*2.1),2.1,exp(-0.5*2.6), lwd=2, col=\"red\")\nsegments(2.1,exp(-0.5*2.6),2.6,exp(-0.5*2.6), lwd=2, col=\"red\")\nsegments(5,exp(-0.5*5),5,exp(-0.5*5.5), lwd=2, col=\"red\")\nsegments(5,exp(-0.5*5.5),5.5,exp(-0.5*5.5), lwd=2, col=\"red\")\n\n\n\n\n\n\n\n\nFor vector-borne pathogens with complex life cycles, passage through an intermediate host is obligate for the perpetuation of the transmission cycle. Frequently, passage through the ultimate host is also obligate. Elimination of either intermediate or ultimate hosts from the community will lead to pathogen extinction. Thus, in the alternating case, a critical threshold exists for one or both species. As long as both host species co-exist above their minimum critical densities, the presence of a mix of both hosts makes pathogen persistence more efficient – this is why the isocline bends inward (i.e., is convex).\n\nx &lt;- seq(0,10,,100)\ny &lt;- 2/x\n\nplot(x,y,type=\"n\", axes=FALSE, frame=TRUE, xlab=\"Species 1\",\n     ylab=\"Species 2\", xlim=c(0,7), ylim=c(0,7))\npolygon(c(x,rev(x)), c(y,rep(8,100)), col=grey(0.95))\nlines(x,y, lwd=3, col=\"red\")\ntext(0.65, 0.61, expression(R[0]&lt;1))\ntext(2.7, 2.1, expression(R[0]&gt;1))\n\n\n\n\n\n\n\n\nSo far, all the interactions between species have made the epidemic more likely (or have been neutral). Sometimes, the presence of a second host species actually makes an outbreak less likely. For example, in zooprophylaxis, a dead-end host protects the host of interest (usually humans) by providing an alternate source of blood meals for biting arthopods such as mosquitoes, ticks, or triatomine bugs. This isocline for the inhibitory interaction has a positive slope: the presence of species 2 means you need more of species 1 to have an epidemic.\n\nx &lt;- seq(1,15,,100)\ny &lt;- -15 + 2*x\n\nx1 &lt;- seq(8,15,,100)\ny1 &lt;- -15 + 2*x1\n\nplot(1:15,1:15,type=\"n\", xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE, frame=TRUE, xlab=\"Species 1\", ylab=\"Species 2\")\naxis(1,at=c(8), labels=c(expression(hat(S)[1])))\n#polygon(c(8,8:15, col=grey(0.95)))\npolygon(c(x1[1],x1,x1[100]), c(1,y1,1), col=grey(0.95), border=FALSE)\ntext(12,4.2, expression(R[0]&gt;1))\ntext(7.5, 5.2, expression(R[0]&lt;1))\nlines(x,y, lwd=3, col=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#contour-plots",
    "href": "interpreting.html#contour-plots",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.7 Contour Plots",
    "text": "2.7 Contour Plots\nA contour plot uses an idea very similar to that of an isocline. The most familiar example of a contour plot for most people is probably an elevation map, where the contour line joins points of equal elevation. It can take some practice to learn how to read a contour plot, but it is well worth the effort. Contour plots actually provide better information on spatial relationships in three dimensions than do false 3D plots, even if the latter can look cool.\nThe concept of resilience is central to human ecology and sustainability. A common visual representation of resilience depicts two adjacent basins in a plane. These basins represent attractors for the system. A ball moving along the plane can get pulled into either of them. The catch is that one of these basins represents a good attractor, while the other one is bad. One of these basins is deeper than the other and represents the good attractor. However, some slow-moving factor is forcing the system, making it more likely that it will end up in the bad attractor. As intuitive as the spatial metaphor for resilience may be, it turns out to be quite hard to represent it.\nOne way to represent this figure is using a contour plot. Here we will take advantage of the ggplot aesthetic geom_contour() to render our contours.\n\nrequire(mvtnorm)\nrequire(ggplot2)\n# Create grid\nx &lt;- seq(-4, 4, length.out = 100)\ny &lt;- seq(-4, 4, length.out = 100)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Define parameters for the two basins\nmu1 &lt;- c(-1, 0)  # Center of first basin\nmu2 &lt;- c(2, 0)   # Center of second basin\nsigma1 &lt;- matrix(c(0.5, 0, 0, 0.5), 2, 2)  # Deeper basin\nsigma2 &lt;- matrix(c(1, 0, 0, 0.7), 2, 2)  # Shallower basin\n\n# Calculate heights\nz1 &lt;- -dmvnorm(grid, mu1, sigma1)\nz2 &lt;- -0.7 * dmvnorm(grid, mu2, sigma2)\nz &lt;- matrix(z1 + z2, nrow = length(x))\n## for ggplot\ngrid$z &lt;- z1 + z2\n\n## ggplot topo rendering\nggplot(grid, aes(x = x, y = y)) +\n  geom_contour(aes(z = z), bins = 15, color = \"gray30\") +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\n\n\nThe left-hand basin is deeper, as indicated by the greater number of contour lines making it up. It’s also steeper, since these lines are closer together. A ball that gets pulled down into that basin should stay there. However, if some slow-moving force like climate change alters the structure of the landscape, a ball in that basin that is subjected to shocks might more easily jump out and land in the less desirable basin to the right.\nWe can also represent this surface using a false 3D image, along with the projected contours, using the plot3D library.\n\nlibrary(plot3D)\n\npersp3D(x, y, z,\n        theta = 90, phi = 30,\n        contour = TRUE,\n        shade = 0.5,\n        colkey = FALSE,\n        expand = 0.5,\n        colvar = z,\n        col = colorRampPalette(c(\"darkblue\", \"blue\", \"lightblue\", \"white\"))(100),\n        axes = FALSE,  # Remove axes\n        box = FALSE,   # Remove box\n        xlab = \"\", ylab = \"\", zlab = \"\") \n\n\n\n\n\n\n\n\nAnother nice example of a contour plot is a little less abstract. In Jones (2009), I introduced an idea called the human demographic space. Essentially, there is a range of values of total fertility rate (TFR), which is the number of live births that a woman would have if she survived to age 50, and life expectancy at birth (\\(e_0\\)), which is the average age of death in a population, that is compatible with the human life cycle. Different combinations of TFR and \\(e_0\\) imply different population growth rates. All things being equal, a population that has a higher TFR will grow faster. However, populations with high TFR often also have low \\(e_0\\), which will lower the growth rate.\nTo calculate the intrinsic rate of increase, \\(r\\), implied by a particular combination of TFR and \\(e_0\\), we use the following approximation:\n\\[\nr \\approx \\frac{\\log(TFR) + \\log(S(a))}{T},\n\\] where \\(S(a)\\) is the survivorship (i.e., the fraction of all ever-born individuals still alive) at age at first reproduction, and \\(T\\) is the mean age of childbearing, also known as the generation time of the population. This approximation comes from the demographic identity \\(R_0 = e^{rT}\\), where \\(R_0\\) is the net reproduction ratio, or the ratio of population size from one generation to the next. Livi-Bacci noted that the net reproduction ratio is well-approximated by the product of TFR and \\(S(a)\\). The rest is just algebra.\nNote that this relationship uses the the fraction surviving until age 20 and not life expectancy at birth. Fortunately, these two are very highly correlated. I use \\(S(a)\\) from the West model life tables of Coale and Demeney, which are indexed by \\(e_0\\). Life tables (i.e., survivorship curves) are given for different levels of life expectancy at birth and we can simply read off the value of \\(S(a)\\) associated with a given value of \\(e_0\\).\n\n# S(20) from CDMLT West\nSa &lt;- c(0.3562541, 0.3969429, 0.4358999, 0.4732345, 0.5090507,\n        0.5434457, 0.5765105, 0.6083286, 0.6389776, 0.6685286, 0.697048,\n        0.724596, 0.7543878, 0.7817066, 0.805704, 0.8290955, 0.8518434,\n        0.8739262, 0.8953284, 0.9160435, 0.935412, 0.952371, 0.9676141,\n        0.980381, 0.9899677)\ntfr &lt;- seq(1,8, by=0.5)\nT &lt;- 27.5 # human generation time\n\n# calculate r from TFR, l(20), and generation time (T)\nf &lt;- function(r,Sa,TFR,T) y &lt;- -r + (log(TFR) + log(Sa))/T\n\ncalcr &lt;- function(S20,tfr, twosex=TRUE){\n  rr &lt;- matrix(0,nr=length(S20),nc=length(tfr))\n  if(twosex) tfr &lt;- tfr/2\n  \n  for(i in 1:length(S20)){\n    for(j in 1:length(tfr)){\n      rr[i,j] &lt;- uniroot(f, c(-0.2,0.1), Sa=S20[i], TFR=tfr[j], T=T)$root\n    }\n  }\n  \n  rr\n}\n\n## calculate the implied growth rates\nrr &lt;- calcr(S20=Sa,tfr=tfr)\n\n# e(0) from CDMLT West\ne0 &lt;- c(18.03431, 20.44308, 22.85202, 25.26012, 27.66740, 30.07391,\n        32.47963, 34.88452, 37.28851, 39.69149, 42.09338, 44.49401, 47.08864,\n        49.54198, 51.79709, 54.08686, 56.40571, 58.75305, 61.1244, 63.51476,\n        65.85406, 68.33622, 70.91915, 73.5699, 76.27995)\n\n## empirical TFR/e0 pairs that span the space\n# data points\nusa &lt;- c(2.05,77)\nven &lt;- c(6.5,65)\nache &lt;- c(8,37.5)\nkung &lt;- c(4,34)\ntaiwan &lt;- c(7.35,29)\n\n\ncontour(tfr,e0,t(rr), \n        lwd=2, col=\"blue4\", \n        xlim=c(2,8), \n        xlab=\"Total Fertility Rate\", \n        ylab=\"Life Expectancy\")\n\ntext(usa[1],usa[2],\"USA\", col=\"red4\", bg=\"red4\")\ntext(ven[1],ven[2], \"Venezuela\", col=\"red4\", bg=\"red4\")\ntext(ache[1],ache[2], \"Ache\", col=\"red4\", bg=\"red4\")\ntext(kung[1],kung[2], \"!Kung\", col=\"red4\", bg=\"red4\")\n\n\n\n\n\n\n\n\nThe contours represent a surface that increases monotonically as it goes up and to the right (i.e., as \\(e_0\\) and TFR increase). The four populations largely span the space and each one is above the \\(r=0\\) contour (though the USA is just barely above). Venezuela (in 1967) was characterized by very high life expectancy and incredibly high fertility, giving it an astounding growth rate of nearly \\(r=0.04\\). This is likely the fastest a sizable human population has ever grown intrinsically. That translates into a doubling every 17 years! See Jones (2009) for more details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "interpreting.html#plotting-tricks",
    "href": "interpreting.html#plotting-tricks",
    "title": "2  Interpreting Scientific Figures",
    "section": "2.8 Plotting Tricks",
    "text": "2.8 Plotting Tricks\nIn chapter Chapter 3, I will introduce how to produce scientific figures in R. However, I’ve repeatedly done some things in these notes that merit a brief explanation. Otherwise, there is a risk of things seeming obscure and generally confusing.\nTheoretical plots usually don’t depend on specific values of inputs or functions – you typically care just about the shapes and not the specific values. You are trying to show the general behavior of your system. R is a statistical programming language and, as such, expects you to be plotting data. Presumably, you care about the actual values when data are involved. For our theoretical plots, we usually want to suppress the values on the plot’s axes. This is why nearly all of these figures include the arguments to the plot() command axes=FALSE and frame=TRUE. This suppresses the axes and any ticks and labels indicating specific values on them. We can then add in custom axis labels, such as the critical population size for each species in the multi-species epidemic isoclines using the command axis().\nPerhaps a more mysterious trick I use is to include the arguments xaxs=\"i\" and yaxs=\"i\". This is really the special sauce of a scientific-theory plot in R. Again, R expects data when you call the plot() command. A good aesthetic practice for data plots is to pad the range of the observed data and R does this by default. By forcing the style of the axes to be “internal” (that’s what the “i” stands for), you restrict the axes to the range of your data. This means that \\(y\\)-intercepts actually intercept the \\(y\\) axis, curves that should start at zero actually look like they’re starting at zero, etc.\nWe often want to lay out the axes but not draw a curve quite yet. To do this, we add the argument type=\"n\" to the plot() command. This allows us to build up complex figures with more precision and control. You might notice that we often plot the actual curve we care about last. This is because we want it on top of the various lines we’ve added to indicate interesting bits of the curve (e.g., equilibria and such).\n\n2.8.1 locator\nSometimes you need to find a spot on your figure where you want to add text or draw a segment or an arrow. R has a very handy function that allows you to interactively determine the coordinates of a point on your axes. Use the function locator(n) with a plot rendered in the RStudio window. You can then click your mouse n times on the plot and will get returned a list with the n (x,y)-coordinate pairs. There are a couple figures in chapter Chapter 3 where I use locator() to find coordinates for drawing points or text. This interactive usage is hard to translate into static notes, so I have to hard-code the coordinates in this document.\n\n\n2.8.2 Colors\nYou may have noticed that I use several different ways to specify colors throughout these notes. R has a number of colors built into its base. You can see them all by typing colors() at the prompt. I’m not going to do that here, because there are more than 650 of them and it would be a mess. But we can get a hint.\n\nwhat_are_the_colors &lt;- colors()\nlength(what_are_the_colors)\n\n[1] 657\n\n# the first 10\nwhat_are_the_colors[1:10]\n\n [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n [5] \"antiquewhite2\" \"antiquewhite3\" \"antiquewhite4\" \"aquamarine\"   \n [9] \"aquamarine1\"   \"aquamarine2\"  \n\n\nSometimes I specify colors based on their hexadecimal codes such as #0D0887FF or #9C179EFF. These start with a hash (#). What follows the hash is actually a triplet of codes specifying red, green, and blue components of the color using hexidecimal (i.e., base 16) codes, which go from 00 to FF. I can almost hear your cries as I write this: “but those codes have eight digits following the hash, not six!” Sometimes a hex color code has a fourth byte (as these examples do), which specifies the alpha channel (i.e., degree of transparency) of the color. Note that for the two codes I’ve used here, this last byte is FF, which is the maximum value, so not at all transparent.\nYou can find various web tools for picking colors and generating hex color codes. There is a lot to be said for using hex codes because the code #F0F8FF is a lot less ambiguous than aliceblue.\nIf hexidecimal codes are not your jam, you can also specify a color using RGB (red, green, blue) codes. Once again, an optional fourth value specifies the alpha channel. The one thing that’s tricky about rgb() is that you need to specify what’s known as a radix or what the maximum value is. By convention, this is typically 255 (because of course it is). You could also use a radix of 1 or really anything you like. However, conventions make coding more legible for a wider audience (that may include your future self!). Note that the radix for the hex codes is 16 (or FF is you want to be cute).\nHere we can recreate a figure from earlier, using rgb(), rather than built-in colors, to specify the line colors.\n\ncurve(5*exp(x), 0, 10, lwd=3, col=rgb(255,0,0,255, maxColorValue = 255), xlab=\"x\", ylab=\"y\")\ncurve(1*exp(x), 0, 10, lwd=3, col=rgb(0,0,0,255, maxColorValue = 255), add=TRUE)\n\n\n\n\n\n\n\n\nAnother way to specify colors is by using a color palette. Base R has a few built-in palettes, including rainbow, heat, topo.colors, and terrain.colors. To be honest, these aren’t really the best. We used a color palette above when we compared the curves of linear, quadratic, exponential, and logarithmic functions using the viridisLite package. In that plot, we used a classic palette called plasma. It’s interesting to see what these palette functions actually produce:\n\nrequire(viridisLite)\nplasma(3)\n\n[1] \"#0D0887FF\" \"#CC4678FF\" \"#F0F921FF\"\n\n\nA good color palette is designed to do several things. It should make attractive figures. Basically, you want the colors to be cool and, well, colorful. But you also want them to be perceptually uniform, meaning that values that are close to each other have colors that are close to each other throughout the range of the palette. Ideally, you also want your color choices to be robust for people with color blindness.\nMy personal favorite color-palette package is MetBrewer, which translates the color schemes of various masterworks from the Metropolitan Museum of Art. You can check out the various palettes and the works that inspired them on the MetBrewer GitHub.\n\nlibrary(MetBrewer)\nmet.brewer(\"Johnson\",7)\n\n\n\n\n\n\n\n\nAnd just to remind ourselves what we’re actually doing when we create a palette:\n\ndope_colors &lt;- met.brewer(\"Johnson\",7)\nas.character(dope_colors)\n\n[1] \"#A00E00\" \"#C03800\" \"#DC7400\" \"#F6C200\" \"#529A6F\" \"#066793\" \"#132B69\"\n\n\nNote that these are in the six-digit form (no alpha channel).\n\n\n\n\nGadgil, Madhav, and William H. Bossert. 1970. “Life Historical Consequences of Natural Selection.” The American Naturalist 104 (935): 1–24. http://www.jstor.org/stable/2459070.\n\n\nHolt, R. D., A. P. Dobson, M. Begon, R. G. Bowers, and E. M. Schauber. 2003. “Parasite Establishment in Host Communities.” Ecology Letters 6 (9): 837–42. https://doi.org/10.1046/j.1461-0248.2003.00501.x.\n\n\nJones, J. H. 2009. “The Force of Selection on the Human Life Cycle.” Evolution and Human Behavior 30 (5): 305–14. https://doi.org/10.1016/j.evolhumbehav.2009.01.005.\n\n\nNair, Jayakrishnan, Adam Wierman, and Bert Zwart. 2022. The Fundamentals of Heavy Tails: Properties, Emergence, and Estimation. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge: Cambridge University Press.\n\n\nWeitzman, M. L. 2009. “On Modeling and Interpreting the Economics of Catastrophic Climate Change.” The Review of Economics and Statistics XCI (1): 1–19. https://doi.org/10.1162/rest.91.1.1 .",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  },
  {
    "objectID": "drawing.html#introduction",
    "href": "drawing.html#introduction",
    "title": "2  Theoretical Scientific Figures in R",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nR has powerful graphics capabilities. While we typically use these for plotting data, we can also make publication-quality plots for elucidating theoretical topics as well.\nThese notes are a very tentative start to a much larger body of work. I hope they are nonetheless helpful in their rather incomplete form."
  },
  {
    "objectID": "drawing.html#the-taylor-series-approximation-is-your-friend",
    "href": "drawing.html#the-taylor-series-approximation-is-your-friend",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.2 The Taylor-Series Approximation is Your Friend",
    "text": "3.2 The Taylor-Series Approximation is Your Friend\nYou may be familiar with Taylor polynomials (or series) and how useful they are for applied mathematics and science. A Taylor series allows you to approximate a function \\(f(x)\\) in terms of an infinite sum of its derivatives taken at a single point \\(a\\):\n\\[ f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2!} + \\frac{f'''(a)(x-a)^3}{3!} + \\cdots\n\\]\nWhen we truncate the Taylor series at the first term, we produce a linear approximation of our function. In other words, it’s a tangent line. It turns out that we often want to draw tangents in theoretical figures and just as the Taylor series can help us derive theory, so too can it help us generate plots!\n\n3.2.1 Cobb-Douglas Production Function\nThe Cobb-Douglas production function is a model for production that is the product of two power functions. The classic model combines capital (\\(K\\)) and labor \\(L\\). The inputs are raised to powers \\(\\alpha\\) and \\(\\beta\\).\n\\[ W = K^{\\alpha} L^{\\beta}\n\\]\nIn the Cobb-Douglas form, the exponents also turn out to be the elasticities of production with respect to the inputs. So, suppose that \\(\\alpha=0.25\\), this means that a 1% increase in the capital will increase overall wealth by 0.25%.\nIf \\(\\alpha + \\beta =1\\), then there are constant returns to scale: doubling inputs will double the output. If, on the other hand, \\(\\alpha + \\beta &gt; 1\\), there are increasing returns to scale so that doubling inputs will more than double the output.\nThe optimal balance between capital and labor occurs when a budget line is tangent to this curve.\n\nx &lt;- seq(0,0.8,length=100)\nL &lt;- seq(0,1,length=100)\nK &lt;- (1/L^0.7)^(1/0.3)\n#derivative\nfprime &lt;- -5.6/(0.3^2.4)\n# make a function to calculate K from L\nkf &lt;- function(L) (2/L^0.7)^(1/0.5)\n# pick an arbitrary point for tangency\na &lt;- 0.3\n\n\nplot(L,kf(L), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     xlim=c(0,1), ylim=c(0,50), \n     col=grey(0.65), \n     xlab=\"Labor\", ylab=\"Capital\")\n# tangent\nlines(x, kf(a)+fprime*(x-a), lwd=1, col=\"black\")\n# values on the axes\nsegments(0.3,0,0.3,kf(0.3), col=grey(0.65), lty=3)\nsegments(-0.1,kf(0.3),0.3,kf(0.3), col=grey(0.65), lty=3)\nmtext(expression(hat(L)),1,at=0.3, padj=0.5)\nmtext(expression(hat(K)),2, at=kf(0.3), adj=2, padj=0.5, las=2)\n\n\n\n\n\n\n\n\nGraphics Tip: Note that in labeling the optimum value of capital, we used the argument las=2. This says to print text always perpendicular to the axis and is useful for labeling interesting points on particularly the vertical axis.\nYou will notice that in most of these plots, we suppress the axis labeling. For a theoretical plot, we want to see the shape of the relationships between variables and care less about the specific \\((x,y)\\) values. This means that we typically include in the plot() command the argument axes=FALSE. When we turn off the axes, this also, by default, removes the frame around the axes. We usually want that, so we have to also include the argument frame=TRUE.\nWe can add mathematical, typset material to any text (e.g., axis labels, labels for equilibria and other interesting points, titles, etc.) using expression(). There is a stripped-down markup language for this in R. To learn more, do a help search for plotmath.\n\n\n3.2.2 Marginal Value Theorem\nSomething like the Marginal Value Theorem (MVT), a phrase coined by Charnov (1976), appears in many applications: foraging theory (Charnov 1976), sexual selection (Parker and Stuart 1976), the evolution of virulence (Baalen and Sabelis 1995), and of course, life history theory (Smith and Fretwell 1974). If you pay careful attention, you will notice that it’s always the maximization of some sort of ratio, where the numerator and denominator trade-off. In this case, the MVT solution arises naturally from the quotient rule for differtiation. The MVT states that the optimal value of the ratio fitness measure can be found when a straight line rooted at the origin is tangent to the fitness/constraint function. This the eponymous “marginal value.”\nIn previous examples, we specified the tangent point. Here we solve for the optimal value, which for the marginal value theorem, says that the rate is maximized where a line rooted at the origin is tangent to the utility function. We can mess around with different slopes and try to find something that’s approximately right or we can find the actual solution using the root-finding function in R. I need to acknowledge Mike Price here because he helped me get unstuck as I flailed to get uniroot() to work. I should also note that when I publish notes like this, even if I accompany them with caveats about incompleteness, you are seeing the product of lots of trial and error (lots of error, I assure you). Remember, the struggle is part of the scientific process.\nAnyway, code for the marginal value theorem (in its many guises).\n\nx &lt;- seq(0,20,length=500)\n# utility function fp&gt; 0 fpp &lt; 0\n# turns out that RMarkdown does not handle comments with single quotes \n# fp == deriv of f; fpp == 2nd deriv of f\nf &lt;- function(x) {\n  1 - exp(-0.2*(x-1))\n}\n# derivative of the utility function\nfprime &lt;- function(x) {\n  0.2*exp(-0.2*x)*exp(0.2)\n}\n\n# f + fp*(z-x) = 0\n# z = x -(f/fp)\n# solve for tangency; find the root of this\nxinter &lt;- function(x) {\n  return(x - f(x)/fprime(x))\n}\n\nsoln &lt;- uniroot(xinter,c(0,10))\n\nplot(x,f(x), type=\"l\", lwd=2, xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Time\",\n     ylab=\"Rate of Gain\",\n     ylim=c(0,1))\nlines(x,(f(soln$root)/soln$root)*x,col=\"red\")\nsegments(soln$root,0,soln$root,f(soln$root), lty=2, col=\"red\")\nsegments(0,f(soln$root),soln$root,f(soln$root), lty=2, col=\"red\")\nmtext(expression(hat(t)),1,at=soln$root, padj=1)\n## some non-optimal adaptive functions\nlines(x,(f(2)/2)*x,col=grey(0.85))\nlines(x,(f(1.5)/1.5)*x,col=grey(0.85))\nlines(x,(f(11)/11)*x,col=grey(0.85))\n\n\n\n\n\n\n\n\nMarginal value theorem plot. A line rooted at the origin that is tangent to the gain curve provides the optimal patch-residence time, \\(\\hat{t}\\).\nGraphics Tip: In this case, we had to find the point where a line rooted at the orgin is tangent to the gain curve. We found this using uniroot() which is a one-dimensional optimization routine that searches an interval for the zero of a function. The function returns an list with at least four elements. The one we want is called root, hence the use of soln$root in plotting arguments in the above code.\n\n\n3.2.3 Optimal Age at First Reproduction\nSimple Example of the optimal trade-off between adult reproductive value (\\(E\\)) and juvenile recruitment (\\(S\\)). Charnov (1997) has suggested that fitness is a product, broadly construed, of three things: juvenile recruitment, annual fertility of adults, and adult life expectancy. In turn, these elements can be combined. For example, the product of annual fertility and adult life expectancy can be thought of as adult reproductive value because it is the expected total reproduction over the an individual’s lifespan, conditional on them being recruited into the breeding population. As Charnov notes, these things are likely to trade-off. Moreover, the multiplicative form of fitness makes these trade-offs particularly straightforward to visualize and analyze.\nWhen we plot the allowable combinations of \\(\\log(S)\\) and \\(\\log(E)\\), we get a convex plot of the iso-fitness plot linking the logs of \\(E\\) and \\(S\\), which indicates diminishing marginal returns in both dimensions. The optimal life history is the one for which a line with a slope of -1 is tangent to this iso-fitness constraint curve. Why? The fitness measure (assuming population stationarity) is \\(R_0 = S\\, E\\). Take logs such that \\(\\log(R_0) = \\log E + \\log S\\) and differentiate with respect to age at first reproduction (\\(\\alpha\\); which, in Charnov’s formalism, is the control parameter for the life history):\n\\[\n\\frac{d \\log R_0}{d\\alpha} = \\frac{d \\log E}{d\\alpha} + \\frac{d \\log S}{d\\alpha}.\n\\]\nSet this equal to zero and rearrange. Divide \\(d\\log E/d\\log \\alpha\\) by \\(d\\log S/d\\log \\alpha\\) and we find the optimality criterion:\n\\[\n\\frac{d\\log E}{d \\log S} = -1.\n\\]\n\ng &lt;- seq(0,sqrt(1/5),length=200)\nh &lt;- sqrt(1-(5*g^2))\nhf &lt;- function(g) sqrt(1-(5*g^2))\n## derivative\nfp &lt;- function(g) -5*g/sqrt(1-5*g^2)\n\n# solve for tangency; find the root of this\n# note the sign change\nginter &lt;- function(g) {\n  return(g + hf(g)/fp(g))\n}\n\n## do not search over whole interval \n## because g values &gt; sqrt(5) will give NaNs!\na &lt;- uniroot(ginter,c(0,0.4))$root\n## this simply extends the plotting range \n## so that the tangent line fills the plotting range\ngg &lt;- seq(0,0.5+a,length=500)\n\nplot(g,hf(g), type=\"l\", lwd=2, axes=FALSE, frame=TRUE, \n     yaxs=\"i\", xaxs=\"i\", \n     ylim=c(0,1.1), xlim=c(0,0.5), \n     xlab=\"log(E)\", \n     ylab=\"log(S)\")\n## first-order Taylor Series approx\nlines(gg, hf(a)+fp(a)*(gg-a), col=\"red\")\nsegments(a,0,a,hf(a), col=grey(0.65), lty=3)\nsegments(-0.1,hf(a),a,hf(a), col=grey(0.65), lty=3)\nmtext(expression(hat(E)),1,at=a, padj=0.5)\nmtext(expression(hat(S)),2, at=hf(a), adj=2, padj=0.5, las=2)\n\n\n\n\n\n\n\n\n\n\n3.2.4 Fitness Sets\nRichard Levins (1962) introduced a the idea of fitness sets as a way to think about evolution in variable environments. This approach was more fully fleshed out in his subsequent monograph (R. Levins 1968). The fundamental idea is to represent the fitness of organisms in the different conditions that make up their variable environments and then find the strategy that maximizes fitness across these environments. The optimum can be a generalized compromise across the different environments or it can be the production of polymorphic specialized phenotypes that better match specific environmental conditions.\nConsider first a population with two phenotypes where the peak fitness in the two environments are quite separated from each other such that the fitness functions do not overlap tremendously. It’s conventional to assume Gaussian distributions of fitnesses with respect to the environment for simplicity, but to make things more interesting, we can use Gamma distributions, which will have more right-skew. The basic idea behind using these peaked functions is that there is an optimum for the environment and that fitness falls off as you move away from this optimum value of the phenotype. A Gaussian distribution just makes quite specific assumptions about how fitness falls off as the phenotype differs from the optimum: it does so symmetrically around the maximum and it declines exponentially in the squared difference from the optimum, while the Gamma distributions will be asymmetric in the way fitness falls off from the peak. The actual form of the fitness function will depend on the particulars of the environment and the phenotypes in question.\n\n## skewed gamma distributions\nx &lt;- seq(0,25,,1000)\nk1 &lt;- 9\ns1 &lt;- 0.5\nk2 &lt;- 7.5\ns2 &lt;- 1\n\nplot(x,dgamma(x,shape=k1,scale=s1), type=\"l\", lwd=2,\n     axes = FALSE, frame=TRUE,\n     xlab=\"Phenotype\", ylab=\"Fitness\")\nlines(x,dgamma(x,shape=k2,scale=s2))\n\n\n\n\n\n\n\n\nIn this figure, we plotted the fitness functions against the environment. We can cut out the middleman, as it were, and simply plot the fitness functions against each other in a manner analogous to phase-plane analysis of, e.g., the Lotka-Volterra predator-prey model. Environment becomes implicit in the plots. What we have done is represent all possible phenotypes in our 2-dimensional fitness space.\nA quick note on convexity is probably warranted here. A space is said to be convex if, for any two points contained within the space, the entirety of the line segment that connects these points is also contained within the space. It’s easy to see that a line segment connecting points in horns of this fitness set would not be entirely contained within the set.\nOur two distributions overlap quite a bit and we will see that they form a convex fitness set. This suggests the geometrical interpretation of convexity, namely, that it implies the ability of a compromise phenotype. To find the optimal (compromise) phenotype, we add the adaptive function for a coarse-grained environment. For a coarse-grained environment, the adaptive function will have a hyperbolic form. Here again, the issue of convexity arises. An adaptive function that takes the hyperbolic form as in figure 3, is also said to be convex. Just as a convex fitness set implies an an optimum phenotype that is a compromise, convexity in the adaptive function suggests that average values have higher fitness than extremes. As the adaptive-function isoclines move from the center to the extremes, the increase in fitness in one dimension must be greater than the reduction of fitness in the other dimension. This is also related to diminishing marginal rate of substitution. Note, for example, as the isocline moves away from its convex center upward in the direction of \\(W_2\\), it takes increasing fitness in the \\(W_2\\) dimension to make up for lost fitness in the \\(W_1\\) direction.\n\n## for the isoclines\nG &lt;- seq(0,0.5,length=100)\nalpha &lt;- 0.5\nbeta &lt;- 0.5\n# simple function to calculate hyperbolic isolclines following Cobb-Douglas form\nkf &lt;- function(G,W,alpha,beta) (W/G^alpha)^(1/beta)\n\n## fitness set\nplot(dgamma(x,shape=k1,scale=s1),dgamma(x,shape=k2,scale=s2), lwd=3,\n     type=\"l\", axes = FALSE, las=1,\n     xlim=c(0,0.4), ylim=c(0,0.25),\n     xlab=expression(W[1]), ylab=expression(W[2]))\nbox()\n## convex adaptive functions\nlines(G,kf(G=G,W=0.05,alpha=0.75,beta=1), lty=2)\nlines(G,kf(G=G,W=0.05,alpha=0.85,beta=1), lty=2)\nlines(G,kf(G=G,W=0.05,alpha=0.63,beta=1), lty=2)\n\n\n\n\n\n\n\n\n\n\n3.2.5 Graphical Newton-Raphson Method\nAnother cool application of tangent lines in plots illustrates the popular and powerful family of optimization algorithms are broadly known as “Newton’s Method’ or the”Newton-Raphson Algorithm.” This algorithm finds roots of functions – that is, points where the function is zero. The basic idea is that we start from an initial guess point on our function. We then draw a line tangent to our function at this point. Finding the \\(x\\)-intercept for the tangent line, we repeat the process only this time drawing our tangent line from the point on the curve corresponding to the \\(x\\)-intercept of our last tangent line. It turns out that, for some initial guess, \\(x_0\\), the value \\(x_1 = x_0 - f(x_0)/f'(x_0)\\) is a better estimate of the root. We can then repeat this process until we are satisfactorily close to the root of the function. We can make a quick graphical demonstration Newton’s method for a very simple function \\(y=x^2-9\\). We start with a guess at \\(x=8\\). The plot shows three iterations (numbered sequentially). We can see that each iteration gets much closer to the root of this equation (at \\(x=3\\)). In fact, it gets so close after three steps that plotting another iteration is indistinguishable from the correct solution (though for a realistic tolerance, it would take a couple more steps to get right to \\(x=3\\)).\n\n## graphical newton-method\n\nx &lt;- seq(0,10, length=100)\ny &lt;- function(x) x^2 - 9\nx1 &lt;- function(a) a-y(a)/(2*a)\n\n# first iteration\na &lt;- 8\nplot(x,y(x),type=\"l\",lwd=3)\nabline(h=0)\npoints(8,55, pch=19, cex=1.5)\ntext(8,59,\"1\")\nlines(x, y(a) + (x-a)*2*a, col=\"red\")\n# second iteration\npoints(x1(a),y(x1(a)), pch=19, cex=1.5)\ntext(x1(a),y(x1(a))+4,\"2\")\na &lt;- x1(a)\nlines(x, y(a) + (x-a)*2*a, col=\"red\")        \n# third iteration\npoints(x1(a),y(x1(a)), pch=19,cex=1.5)\ntext(x1(a),y(x1(a))+4,\"3\")\na &lt;- x1(a)\nlines(x, y(a) + (x-a)*2*a, col=\"red\")  \n\n\n\n\n\n\n\n\nIf we were really feeling ambitious, we could animate this!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#using-expression-to-draw-functional-response-curves",
    "href": "drawing.html#using-expression-to-draw-functional-response-curves",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.3 Using expression() to Draw Functional-Response Curves",
    "text": "3.3 Using expression() to Draw Functional-Response Curves\nThere are several different ways that you can draw a theoretical curve. In the last chapter, I used curve() to draw many of the figures. I find that I have better control over the overall figure, however, if I can access the equation and its evaluation more directly. The quick-and-dirty way to do this is to write an expression(). An expression is an object whose evaluation is delayed until explicitly called for with the function eval(). So you can define your equation of theoretical interest, then enter its inputs, and then evaluate it when it is convenient for you (e.g., when you’re plotting).\nWe can demonstrate this functionality by comparing the Holling family of functional-response curves. These curves model, among other things, the satiation of a predator as prey density increases.\nThe most commonly-used of these functional responses is certainly Holling Type II, which is also known (e.g., in physiology) as the Michaelis–Menten function, which is a simple, if ubiquitous, model of enzyme kinetics.\n\nx &lt;- seq(0,10, length=200)\na &lt;- 0.7\nb &lt;- 1.5\nc &lt;- -1.5\nh2 &lt;- expression(a*x/(1 + a*x))\nh3 &lt;- expression(a*x^2/(b^2+x^2))\nh4 &lt;- expression(a*x^2/(b+c*x+x^2))\n#\nplot(x, eval(h4), type=\"l\", lwd=3, col=\"magenta\",  xaxs=\"i\", yaxs=\"i\", axes=FALSE, xlab=\"Abundance\", ylab=\"Response\", ylim=c(0,1.2))\nlines(x,eval(h3), lwd=3, col=\"cyan\")\nlines(x, eval(h2), lwd=3, col=\"black\")\nlegend(\"topright\", c(\"Type II\", \"Type III\", \"Type IV\"), col=c(\"black\",\"cyan\",\"magenta\"), lwd=3)\nbox()\n\n\n\n\n\n\n\n\nGraphics Tip: We add a legend to a figure, not surprisingly, with the function legend(). The first argument to legend() is its location. You can specify this with x,y-coordinates or, more simply, with a keyword from the list that includes: “bottomright”, “bottom”, “bottomleft”, “left”, “topleft”, “top”, “topright”, “right” and “center”. Sometimes you need to fiddle around with the location of the legend as it might appear quite different under different graphics devices (e.g., in the Plots window of RStudio vs. a .png file vs. a .pdf file).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#fold-catastrophe-model",
    "href": "drawing.html#fold-catastrophe-model",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.4 Fold-Catastrophe Model",
    "text": "3.4 Fold-Catastrophe Model\nA catastrophe is a sudden shift in system state (Zeeman 1976). An interesting form of catastrophe, which Scheffer (2009) discusses in detail, is the fold catastrophe.\nThis is a pretty complicated figure. The solid parts of the curve are stable – when the system state is perturbed when in the vicinity of this part of the attractor, it tends to return, as indicated by the grey arrows pointing back to the attractor. The dashed part of the attractor is unstable – perturbations in this neighborhood tend to move away from the attractor. This graphical representation of the system makes it pretty easy to see how a small perturbation could dramatically change the system if the current combination of conditions and system state place the system on the attractor near the neighborhood where the attractor changes from stable to unstable. The figure illustrates one such scenario. The conditions/system state start at point \\(F1\\). A small forcing perturbs the system off this point across the bifurcation. Further forcing now moves the system way off the current state to some new, far away, stable state. We go from a very high value of the system state to a very low value with only a very small change in conditions. Indeed, in this figure, the conditions remain constant from point \\(F1\\) to the new value indicated by the white point – just a brief perturbation was sufficient to cause the drastic change.\n\nx &lt;- seq(-12,12,length=10000)\ny &lt;- seq(12,10/sqrt(3), length=1000)\n## fold-catastrophe is a cubic\nplot(-x^3+100*x,x,type=\"l\", axes=FALSE, lwd=2, lty=2, \n     xlab=\"Conditions\", ylab=\"System State\")\nbox()\nlines(-y^3+100*y,y, lwd=2)\nlines(y^3-100*y,-y, lwd=2)\n\n# unstable\narrows(-200,-5,-200,-2.75, code=1, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,-1.5,-200, 0.75, code=2, lwd=3, length=0.1, col=grey(0.75))\n#lower stable\narrows(-200,-6,-200, -8.25, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,-11.5,-200, -9.25, code=2, lwd=3, length=0.1, col=grey(0.75))\n#upper stable\narrows(-200,13.5,-200, 11.25, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(-200,8.25,-200, 10.5, code=2, lwd=3, length=0.1, col=grey(0.75))\n# use locator() to find coordinates\npoints(357,7, pch=21, cex=2, lwd=3, bg=grey(0.75))\ntext(376.6749, 7.87279, \"F1\")\npoints(357,-11.452987, pch=21, cex=2, bg=\"white\")\narrows(357,6.5,357,4, lwd=3, length=0.1)\narrows(357,3.5,357,-10.8, code=2, lwd=3, length=0.1, col=grey(0.75))\n\n\n\n\n\n\n\n\nGraphics Tip: For the fold catastrophe, we want the upper and lower arms of the curve to be solid lines, indicating a that the attractor lies in a basin of attraction in these regions, and a dashed line in the middle, indicating that the attractor is unstable there. To do this we plot the whole curve as a dashed line lty=2 and then plot solid lines over this curve in the regions we want it to be solid. Lots of trial-and-error in making such a plot!\n\n3.4.1 Mechanistic Foundation of Fold-Catastrophe\nThe fold-castastrophe may seem like an incredibly specific model. It turns out there are various very natural ways of constructing such an attractor. Here, we discuss the approach of Noy-Meir (1975) for a resource-exploitation case.\n\n# Logistic Recruitment\nlogistic.recruit &lt;- expression(r*N*(1 - (N/K)^theta))\nno &lt;- 1\nr &lt;- 0.45\nK &lt;- 100\ntheta &lt;- 1\nN &lt;- seq(0,K,length=500)\n\n# Holling Type II Functional Response\nh2 &lt;- expression(a*N/(b + a*x))\nx &lt;- N+1\na &lt;- 0.7\nb &lt;- 3\nplot(N,eval(logistic.recruit), type=\"l\", yaxs=\"i\", lwd=3, axes=FALSE, xlab=\"Relative Producer Density\", ylab=\"Relative Productivity\", ylim=c(0,15))\nbox()\nlines(x, 6*eval(h2), lwd=2)\nlines(x, 12*eval(h2), lwd=2)\n\npoints(c(8.702413, 36.521362, 57.497146, 85.464859), c(3.485678, 10.441517, 11.003335,  5.652690), col=c(\"red\",\"red\",\"green\",\"green\"), cex=2, pch=16)\narrows(36.521362-7.5, 10.441517+2, 36.521362-1, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\narrows(36.521362+1, 10.441517+2, 36.521362+7.5, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\n#\narrows(57.497146-7.5, 10.441517+2, 57.497146-1, 10.441517+2, code=2, lwd=3, length=0.1, col=grey(0.75))\narrows(57.497146+1, 10.441517+2, 57.497146+7.5, 10.441517+2, code=1, lwd=3, length=0.1, col=grey(0.75))\ntext(c(59.13355, 86.65497), c(11.458140,  6.107494), c(\"F2\", \"F1\"))\n\n\n\n\n\n\n\n\nThere are three fixed points where the recruitment curve and the extraction curve intersect. The green points are stable fixed points, whereas the red points are unstable. The third set of points is near zero and I’ve not drawn those just to keep the plot less cluttered. This fixed point is also stable.\nIf we imagine keeping the recruitment curve constant but sweeping extraction curves continuously up through the space (as we have for one big jump in the plot from \\(F1\\) to \\(F2\\)) and tracked the three fixed points along this sweeping, we would have a fold catastrophe.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#cobwebbing",
    "href": "drawing.html#cobwebbing",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.5 Cobwebbing",
    "text": "3.5 Cobwebbing\nWith a discrete-time model in one dimension (e.g., an unstructured population model), we can trace the dynamics as we iterate the model forward using a technique called cobwebbing. Here’s a quick example of a Ricker recruitment model, a density-dependent population model with the feature that it overcompensates when numbers exceed the carrying capacity. When the population of highly reactive (i.e., has strong growth potential), this tendency for overcompensation can lead to some pretty wild dynamics. This plot shows such a case.\n\n## Ricker recruitment function\nricker.recruit &lt;- function(r0,K,N) N*exp(r0*(1-(N/K)))\n## fast growth!\nr0 &lt;- 3\nK &lt;- 50\nN &lt;- 0:150\nn0 &lt;- 25\n## iterate model for 10 time steps\nt &lt;- 10\ny &lt;- rep(0,t)\ny[1] &lt;- ricker.recruit(r0=r0,K=K,N=n0)\nfor(i in 2:t)  y[i] &lt;- ricker.recruit(r0=r0,K=K,N=y[i-1])\n\nplot(N,ricker.recruit(r0=r0,K=K,N=N), type=\"l\", col=\"black\", lwd=3, yaxs=\"i\",\n     ylim=c(0,150),\n     xlab=\"Current Number of Infections\", ylab=\"New Infections\")\nabline(a=0,b=1, lwd=2, col=grey(0.75))\nsegments(n0,0,n0,y[1], col=\"red\")\nsegments(n0,y[1],y[1],y[1], col=\"red\")\nfor(i in 2:(t-2)){\n    segments(y[i],y[i],y[i],y[i+1], col=\"red\") #vertical\n    segments(y[i],y[i+1],y[i+1],y[i+1], col=\"red\") #horiz\n    segments(y[i+1],y[i+1],y[i+1],y[i+2], col=\"red\") #vert\n    segments(y[i+1],y[i+2],y[i+2],y[i+2], col=\"red\") #horiz\n}\n\n\n\n\n\n\n\n## this could very easily be made into a function (and probably should be)\n\nThis population model with highly over-compensatory dynamics will never settle down. It always overshoots or undershoots and so fluctuates wildly. May (1976) notes that we can measure the strength of the response by the slope of the recruitment function at its equilibrium value (i.e., where the grey line of equality intersects with the recruitment function). Using the tools we’ve discussed in these note, we could calculate that slope and draw a tangent line at that point!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#numerical-derivative-for-equilibria-and-tangent-lines",
    "href": "drawing.html#numerical-derivative-for-equilibria-and-tangent-lines",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.6 Numerical Derivative for Equilibria and Tangent Lines",
    "text": "3.6 Numerical Derivative for Equilibria and Tangent Lines\nWe can recreate the figures from May’s classic paper on how simple population models can yield very complex dynamics (May 1976). May investigates the logistic map, a first-order difference equation. The logistic map is essentially a discrete-time density-dependent model.\n\\[\nX_{t+1} = aX_t(1 -X_t),\n\\]\nwhere \\(X_t\\) is the state of the population (e.g., its size) at time \\(t\\) and \\(a\\) is the per-period multiplicative growth rate in the absence of any density effect. May (1976) shows that when \\(a&gt;3\\), this model becomes unstable about its fixed point and when\n\n# logistic map\nlmap &lt;- expression(a*x*(1-x))\nx &lt;- seq(0,1,,1000)\n\n### first plot the unstable recruitment curve\na &lt;- 3.414\nx1 &lt;- eval(lmap)\nplot(x,x1, type=\"l\", lwd=2, \n     xaxs=\"i\", yaxs=\"i\", ylim=c(0,1),\n     xlab=expression(X[t]),ylab=expression(X[t+1]))\n\n## equilibrium for logistic map\nxstar1 &lt;- 1-(1/a)\n\n## numerical derivative\nx1p &lt;- diff(x1)\nxp &lt;- diff(x)\n### the equilibrium x is approximately x[706]\nm1 &lt;- x1p[706]/xp[706]\n\n### use point-slope eq for a line y - y_1 = m(x - x_1)\n### we know the point (xstar,xstar) so solve for eq we can use to draw line\nlines(x[550:850], m1*x[550:850]-m1*xstar1+xstar1,lty=2)\n\n### now plot the stable recruitment curve\na &lt;- 2.707\nx2 &lt;- eval(lmap)\nlines(x,x2, lwd=2, col=grey(0.75))\nxstar2 &lt;- 1-(1/a)\nx2p &lt;- diff(x2)\nxp &lt;- diff(x)\n### the equilibrium x is approximately x[700]\nm2 &lt;- x2p[630]/xp[630]\nlines(x[480:780], m2*x[480:780]-m2*xstar2+xstar2,lty=2)\nabline(a=0,b=1)\nlegend(\"topleft\", c(\"a=2.707\", \"a=3.414\"), col=c(grey(0.75), \"black\"), lwd=2)\n\n\n\n\n\n\n\n\nThe slope on the black curve at the fixed point is steeper than \\(-45^{\\circ}\\) so the fixed point for this higher-growth model is unstable, while the slope at the fixed point for the grey curve is shallower than \\(-45^{\\circ}\\) and is therefore stable.\nNaturally, we could have actually calculated the derivatives of the recruitment function, but this hack actually works pretty well. You just need to make sure that your \\(x\\) values are fine-grained enough that the numerical derivative is approximately right.\n\n3.6.1 Why Period Doubling\nMay (1976) shows how plotting the iterated map can help us understand the phenomenon of period-doubling.\n\nlmap3 &lt;- expression(a*(a*(a*x*(1-x))*(1-(a*x*(1-x))))*(1-(a*(a*x*(1-x))*(1-(a*x*(1-x))))))\na &lt;- 3.7\nx3 &lt;- eval(lmap3)\nplot(x,x3, type=\"l\",lwd=2,\n     xaxs=\"i\", yaxs=\"i\",\n     ylim=c(0,1),\n     xlab=expression(X[t]),ylab=expression(X[t+3]))\na &lt;- 3.9\nlines(x,eval(lmap3), lwd=2, col=grey(0.75))\nlines(x,x)\n\n\n\n\n\n\n\n\nThe black curve is only intersected by the line of equality once, indicating that there is a single period-3 cycle for \\(a=3.7\\). However, when we raise the growth rate to \\(a=3.9\\), the hills and valleys become much steeper and six more period-3 cycles appear!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#bifurcation-diagram",
    "href": "drawing.html#bifurcation-diagram",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.7 Bifurcation Diagram",
    "text": "3.7 Bifurcation Diagram\nMay (1976) logistic map. There is a doubling of the period of the time series at \\(a=3\\). At \\(a=3.57\\), cycles of period \\(2^n\\) begin to appear. At \\(a=3.68\\), period-3 cycles appear, and at \\(a=3.83\\), every integer period is present. We can show this using a bifurcation diagram, which shows the different values of \\(X\\) that the population will cycle between for various values of \\(a\\). For values of \\(a&lt;3\\), the equilibrium is stable, so there is only a line.\n\nn &lt;- 1\nR &lt;- seq(2.5,4,length=1000)\nf &lt;- expression(a*x*(1-x))\ndata &lt;- matrix(0,200,1001)\n\nfor(a in R){\n  x &lt;- runif(1) # random initial condition\n  ## first converge to attractor\n  for(i in 1:200){\n    x &lt;- eval(f)\n  } # collect points on attractor\n  for(i in 1:200){\n    x &lt;- eval(f)\n    data[i,n] &lt;- x\n  }\n  n &lt;- n+1\n}\n\ndata &lt;- data[,1:1000]\nplot(R,data[1,], pch=\".\", xlab=\"a\", ylab=\"X\")\nfor(i in 2:200) points(R,data[i,], pch=\".\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#bistable-attractors",
    "href": "drawing.html#bistable-attractors",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.8 Bistable Attractors",
    "text": "3.8 Bistable Attractors\nBistability means that a dynamic system has two distinct stable states.\nEfferson, Vogt, and Fehr (2020) develop a model of cultural change with heterogeneous preferences. They show that the system is characterized by bistability.\n\n## mixture of 2 logistic curves\nf &lt;- function(a1,b1,c1,a2,b2,c2,x,p){\n  return(x-(p*(c1/(1+exp(-(b1*(x-a1)))))+(1-p)*(0.25+c2/(1+exp(-(b2*(x-a2)))))))\n}\n\na1 &lt;- 1/2\nb1 &lt;- 15\nc1 &lt;- 1\na2 &lt;- 1/2\nb2 &lt;- 7\nc2 &lt;- 1/2\np &lt;- seq(0,1,length=1000)\nlibrary(rootSolve)\nuniroot.all(f,a1=a1,b1=b1,c1=c1,a2=a2,b2=b2,c2=c2,p=0.25,interval=c(0,1))\n\n[1] 0.5000000 0.2479318 0.7520682\n\nAA &lt;- matrix(NA,1000,3)\nfor(i in 1:1000){\n  tmp &lt;- uniroot.all(f,a1=a1,b1=b1,c1=c1,a2=a2,b2=b2,c2=c2,p=p[i],interval=c(0,1))\n  ifelse(length(tmp)==1,AA[i,1] &lt;- tmp, \n         ifelse(length(tmp)==2,AA[i,1:2] &lt;- tmp, AA[i,] &lt;- tmp))\n}\n\n## Attractor\nplot(p[-1000],AA[1:999,3],type=\"l\", lwd=3,\n     xlab=\"Proportion Wearing Masks\", \n     ylab=\"Probability of Mask Adoption\",\n     xlim=c(0,1),ylim=c(0,1))\nlines(p[-1000],AA[1:999,2], lwd=3)\nlines(p[-1000],AA[1:999,1], lty=2, lwd=3, col=\"grey\")\nlines(p[1:44],AA[1:44,1], lwd=3)\narrows(x0=c(0.2,0.4,0.6,0.8,1.0), y0=0.55, x1=c(0.2,0.4,0.6,0.8,1.0), y1=0.65, \n       lwd=3, col=\"red\", length=0.1)\narrows(x0=c(0.2,0.4,0.6,0.8,1.0), y0=0.45, x1=c(0.2,0.4,0.6,0.8,1.0), y1=0.35, \n       lwd=3, col=\"red\", length=0.1)\narrows(x0=0, y0=0.55, x1=0, y1=0.65, \n       lwd=3, col=\"red\", code=1, length=0.1)\narrows(x0=0, y0=0.45, x1=0, y1=0.35, \n       lwd=3, col=\"red\", code=1,length=0.1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#polygons",
    "href": "drawing.html#polygons",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.9 Polygons",
    "text": "3.9 Polygons\nTo create a polygon between two curves, \\(y_1\\) and \\(y_2\\), which are both functions of \\(x\\), you need to pass polygon() a concatenated vector of \\(x\\) and the reverse of \\(x\\) for the first vector, and then a concatenated vector of \\(y_2\\) and the reverse of \\(y_1\\) for the second vector.\nHere, fill in the polygon formed between the convex part of a logistic curve and a straight line that connects its endpoints.\n\nx &lt;- seq(10,30,,500)\nux &lt;- function(a,b,x) 1/(1+exp(-(x-a)/b))\na &lt;- 20\nb &lt;- 1.75\n\nx1 &lt;- seq(10,20,,250)\n# straight line\ny1 &lt;- (x1-10)/20\n# convex part of logistic\ny2 &lt;- ux(a=a,b=b,x=x1)\n\n\nplot(x,ux(a=a,b=b,x=x), type=\"l\", lwd=3, axes=FALSE, frame=TRUE, \n     xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Technological Development\")\nsegments(10,ux(a=a,b=b,x=10),30, ux(a=a,b=b,x=30), lwd=2, col=grey(0.85))\npolygon(c(x1, rev(x1)), c(y2, rev(y1)), col=\"plum\")\n\n\n\n\n\n\n\n\nProbably one of the most common uses for polygons is to fill in the tail (or some other part) of a probability density to show how much relative probability is contained in an interval or a tail. Here we compare the tail probability of a standard normal distribution with a low-df \\(t\\) distribution. We will color in the upper tail above the value of 1.96, the approximate 97.5th quantile of the standard normal distribution and the conventional definition of “statistical significance.”\n\n## normal\nz &lt;- seq(-20, 20, length=2000)\np &lt;- dnorm(z)\nplot(z,p, type=\"l\", lwd=2, xlim=c(-4,4), \n     xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nz0 &lt;- z[z &gt;= 1.96]    # define region to fill\nz0 &lt;- c(z0[1], z0)\np0 &lt;- p[z &gt;= 1.96]\np0 &lt;- c(0, p0)\npolygon(z0, p0, col=\"grey\")\n\n\n\n\n\n\n\n# integrate to see how much probability mass is in the tail\nintegrate(dnorm, 1.96, Inf)\n\n0.0249979 with absolute error &lt; 1.9e-05\n\n\nNow for the \\(t\\) distribution\n\n## t, df=1\nq &lt;- dt(z,df=1)\nplot(z,q, type=\"l\", lwd=2, xlab=\"Outcome\", ylab=\"Probability of Outcome\")\nt0 &lt;- z[z &gt;= 1.96]    # define region to fill\nt0 &lt;- c(t0[1], t0)\nq0 &lt;- q[z &gt;= 1.96]\nq0 &lt;- c(dt(20,df=1), q0)\npolygon(t0, q0, col=\"grey\")\n\n\n\n\n\n\n\n## integrate to see how much probability mass is in the tail\nintegrate(dt, 1.96, Inf, df=1)\n\n0.1501714 with absolute error &lt; 1.1e-10\n\n\nOoh, pointy. Note that the \\(t\\) distribution decays so slowly that you can see that the polygon has a slope to it even when you extend the range out to 20. This explains why we added dt(20,df=1) as the first element of the q variable for the polygon.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#s-shaped-curves",
    "href": "drawing.html#s-shaped-curves",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.10 S-Shaped Curves",
    "text": "3.10 S-Shaped Curves\nThere are numerous instances where we want to draw S-shaped curves. For example, we might want to show density-dependent population growth or the fraction of a population who have adopted an innovation.\nThere are a number of ways to draw S-shaped curves. The first is to use a logistic function.\nAnother possibility is to use a cumulative distribution function for a normal random variable.\nHere is an example of an adoption curve (E. M. Rogers 2003).\n\n## stylized adoption curve\nx &lt;- seq(-4,4,,500)\nq &lt;- c(0.025, 0.16, 0.5, 0.84)\nqq &lt;- qnorm(q)\ndd &lt;- dnorm(qq)\nzz &lt;- rep(0,4)\n\nplot(x, dnorm(x), type=\"n\", axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Time\", ylab=\"Fraction Adopting\", ylim=c(0,1.05))\naxis(2)\nsegments(qq,zz,qq,dd, lwd=3, col=rgb(1,0,0,0.5))\nlines(x, dnorm(x), lwd=3, col=grey(0.65))\nlines(x,pnorm(x), lwd=3)\nlegend(\"topleft\",c(\"incident\",\"cumulative\"),lwd=3, col=c(grey(0.65),\"black\"))\n\n\n\n\n\n\n\n\n\n3.10.1 Polygyny Threshold Model\nAnother application of S-shaped curves in behavioral ecology is the Polygyny Threshold Model (Orians 1969).\n\n## Polygyny Threshold\nlogisfun &lt;- function(n0=1,K=100,r=0.05,t,delay=0) n0*exp(r*(t-delay))/((1+n0*(exp(r*(t-delay))-1)/K))\n\nt &lt;- seq(0,250,,500)\ndelay &lt;- 50\nn0 &lt;- 1\nK &lt;- 100\nr &lt;- 0.05\n\ny1 &lt;- logisfun(t=t)\ny2 &lt;- logisfun(t=t, delay=50)\n\nplot(t,y1/100, type=\"l\", lwd=3, col=\"magenta\",\n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     ylab=\"Female Reproductive Success\", xlab=\"Resource-Holding Capacity\")\nlines(t,y2/100, col=\"purple\", lwd=3)\nsegments(150,0,150,0.6,lty=3)\nsegments(100,0,100,0.6, lty=3)\nsegments(150,0.6,0,0.6,lty=3)\nmtext(expression(Delta[RHC]),1, at=125, col=\"red\")\naxis(1,at=c(100,150), labels=FALSE, col=\"red\")\ntext(177,0.6, expression(RS[S]==RS[P]), col=\"red\")\nlegend(\"topleft\",c(\"primary\",\"secondary\"), lty=1, lwd=3, col=c(\"magenta\",\"purple\"))\n\n\n\n\n\n\n\n\nThe fitness of the secondary mate is lower for all resource levels except the very highest. If the quality of an already-mated male’s territory is greater than an unmated male’s territory by \\(\\Delta_{RHC}\\), then the fitness of the secondary female is greater than the fitness of a primary female mated to the male with the lower-quality territory. As a result, we expect a polygynous mating.\nFor a quick-and-dirty solution, it is convenient to just represent some equation of interest as an expression(), define some parameters, and then get the function’s values by using eval(). In most cases, it is going to be better to write a function for your equation. In this case, it allowed us to find values for FRS from the two logistic curves associated with specific values of RHC. While it might be a bit more work to write a function, it will probably save effort in the long run. I often prototype plots using expression() and then write a function once I’ve worked out the general features of the plot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#evolutionary-stable-strategies",
    "href": "drawing.html#evolutionary-stable-strategies",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.11 Evolutionary Stable Strategies",
    "text": "3.11 Evolutionary Stable Strategies\n\n3.11.1 Rogers Paradox\nSelection doesn’t necessarily increase mean fitness (A. R. Rogers 1988). Different Rogers.\n\n## Figure 1 from Rogers (1988)\nwi &lt;- function(b=1,c=0.9) 1 + b*(1-c)\nws &lt;- function(b=1,s=0,p,u=0.8) 1 + (b*(1-s)*(1-p)*(1-u))/(1 - p*(1-u))\n\np &lt;- seq(0,1,,100)\n\ncult &lt;- ws(p=p)\nacult &lt;- rep(wi(),100)\nm &lt;- cult*p + acult*(1-p)\n\nplot(p,cult, type=\"l\", lwd=2, axes=FALSE, frame=TRUE,\n     xaxs=\"i\",\n     xlab=\"Frequency of Social Learning (p)\", ylab=\"Fitness\")\nlines(p,acult, col=\"red\",lwd=2)\nlines(p,m, col=grey(0.75), lty=2,lwd=2)\naxis(1)\nlegend(\"topright\", c(\"individual\",\"social\",\"mean\"), \n       col=c(\"red\",\"black\",grey(0.75)), lty=c(1,1,2))\n\n\n\n\n\n\n\n\n\n\n3.11.2 Producer-Scrounger Game\n\np1 &lt;- seq(0,1,,200)\nwthief &lt;- expression(1.5 - 2*p1)\nwfarmer &lt;- expression(1 - 1.2*p1)\n\nplot(p1, eval(wthief), type = \"l\", col=\"red\", lwd=2,\n     axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     xlab=\"Fraction Scroungers\", ylab = \"Fitness\", \n     xlim=c(0,1), ylim=c(0,2))\nlines(p1, eval(wfarmer), lwd=2, col=\"blue\")\nlines(p1,(p1*eval(wthief)+(1-p1)*eval(wfarmer)), lwd=2, lty=2)\naxis(1,at=c(0,1),labels=c(\"0\",\"1\"))\nabline(v=0.625, col=grey(0.75), lwd=1)\narrows(0.4,1.5,0.62,1.5,  col=grey(0.75),lwd=2)\narrows(0.8,1.5,0.63,1.5,  col=grey(0.75),lwd=2)\nlegend(\"topright\", c(\"Scrounger\",\"Producer\",\"Mean\"), \n       col=c(\"red\",\"blue\",\"black\"), lwd=2, lty=c(1,1,2))\naxis(3,at=0.625,labels=c(\"ESS\"))\n\n\n\n\n\n\n\n\nDraw arrows to show the equilibrium and its stability.\nGraphics Tip: Note how we marked the location of the ESS on the top of the plot using axis(3)!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#prey-choice-model",
    "href": "drawing.html#prey-choice-model",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.12 Prey-Choice Model",
    "text": "3.12 Prey-Choice Model\n\nx &lt;- seq(0,20,length=500)\nf &lt;- function(x,b) {\n  1 - exp(-b*(x-1))\n}\n\nf1 &lt;- function(x,a) a/x\n\nplot(x,f(x,b=0.1), type=\"l\", lwd=3, xaxs=\"i\", yaxs=\"i\",\n     axes=FALSE,\n     xlab=\"Item Rank\",\n     ylab=\"Energy Gain\",\n     ylim=c(0,1))\naxis(1)\nbox()\nlines(x,f(x,b=0.05), lwd=3, col=\"red\")\nlines(x,f1(x,a=2), lwd=3, col=\"grey\")\n# found approximate points using locator()\ntext(2.9,0.9648077,expression(E/h))\ntext(18.75,0.8848253,expression(E^g/t))\ntext(18.75,0.6362313,expression(E^b/t))\nsegments(5.5,0,7.4,0, lwd=10, col=\"green\", lend=2) \ntext(6.45,0.04825, \"Fallback Foods\")\n\n\n\n\n\n\n\n\nIn a bad year, diet breadth expands. The extent of this expansion is indicated by the green bar.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#linear-programming",
    "href": "drawing.html#linear-programming",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.13 Linear Programming",
    "text": "3.13 Linear Programming\nFrom Belovsky (1987). Optimal diet for hunter-gatherers based on constraints on feeding time, digestive capacity, energy requirements, and protein needs. Uses linear programming to find the optimal solution. Belovsky shows that, contrary to much thinking about foraging, that hunter-gatherers tend to maximize either total food intake or protein and do not time-minimize.\n\n#### constraint functions\n\n## feeding time constraint\n## 393 &gt;= 0.34*H + 0.42*G\n\ntime.slope &lt;- -0.42/0.34\ntime.int &lt;- 393/0.34\n\n## digestive constraint\n## 700 &gt;= H + 0.67*G\n\ndig.slope &lt;- -0.67\ndig.int &lt;- 700\n\n## engergy constraint\n## 1975 =&lt; 3*H + 3.05*G (or 3.22*G)\n\nenergy.slope &lt;- -3.05/3\nenergy.int &lt;- 1975/3\n\n## protein constraint\n## 60 =&lt; 0.15*H + 0.12*G\n\nprotein.slope &lt;- -0.12/0.15\nprotein.int &lt;- 60/0.15\n\n\n#  polygon top is digestion until it meets time; bottom is energy\nxx &lt;- 0:1000\nddy &lt;- 700 - 0.67*xx\ntty &lt;- 393/.34 - (0.42/0.34)*xx\neey &lt;- 1975/3 - (3.05/3)*xx\n\nmin(which(tty&lt;0))\n\n[1] 937\n\n# 937\ngg &lt;- c(1:937, 937:1)\n\nmin(which(ddy&gt;=tty))\n\n[1] 808\n\n# 808\nhh &lt;- c(ddy[1:808],tty[809:937],eey[937:1])\n\n\nplot(1:1200,1:1200, type=\"n\", yaxs=\"i\", xaxs=\"i\",\n     xlab=\"Gathered Food (g)\", ylab=\"Hunted Food (g)\")\nabline(a=time.int, b=time.slope)\nabline(a=dig.int, b=dig.slope)\nabline(a=energy.int, b=energy.slope)\nabline(a=protein.int, b=protein.slope)\npolygon(gg,hh,col=grey(0.85), border=\"black\")\npoints(807,ddy[808], pch=21, cex=1.5)\nabline(h=0,lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#extreme-value-distribution-for-innovation",
    "href": "drawing.html#extreme-value-distribution-for-innovation",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.14 Extreme-Value Distribution for Innovation",
    "text": "3.14 Extreme-Value Distribution for Innovation\nInnovation happens when the skill of some learner exceeds the current highest level of skill. The distribution for extreme values like this is know, oddly enough, as an extreme value distribution. There are a number of specific flavors of such distributions.\nHenrich (2004) suggests that the specific EVD for the highest skill is a Gumbel distribution (a type of extreme-value distribution). The level of skill that improves culture is \\(z_h\\). The probability that there will be models with skill level \\(z_h\\) or better is small in smaller populations because this value is out in the tail of the distribution.\nTwo parameters: \\(\\alpha\\) (difficulty) and \\(z\\) (skillfulness), which are assumed independent.\n\n# extreme-value distribution package\nlibrary(evd)\nx &lt;- seq(0,100,,500)\n\nplot(x,dgumbel(x,30,10), type=\"l\", lwd=3, axes=FALSE, frame=TRUE,\n     xlab=expression(paste(\"Learner's Skill, \", z[i])), \n     ylab=expression(paste(\"Probability of Acquiring \", z[i])))\nlines(x,dgumbel(x,30,15), col=\"magenta\", lwd=3)\nabline(v=60,lty=2)\nmtext(expression(z[h]),1, at=60, padj=1)\n\n\n\n\n\n\n\n\nNote that in the more-innovative population, far more individuals exceed the critical threshold \\(z_h\\), but also way more people are clearly bad at the skill.\nThis result flows precisely from the assumption that learner’s skill follows a Gumbel distribution (i.e., Henrich 2004). It is perfectly fair to ask if this is a reasonable model, but we do know that innovation cultures make a lot of mistakes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#the-right-ibis",
    "href": "drawing.html#the-right-ibis",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.15 The Right Ibis",
    "text": "3.15 The Right Ibis\nLeslie and Winterhalder (2002) describe a bespoke utility function they call the right ibis model. We can code that and then use it to illustrate Tainter’s model of social collapse.\n\nribis &lt;- function(x,m,n,r){\n  if(x&lt;0 | x&gt;=r){\n    ibis &lt;- 0\n  }\n  if(x&gt;=0 & x&lt;m) {\n    ibis &lt;- exp(m^2/((m-n)^2) - (m*x/(m-n)^2)) * x^(m^2/((m-n)^2)) *\n      m^(-m^2/(m-n)^2)\n  }\n  if(x&gt;=m & x&lt;r){\n    ibis &lt;-  1-(x-m)^2/(m-r)^2\n  }\n  return(ibis)\n}\n\nm &lt;- 8\nn &lt;- 2\nr &lt;- 18\nx &lt;- 0:16\n### Use Right Ibis for Tainter plot\n## plot a nice smooth curve\ny &lt;- seq(0,16,length=100)\naaa &lt;- rep(0,100)\nfor(i in 1:100) aaa[i] &lt;- ribis(x=y[i],m=m,n=n,r=r)\n\nmc &lt;- ribis(x=8,m=m,n=n,r=r)\ndb &lt;- ribis(x=15,m=m,n=n,r=r)\ncl &lt;- ribis(x=2.895,m=m,n=n,r=r)\n\nplot(y,aaa,type=\"l\", lwd=2, axes=FALSE, frame=TRUE,\n     xaxs=\"i\", yaxs=\"i\",\n     ylim=c(0,1.1),\n     xlab=\"Complexity\", ylab=\"Benefits to Complexity\")\nsegments(8,mc,0,mc,lty=3)\nsegments(8,0,8,mc,lty=3)\n\nsegments(15,db,0,db,lty=3, col=\"red\")\nsegments(15,0,15,db,lty=3, col=\"red\")\n#segments(2.895,cl,0,cl,lty=3, col=\"red\")\nsegments(2.895,0,2.895,cl,lty=3, col=\"red\")\n#mtext(expression(paste(B, \"*\")),2, at=mc, adj=1, las=2)\nmtext(expression(B[max]),2, at=mc, adj=1, las=2)\nmtext(expression(B[mid]),2, at=db, adj=1, las=2)\nmtext(expression(C[lo]),1, at=2.895, padj=1)\nmtext(expression(C[opt]),1, at=8, padj=1)\nmtext(expression(C[hi]),1, at=15, padj=1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#autocorrelated-time-series",
    "href": "drawing.html#autocorrelated-time-series",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.16 Autocorrelated Time Series",
    "text": "3.16 Autocorrelated Time Series\nSometimes you want to plot a time series for illustrative purposes. More interesting time series, like most actual biophysical series, are likely to show some positive autocorrelation. The AR(1) model is simple model that can produce interesting plots. It is the simplest of the autoregressive models, where the state in the present depends only on the state in the previous time step. A\n\\[\nX_{t+1} = \\varphi X_t + \\varepsilon_t\n\\]\nThe parameter \\(\\varphi\\) is the autocorrelation coefficient and \\(\\varepsilon_t\\) is the shock or “innovation” at time \\(t\\), which is taken to be \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\).\nWe might use this model to represent volatile income of an individual hunter:\n\nac1fun &lt;- function(xo,alpha,sigma,tmax){\n  x &lt;- rep(0,tmax+1)\n  x[1] &lt;- xo\n  rr &lt;- rnorm(tmax,0,sigma)\n  for(i in 2:(tmax+1)) x[i] &lt;- alpha*x[i-1] + rr[i-1]\n  return(x)\n}\n\n## xo = initial size\n## alpha = autocorrelation\n## sigma = sd of Gaussian noise\n## tmax = max number of time steps\nxo &lt;- 1\nalpha &lt;- 0.5\nsigma &lt;- 1\ntmax &lt;- 100\n\nx1 &lt;- ac1fun(xo=xo,alpha=alpha,sigma=sigma,tmax=tmax)\n\nplot(0:tmax+1, x1, type=\"l\", lwd=2, axes=FALSE, xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Income\")\nabline(h=mean(x1), col=\"red\")\naxis(1,labels=FALSE)\naxis(2,labels=FALSE)\nbox()\n\n\n\n\n\n\n\n## Calculate frequency spectrum\n\nspect &lt;- spectrum(x1, log=\"no\", spans=c(2,2), plot=FALSE)\nspecx &lt;- spect$freq\nspecy &lt;- 2*spect$spec\nttext &lt;- \"Spectrum for Positive Autocorrelation,\"\nplot(specx, specy, type=\"l\", lwd=2,\n     xlab=\"Frequency (1/day)\", ylab=\"Spectral Density\")\ntitle(bquote(.(ttext) ~ alpha==0.5))\n\n\n\n\n\n\n\n\nspectrum calculates the frequency axis in terms of cycles per sampling interval; it makes more sense to convert to cycles per unit time (so divide by the sampling interval). The spectrum is generally far more interpretable if it is smoothed. To do this, we use the argument spans, which specifies the parameter(s) for the what is known as the modified Daniell kernel for smoothing the spectrum. The modified Daniell kernel is essentially just a running average (see code below for a sense of what these parameters do). There is no hard-and-fast rule for how to do this (try a couple different values), but the higher the number of spans, the more smoothing and the lower the peaks of the spectrum will be.\nThe default for spectrum is to calculate the spectrum on a log-scale. Use the argument log=\"no\" to change this default. Note also that the spectrum needs to be multiplied by 2 to make it actually equal to variance!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#how-wavelets-work",
    "href": "drawing.html#how-wavelets-work",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.17 How Wavelets Work",
    "text": "3.17 How Wavelets Work\nWavelets are like spectral analysis, but they work at multiple scales. Suppose we have a series \\(x(t)\\). The wavelet transform involves multiplying the series by the wavelet which has been stretched to various scales spanning the series and summing this product. The scale is determined by the parameter \\(\\tau\\). Note how conceptually similar this is to calculating the covariance of two variables.\n\\[\nW_x(a,\\tau) = \\frac{1}{\\sqrt{a}} \\int_{-\\infty}^{\\infty} x(t)\\psi^* \\left(\\frac{t-\\tau}{a}\\right) dt = \\int_{-\\infty}^{\\infty} x(t)\\psi^*_{a,\\tau}(t) dt\n\\]\n\nlibrary(biwavelet)\nmorlet &lt;- function(x) exp(-x^2/2) * cos(5*x)\nx &lt;- seq(-4,4,length=1000)\ny &lt;- morlet(x)\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\n\n\n\n\n\n\n\n\nNow generate a periodic series and overlay on the mother wavelet.\n\nf &lt;- expression(cos(2*pi*x)*exp(-pi*x^2))\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\nlines(x,eval(f), lwd=2)\n\n\n\n\n\n\n\n\nWe can see that the wavelet captures this periodic variation pretty well. The wavelet transform of the signal (red) shows that most of the function lies above the \\(x=0\\) line and its sum is strongly positive.\n\ndx &lt;- diff(x)\ndx &lt;- c(dx,dx[999])\n\nplot(x, y*eval(f)*dx, type=\"l\", lwd=3, col=\"red\", xlab=\"x\",\n     ylab=expression(paste(\"integrand, \", psi[a,tau](t), x(t))))\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\nWhat happens when the signal is not well matched by the wavelet? In the next figure, the signal in black is largely random with respect to the mother wavelet. When we plot the wavelet transform of this signal (red), there is approximately an equal amount of area above and below the the \\(x=0\\) line and the sum is effectively zero.\n\n# another function\nf1 &lt;- expression(cos(12*pi*x)*exp(-pi*x^2))\nplot(x,y,type=\"l\",  lwd=3, col=\"purple4\",\n     ylim=c(-1.1,1.1),\n     xlab=\"\",ylab=expression(psi[a,tau](t)))\nabline(h=0, lwd=0.5, lty=3)\nlines(x,eval(f1), lwd=2)\n\n\n\n\n\n\n\nplot(x, y*eval(f1)*dx, type=\"l\", lwd=3, col=\"red\", xlab=\"x\",\n     ylab=expression(paste(\"integrand, \", psi[a,tau](t), x(t))))\nabline(h=0, lty=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "drawing.html#to-do",
    "href": "drawing.html#to-do",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "3.18 To Do",
    "text": "3.18 To Do\n\nSplines\nFitness surface/contour plot\nKinship/genealogy\nPhylogenetic trees\nDAGs/causal diagrams\nLexis diagram\nShow how cobwebbed Ricker recruitment translates into chaotic time series\n\n\n\n\n\nBaalen, M. van, and M. W. Sabelis. 1995. “The Dynamics of Multiple Infection and the Evolution of Virulence.” American Naturalist 146 (6): 881–910. http://www.jstor.org/stable/2463102.\n\n\nBelovsky, G. E. 1987. “Hunter-Gatherer Foraging: A Linear Programming Approach.” Journal of Anthropological Archaeology 6 (1): 29–76. https://doi.org/10.1016/0278-4165(87)90016-X.\n\n\nCharnov, Eric L. 1976. “Optimal Foraging, the Marginal Value Theorem.” Theoretical Population Biology 9 (2): 129–36. https://doi.org/10.1016/0040-5809(76)90040-X.\n\n\n———. 1997. “Trade-Off-Invariant Rules for Evolutionary Stable Life Histories.” Nature 387 (6631): 393–94. https://doi.org/10.1038/387393a0.\n\n\nEfferson, Charles, Sonja Vogt, and Ernst Fehr. 2020. “The Promise and the Peril of Using Social Influence to Reverse Harmful Traditions.” Nature Human Behaviour 4 (1): 55–68. https://doi.org/10.1038/s41562-019-0768-2.\n\n\nHenrich, Joseph. 2004. “Demography and Cultural Evolution: How Adaptive Cultural Processes Can Produce Maladaptive Losses: The Tasmanian Case.” American Antiquity 69 (2): 197–214. https://doi.org/10.2307/4128416.\n\n\nLeslie, P., and B. Winterhalder. 2002. “Demographic Consequences of Unpredictability in Fertility Outcomes.” American Journal of Human Biology 14 (2): 168–83. https://doi.org/10.1002/ajhb.10044.\n\n\nLevins, R. 1968. Evolution in Changing Environments. Princeton: Princeton University Press.\n\n\nLevins, Richard. 1962. “Theory of Fitness in a Heterogeneous Environment. I. The Fitness Set and Adaptive Function.” The American Naturalist 96 (891): 361–73. http://www.jstor.org/stable/2458725.\n\n\nMay, R. M. 1976. “Simple Mathematical-Models with Very Complicated Dynamics.” Nature 261 (5560): 459–67. https://doi.org/10.1038/261459a0.\n\n\nNoy-Meir, Imanuel. 1975. “Stability of Grazing Systems: An Application of Predator-Prey Graphs.” Journal of Ecology 63 (2): 459–81. https://doi.org/10.2307/2258730.\n\n\nOrians, Gordon H. 1969. “On the Evolution of Mating Systems in Birds and Mammals.” The American Naturalist 103 (934): 589–603. https://doi.org/10.1086/282628.\n\n\nParker, G. A., and R. A. Stuart. 1976. “Animal Behavior as a Strategy Optimizer: Evolution of Resource Assessment Strategies and Optimal Emigration Thresholds.” American Naturalist 110 (976): 1055–76. https://doi.org/10.1086/283126.\n\n\nRogers, A. R. 1988. “Does Biology Constrain Culture?” American Anthropologist 90 (4): 819–31. https://doi.org/10.1525/aa.1988.90.4.02a00030.\n\n\nRogers, E. M. 2003. Diffusion of Innovations. 5th ed. New York: Free Press.\n\n\nScheffer, M. 2009. Critical Transitions in Nature and Society. Princeton: Princeton University Press.\n\n\nSmith, C. C., and S. D. Fretwell. 1974. “The Optimal Balance Between Size and Number of Offspring.” American Naturalist 108: 499–506. https://www.jstor.org/stable/2459681.\n\n\nZeeman, E. C. 1976. “Catastrophe Theory.” Scientific American 234 (4): 65–65 &. https://doi.org/10.1038/scientificamerican0476-65.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "graphs.html#some-definitions",
    "href": "graphs.html#some-definitions",
    "title": "3  Actual Graphs",
    "section": "3.1 Some Definitions",
    "text": "3.1 Some Definitions\nA graph is simply a collection of vertices (or nodes) and edges (or ties). We can denote this \\(\\mathcal{G}(V,E)\\), where \\(V\\) is a the vertex set and \\(E\\) is the edge set. The vertices of the graph represent the actors in the social system. These are usually individual people, but they could be households, geographical localities, institutions, or other social entities. The edges of the graph represent the relations between these entities (e.g., “is friends with” or “has sexual intercourse with” or “sends money to”). These edges can be directed (e.g., “sends money to”) or undirected (e.g., “within 2 meters of”).\nWhen the relations that define the graph are directional, we have a directed graph or digraph.\nGraphs (and digraphs) can be binary (i.e., presence/absence of a relationship) or valued (e.g., “groomed five times in the observation period”, “sent $100”).\nA graph (with no self-loops) with \\(n\\) vertices has \\({n \\choose 2} = n(n-1)/2\\) possible unordered pairs. This number (which can get very big!) is important for defining the density of a graph, i.e., the fraction of all possible relations that actually exist in a network.\nA bipartite graph is a graph where all the nodes of a graph can be partitioned into two sets \\(\\mathcal{V}_1\\) and \\(\\mathcal{V}_2\\) such that for all edges in the graph connects and unordered pair where one vertex comes from \\(\\mathcal{V}_1\\) and the other from \\(\\mathcal{V}_2\\). Often called an “affiliation graph” as bipartite graphs are used to represent people’s affiliations to organizations or events."
  },
  {
    "objectID": "graphs.html#various-ways-to-specify-graphs-in-igraph",
    "href": "graphs.html#various-ways-to-specify-graphs-in-igraph",
    "title": "4  Actual Graphs",
    "section": "4.2 Various Ways to Specify Graphs in igraph",
    "text": "4.2 Various Ways to Specify Graphs in igraph\nThe R package igraph provides tools for the analysis and visualization of networks. The package is actually just a set of R bindings for functions written in C++ that can be used in a variety of environments (e.g., native, R, Python).\n\n4.2.1 Specifying Small Graphs\nWe can create a small, undirected graph of five vertices from a vector of vertex pairs using the function make_graph():\n\nrequire(igraph)\ng &lt;- make_graph( c(1,2, 1,3, 2,3, 2,4, 3,5, 4,5), n=5, dir=FALSE )\nplot(g, vertex.color=\"skyblue2\")\n\n\n\n\n\n\n\n\nThe call to the function make_graph() (which can be shortened to graph()) takes three arguments in this case. First, we enumerate the edges by listing the pairs of vertices which are connected. In this graph, there are six edges. Second, we define the size of our graph. This simple graph has five vertices, so n=5. Third, the default graph type is directed, so to create an undirected graph, we need to specify dir=FALSE. The function graph() creates a graph object which, like any R object, is associated with a number of methods. When we plot a graph object, the plotting method used is plot.igraph(). There are a number of features (or perhaps peculiarities) of the defaults of plot.igraph(). First, is the vertex color. It’s not hideous but it’s not an obvious choice for a default color either. Second, the default font label style is Roman, which can make the labels look cluttered. I typically change to a sens-serif font using the argument vertex.label.family=\"Helvetica\". Third, the layout will not necessarily make sense to you as a human viewer of the graph and will typically change each time you call plot.igraph(). Fortunately, igraph has a number of excellent tools for assisting with graph layout.\nFor small graphs representing the relationships between a few named individuals, we can create a graph using graph_from_literal(). Undirected edges are indicated with one or more dashes -, --, etc. It doesn’t matter how many dashes you use – you can use as many as you want to make your code more readable. The colon operator : links “vertex sets” – i.e., creates ties between all members of two groups of vertices. So, for the Scooby Gang, we could specify the following graph\n\ng &lt;- graph_from_literal(Fred:Daphne:Velma:Shaggy-Fred:Daphne:Velma:Shaggy, Shaggy-Scooby)\nplot(g, vertex.shape=\"none\", vertex.label.color=\"black\")\n\n\n\n\n\n\n\n\nFor directed edges, use -+ where the plus indicates the direction of the arrow, i.e., A --+ B creates a directed edge from A to B. A mutual edge can be created using +-+.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#a-note-on-visualizing-graphs",
    "href": "graphs.html#a-note-on-visualizing-graphs",
    "title": "4  Actual Graphs",
    "section": "4.3 A Note On Visualizing Graphs",
    "text": "4.3 A Note On Visualizing Graphs\nYou will notice that many of the graphs in these notes are a bit cramped. This happens because when I render the Quartz document, R generates fairly small .png files. If you have, for example, vertex labels that really need to be read, it is a good idea to send your plot to a file that uses a vector-based format and potentially make it big. My preference is .pdf, but an argument can be made that .svg is even better. To do this, you just need to wrap your plotting commands in call to .pdf: pdf(file=\"filename.pdf\", height14, width=14) and then don’t forget to close this off (i.e., after all your plotting commands) with dev.off() or you’ll keep sending graphics to your pdf file! The default size for pdf is \\(7 \\times 7\\) (in inches). By specifying the optional arguments height and width, we’ve doubled the size of the plot. This will spread things out quite a bit and you may actually have to increase the size of your vertices, labels, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#special-graphs",
    "href": "graphs.html#special-graphs",
    "title": "4  Actual Graphs",
    "section": "4.4 Special Graphs",
    "text": "4.4 Special Graphs\nA wide variety of special graphs are built into igraph. Note: I really don’t like the current default color in igraph (a kind of sickly orange), so I set the vertex color for every plot – you don’t have to do that\n\n4.4.1 Empty, Full, Ring\n\n# empty graph\ng0 &lt;- make_empty_graph(20)\nplot(g0, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# full graph\ng1 &lt;- make_full_graph(20)\nplot(g1, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# ring\ng2 &lt;- make_ring(20)\nplot(g2, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.4.2 Lattice, Tree, Star\n\n# lattice\ng3 &lt;- make_lattice(dimvector=c(10,10))\nplot(g3, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# tree\ng4 &lt;- make_tree(20, children = 3, mode = \"undirected\")\nplot(g4, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n# star\ng5 &lt;- make_star(20, mode=\"undirected\")\nplot(g5, vertex.color=\"skyblue2\", vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.4.3 Erdos-Renyi & Power-Law\n\n# Erdos-Renyi Random Graph\ng6 &lt;- sample_gnm(n=100,m=50)\nplot(g6, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n# Power Law\ng7 &lt;- sample_pa(n=100, power=1.5, m=1,  directed=FALSE)\nplot(g7, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#working-with-graphs",
    "href": "graphs.html#working-with-graphs",
    "title": "4  Actual Graphs",
    "section": "4.5 Working with Graphs",
    "text": "4.5 Working with Graphs\n\n4.5.1 Putting Graphs Together\nSometimes you want to plot two (or more) graphs together. The disjoint union operator allows you to merge two graphs with different vertex sets:\n\nplot(g4 %du% g7, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.5.2 Rewiring and Connected Components\nWe often want to shuffle the edges of our graph around. A common application of this functionality is when we want to randomize the edges of a graph while maintaining the same vertex set and overall number of edges.\nWhen you rewire a graph, there is a chance you will create isolates (i.e., vertices with no incident edges). For visualization purposes, you often want to remove these. You frequently will want to extract the largest connected subcomponent of your graph.\nA subgraph is a graph \\(\\mathcal{G}^{\\prime}\\) where all the vertices and edges are also in graph \\(\\mathcal{G}\\). Subgraphs can be generated by selecting either vertices or the edges from \\(\\mathcal{G}\\). A component is a maximally connected subgraph of a graph (i.e., a path exists between all vertices in the subgraph). The igraph function subcomponent() will find all the subcomponents of your graph and order them in terms of their size. The largest subcomponent will be first, so you will often want to subset your graph (g) using the criterion subcomponent(g,1).\n\ngg &lt;- g4 %du% g7\ngg &lt;- rewire(gg, each_edge(prob = 0.3))\nplot(gg, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n## retain only the connected component\ngg &lt;- induced_subgraph(gg, subcomponent(gg,1))\nplot(gg, vertex.color=\"skyblue2\", vertex.size=5, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.5.3 Vertex and Edge Attributes\nYou can add arbitrary attributes to both vertices and edges. Generally, you do this to store information for plotting: colors, edge weights, names, etc. Some attributes are automatically created when you construct an graph object (e.g., “name” or “weight” if you load a weighted adjacency matrix)\n\nV(g) accesses vertex attributes\nE(g) accesses edge attributes\n\n\n## look at the structure\ng4\n\nIGRAPH 11e3c3c U--- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c)\n+ edges from 11e3c3c:\n [1] 1-- 2 1-- 3 1-- 4 2-- 5 2-- 6 2-- 7 3-- 8 3-- 9 3--10 4--11 4--12 4--13\n[13] 5--14 5--15 5--16 6--17 6--18 6--19 7--20\n\nV(g4)$name &lt;- LETTERS[1:20]\n## see how it's changed\ng4\n\nIGRAPH 11e3c3c UN-- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c), name (v/c)\n+ edges from 11e3c3c (vertex names):\n [1] A--B A--C A--D B--E B--F B--G C--H C--I C--J D--K D--L D--M E--N E--O E--P\n[16] F--Q F--R F--S G--T\n\n## see what I did there?\n## hexadecimal color codes\nV(g4)$vertex.color &lt;- \"#4503fc\"\nE(g4)$edge.color &lt;- \"#abed8e\"\ng4\n\nIGRAPH 11e3c3c UN-- 20 19 -- Tree\n+ attr: name (g/c), children (g/n), mode (g/c), name (v/c),\n| vertex.color (v/c), edge.color (e/c)\n+ edges from 11e3c3c (vertex names):\n [1] A--B A--C A--D B--E B--F B--G C--H C--I C--J D--K D--L D--M E--N E--O E--P\n[16] F--Q F--R F--S G--T\n\nplot(g4, vertex.size=15, vertex.label=NA, vertex.color=V(g4)$vertex.color, \n     vertex.frame.color=V(g4)$vertex.color,\n     edge.color=E(g4)$edge.color, edge.width=3)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#data-formats",
    "href": "graphs.html#data-formats",
    "title": "4  Actual Graphs",
    "section": "4.6 Data Formats",
    "text": "4.6 Data Formats\n\n4.6.1 Adjacency Matrices\nWe can represent the relationships of a social network using a matrix. A matrix is simply a rectangular array of numbers with (n) rows and \\(k\\) columns. It is conventional to denote matrices mathematically using capital letters and boldface, such as \\(\\mathbf{A}\\). We indicate the \\(ij\\)th element (i.e., the element corresponding to row \\(i\\) and column \\(j\\)) of \\(\\mathbf{A}\\) as \\(a_{ij}\\). A sociomatrix or adjacency matrix is a square matrix (i.e., \\(n \\times n\\), where \\(n\\) is the number of vertices in the network). It is typically binary, with \\(a_{ij}=1\\) if individuals \\(i\\) and \\(j\\) share an edge and \\(a_{ij}=0\\) otherwise. Consider a triangle:\n\n# generate a triangle\ng &lt;- graph( c(1,2, 2,3, 1,3), n=3, dir=FALSE)\n\nWarning: `graph()` was deprecated in igraph 2.1.0.\nℹ Please use `make_graph()` instead.\n\n### coordinatess to make the triangle look nice\ntri.coords &lt;- matrix( c(228,416, 436,0, 20,0), nr=3, nc=2, byrow=TRUE)\npar(mfrow=c(1,2))\nplot(g, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\nplot(g, layout=tri.coords, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\nThe sociomatrix corresponding to our triangle is\n\\[\\begin{equation}\n\\mathbf{A} = \\left[ \\begin{array}{cccc}\n0   &  1   & 1 \\\\\n1   &  0   & 1 \\\\\n1   &  1   & 0  \\end{array} \\right].\n\\end{equation}\\]\nBy convention, the diagonal elements of a sociomatrix are all zero (i.e., self-loops are not allowed). Sociomatrix \\(\\mathbf{A}\\) in the equation above is symmetric (\\(a_{ij} = a_{ji}\\)) because the graph is undirected. For a digraph, the upper triangle (i.e., matrix elements above the diagonal) of the sociomatrix will generally be different than the lower triangle.\nMost primatologists/behavioral ecologists probably have experience thinking in terms of adjacency matrices. An example of an adjacency matrix is the pairwise interaction matrices (e.g., agonistic or affiliative interactions) that we construct from behavioral observations.\nA very important potential gotcha: when you read data into R, it will be in the form of a data frame. Converting an adjacency matrix to an igraph graph object requires the data to be in the matrix class. Therefore, you need to coerce the data you read in by wrapping your read.table() in an as.matrix() command.\n\nkids &lt;- as.matrix(\n  read.table(\"data/strayer_strayer1976-fig2.txt\",\n                             header=FALSE)\n  )\nkid.names &lt;- c(\"Ro\",\"Ss\",\"Br\",\"If\",\"Td\",\"Sd\",\"Pe\",\"Ir\",\"Cs\",\"Ka\",\n                \"Ch\",\"Ty\",\"Gl\",\"Sa\", \"Me\",\"Ju\",\"Sh\")\ncolnames(kids) &lt;- kid.names\nrownames(kids) &lt;- kid.names\ng &lt;- graph_from_adjacency_matrix(kids, mode=\"directed\", weighted=TRUE)\nlay &lt;- layout_with_fr(g)\nplot(g, layout=lay, edge.arrow.size=0.5,\n     vertex.color=\"skyblue2\", vertex.label.family=\"Helvetica\", \n     vertex.frame.color=\"skyblue2\")\n\n\n\n\n\n\n\n\nNote that you might want to change some of the graphics parameters depending on the type of display you use. For this document, the figures are constrained to be small, so you don’t want edges – and particularly arrows – to be too thick.\n\n\n4.6.2 Edge Lists\nAdjacency matrices are actually very inefficient. The cost of an adjacency matrix increases as \\(k^2\\). However, most sociomatrices are quite sparse, meaning that most entries in a sociomatrix are zero. We can capitalize on this by using a sparse-matrix representation. In social network analysis, this representation is called an edge list and it is much more efficient than storing relational data in matrix format.\nAn edgelist is simply a two-column matrix in which each row represents a (possibly directed) edge between the vertex listed in first column and the second column.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#community-structure",
    "href": "graphs.html#community-structure",
    "title": "4  Actual Graphs",
    "section": "4.7 Community Structure",
    "text": "4.7 Community Structure\nVarious algorithms for detecting clusters of similar vertices – i.e., “communities.” Use fastgreedy.community() to identify clusters in Kapferer’s tailor shop and color the vertices based on their membership.\n\nA &lt;- as.matrix(\n  read.table(file=\"data/kapferer-tailorshop1.txt\", \n             header=TRUE, row.names=1)\n  )\nG &lt;- graph.adjacency(A, mode=\"undirected\", diag=FALSE)\n\nWarning: `graph.adjacency()` was deprecated in igraph 2.0.0.\nℹ Please use `graph_from_adjacency_matrix()` instead.\n\nfg &lt;- fastgreedy.community(G)\n\nWarning: `fastgreedy.community()` was deprecated in igraph 2.0.0.\nℹ Please use `cluster_fast_greedy()` instead.\n\ncols &lt;- c(\"blue\",\"red\",\"black\",\"magenta\")\nplot(G, vertex.shape=\"none\",\n     vertex.label.cex=0.75, edge.color=grey(0.85), \n     edge.width=1, vertex.label.color=cols[fg$membership],\n     vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n# another approach to visualizing\nplot(fg,G,vertex.label=NA)\n\n\n\n\n\n\n\n\nfastgreedy.community() identified four clusters. These clusters are listed as numbers in fg$membership. We can then use this vector to index vertex colors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#graph-layouts",
    "href": "graphs.html#graph-layouts",
    "title": "4  Actual Graphs",
    "section": "4.8 Graph Layouts",
    "text": "4.8 Graph Layouts\n\n4.8.1 Force-Based Layouts\nThe two most common layouts are Fruchterman-Reingold and Kawai-Kamada.\nSometimes you don’t want a force-based layout. You may have noticed that the lattice we plotted when we introduced make_lattice() was a bit funky. This is because for a force-based layout, vertices on the periphery will have very different forces working on them than those in the center.\n\nTo get a proper lattice layout, specify that you want it on a grid\n\n\nplot(g3, vertex.color=\"skyblue2\", \n     layout=layout_on_grid(g3,10,10), vertex.size=10, vertex.label=NA)\n\n\n\n\n\n\n\n\n\n\n4.8.2 Making Graph Layouts “By Hand”\nThe layout is of any given plot is random (e.g., plot the same graph repeatedly and you’ll see that the layout changes with each plot). igraph provides a tool for tinkering with the layout called tkplot(). Call tkplot() and it will open an X11 window (on Macs at least). Select and drag the vertices into the layout you want, then use tkplot.getcoords(gid) to get the coordinates (where gid is the graph id returned when calling tkplot() on your graph).\n\n\n\n\ntkplot() window of triangle graph\n\n\n\n\ng &lt;- graph( c(1,2, 2,3, 1,3), n=3, dir=FALSE)\nplot(g, \n     vertex.color=\"skyblue2\", \n     vertex.frame.color=\"skyblue2\", vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n#tkplot(g)\n#tkplot.getcoords(1)\n### do some stuff with tkplot() and get coords which we call tri.coords\n## tkplot(g)\n## tkplot.getcoords(1) ## the plot id may be different depending on how many times you've called tkplot()\n##     [,1] [,2]\n##[1,]  228  416\n##[2,]  436    0\n##[3,]   20    0\ntri.coords &lt;- matrix( c(228,416, 436,0, 20,0), nr=3, nc=2, byrow=TRUE)\npar(mfrow=c(1,2))\nplot(g, vertex.color=\"skyblue2\",\n     vertex.frame.color=\"skyblue2\", \n     vertex.label.family=\"Helvetica\")\nplot(g, layout=tri.coords, \n     vertex.color=\"skyblue2\", \n     vertex.frame.color=\"skyblue2\", vertex.label.family=\"Helvetica\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "graphs.html#plotting-affiliation-graphs",
    "href": "graphs.html#plotting-affiliation-graphs",
    "title": "4  Actual Graphs",
    "section": "4.9 Plotting Affiliation Graphs",
    "text": "4.9 Plotting Affiliation Graphs\n\ndavismat &lt;- as.matrix(\n  read.table(file=\"data/davismat.txt\", \n            row.names=1, header=TRUE)\n  )\nsouthern &lt;- graph_from_incidence_matrix(davismat) \n\nWarning: `graph_from_incidence_matrix()` was deprecated in igraph 1.6.0.\nℹ Please use `graph_from_biadjacency_matrix()` instead.\n\nV(southern)$shape &lt;- c(rep(\"circle\",18), rep(\"square\",14))\nV(southern)$color &lt;- c(rep(\"blue\",18), rep(\"red\", 14))\nplot(southern, layout=layout.bipartite)\n\n\n\n\n\n\n\n## not so beautiful\n## did some tinkering using tkplot()...\nx &lt;- c(rep(23,18), rep(433,14))\ny &lt;- c(44.32432,   0.00000, 132.97297,  77.56757,  22.16216, 110.81081, 155.13514,\n       199.45946, 177.29730, 243.78378, 332.43243, 410.00000, 387.83784, 354.59459,\n       310.27027, 221.62162, 265.94595, 288.10811,   0.00000,  22.16216,  44.32432,\n       66.48649,  88.64865, 132.97297, 166.21622, 199.45946, 277.02703, 365.67568,\n       310.27027, 343.51351, 387.83784, 410.00000)\nsouthern.layout &lt;- cbind(x,y)\nplot(southern, layout=southern.layout, vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\n\nThe incidence matrix is \\(n \\times k\\), where \\(n\\) is the number of actors and \\(k\\) is the number of events\nProject the incidence matrix \\(X\\) into social space, creating a sociomatrix \\(A\\), \\(\\mathbf{A} = \\mathbf{X}\\, \\mathbf{X}^T\\)\nThis transforms the \\(n \\times k\\) into an \\(n \\times n\\) sociomatrix\n\n\n#Sociomatrix\n(f2f &lt;- davismat %*% t(davismat))\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n\ngf2f &lt;- graph_from_adjacency_matrix(f2f, mode=\"undirected\", diag=FALSE, add.rownames=TRUE)\ngf2f &lt;- simplify(gf2f)\nplot(gf2f, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n## who is the most central?\ncb &lt;- betweenness(gf2f)\n#plot(gf2f,vertex.size=cb*10, vertex.color=\"skyblue2\")\nplot(gf2f,vertex.label.cex=1+cb/2, vertex.shape=\"none\",vertex.label.family=\"Helvetica\")\n\n\n\n\n\n\n\n\n\nProject the matrix into event space\n\n\n### this gives you the number of women at each event (diagonal) or mutually at 2 events\n(e2e &lt;- t(davismat) %*% davismat)\n\n    E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14\nE1   3  2  3  2  3  3  2  3  1   0   0   0   0   0\nE2   2  3  3  2  3  3  2  3  2   0   0   0   0   0\nE3   3  3  6  4  6  5  4  5  2   0   0   0   0   0\nE4   2  2  4  4  4  3  3  3  2   0   0   0   0   0\nE5   3  3  6  4  8  6  6  7  3   0   0   0   0   0\nE6   3  3  5  3  6  8  5  7  4   1   1   1   1   1\nE7   2  2  4  3  6  5 10  8  5   3   2   4   2   2\nE8   3  3  5  3  7  7  8 14  9   4   1   5   2   2\nE9   1  2  2  2  3  4  5  9 12   4   3   5   3   3\nE10  0  0  0  0  0  1  3  4  4   5   2   5   3   3\nE11  0  0  0  0  0  1  2  1  3   2   4   2   1   1\nE12  0  0  0  0  0  1  4  5  5   5   2   6   3   3\nE13  0  0  0  0  0  1  2  2  3   3   1   3   3   3\nE14  0  0  0  0  0  1  2  2  3   3   1   3   3   3\n\nge2e &lt;- graph_from_adjacency_matrix(e2e, mode=\"undirected\", diag=FALSE, add.rownames=TRUE)\nge2e &lt;- simplify(ge2e)\nplot(ge2e, vertex.size=20, vertex.color=\"skyblue2\",vertex.label.family=\"Helvetica\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "odes.html#lotka-volterra-model",
    "href": "odes.html#lotka-volterra-model",
    "title": "5  ODEs in R",
    "section": "",
    "text": "5.1.1 Rosenzweig-MacArthur\nRosenzweig and MacArthur (1963) include two elements of ecological realism in their extension of the classic Lotka-Volterra model. First, they include density-dependence of the prey population. The growth of the prey population in the absence of the predator is no longer exponential, but is now a function of current size of the population, with a fixed upper limit to total prey population size. Second, the kill rate of predators is no longer linear. Predators have a functional response to prey abundance. In particular, the number of prey harvested by predators saturates, reflecting the eventual satiation of the predators. Here, I present slightly modified code for the Rosenzsweig-MacArthur model presented in Stevens (2009).\n\nrequire(deSolve)\n### Lotka-Volterra with Type II Functional Response\n# Rosenzweig & MacArthur (1963) model\npredpreyRM &lt;- function(t, y, p) {\n  H &lt;- y[1]\n  P &lt;- y[2]\n  with(as.list(p), {\n    dH &lt;- b*H * (1 - alpha*H) - w*P*H/(D+H)\n    dP &lt;- e*w*P*H/(D+H) - s*P\n    return(list(c(dH, dP)))\n  }) \n}\nb &lt;- 0.8\ne &lt;- 0.07\ns &lt;- 0.2\nw &lt;- 5\nD &lt;- 400\nalpha &lt;- 0.001\nH &lt;- 0:(1/alpha)\n\n\nHiso &lt;- expression(b/w * (D + (1 - alpha * D) * H - alpha * H^2))\nHisoStable &lt;- eval(Hiso)\n\np.RM &lt;- c(b = b, alpha = alpha, e = e, s = s, w = w, D = D)\ntmax &lt;- 150\ntimes &lt;- seq(0,tmax,by=0.1)\nRM1 &lt;- as.data.frame(ode(c(900, 120), times, predpreyRM, p.RM))\ncolnames(RM1) &lt;- c(\"time\",\"prey\",\"predator\")\n\nplot(RM1[,\"time\"], RM1[,\"prey\"], type=\"l\", lwd=2, col=\"blue\", xaxs=\"i\",\n     xlab=\"Time\", ylab=\"Population Size\",\n     ylim=c(0,900))\nlines(RM1[,\"time\"], RM1[,\"predator\"], col=\"orange\", lwd=2)\nlegend(\"topright\",c(\"Predator\",\"Prey\"), col=c(\"orange\",\"blue\"),lwd=2)\n\n\n\n\n\n\n\n\nNo more cycles!\nI have created a shiny app that allows you to interact with the Lotka-Volterra model here. Using the model of Hastings and Powell (1991), I’ve made another shiny app that shows how a model with just three trophic levels, parameterized with realistic values for the parameters, can display chaotic dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#sir-model",
    "href": "odes.html#sir-model",
    "title": "5  ODEs in R",
    "section": "5.2 SIR Model",
    "text": "5.2 SIR Model\nWe start with a simple Susceptible-Infeced-Recovered (SIR) epidemic. The SIR epidemic is a system of three coupled ODEs.\n\nSimple model for a closed-population (i.e., no births or deaths)\nNeed to write a function that encodes the system of equations\nThe function takes three arguments t, x, and parms\n\nthese are the time over which the equations are integrated, the state values (i.e., S,I, and R), and the model parameters\n\nThe function starts by renaming the elements of the state vector x as things that make the equations easier to read – e.g., I instead of x[2]\nThe line with(as.list(parms) can take some unpacking\n\nUsing with() means setting up a local scope for variables\nas.list() coerces our vector of parameters into a list\nthese two elements allow us to write the equations in a simple and readable way\nNote we don’t have to say something like parms[\"beta\"] or parms[3] in order to use that parameter in our equation\n\n\n\nrequire(deSolve)\nsir &lt;- function(t,x,parms){\n    S &lt;- x[1]\n    I &lt;- x[2]\n    R &lt;- x[3]\n  with(as.list(parms),\n{\n    dS &lt;- -beta*S*I\n    dI &lt;- beta*S*I - nu*I\n    dR &lt;- nu*I\n    res &lt;- c(dS,dI,dR)\n  list(res)\n})\n}\n\n\nIn order to integrate the equations, use the function lsoda() (which is the solver we use) or ode() (which is a wrapper for different types of solvers including lsoda)\nWe pass the solver the initial state vector, the times, the name of our function that describes the system of equations, and the vector of parameters\nThe solver will return the solutions as a matrix; we coerce this using data.frame() to make it easier to work with, plot, etc.\nOnce we have the data frame, we name the columns to make them easier to refer to\n\n\nN &lt;- 1e4\nparms &lt;- c(N=N,beta=0.0001, nu = 1/2.5)\ntimes &lt;- seq(0,50,0.1)\nx0 &lt;- c(N-1,1,0)\nstateMatrix &lt;- as.data.frame(lsoda(x0,times,sir,parms))\n\ncolnames(stateMatrix) &lt;- c(\"time\",\"S\",\"I\",\"R\")\nplot(stateMatrix[,\"time\"], stateMatrix[,\"S\"], type=\"l\", lwd=2, col=\"blue\",\n     ylim=c(0,1e4),\n     xlab=\"Time\", ylab=\"Population Size\")\nlines(stateMatrix[,\"time\"], stateMatrix[,\"I\"], col=\"red\", lwd=2)\nlines(stateMatrix[,\"time\"], stateMatrix[,\"R\"], col=\"green\", lwd=2)\nlegend(\"right\", c(\"S\",\"I\",\"R\"), col=c(\"blue\",\"red\",\"green\"), lwd=2)\n\n\n\n\n\n\n\n\nPlot the phase portrait.\n\nplot(stateMatrix[,\"S\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Susceptible\", ylab=\"Infected\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#measles-model",
    "href": "odes.html#measles-model",
    "title": "5  ODEs in R",
    "section": "5.3 Measles Model",
    "text": "5.3 Measles Model\n\n\n\nState diagram for the SEIR model.\n\n\n\nSusceptible, Exposed, Infected, Recovered (SEIR) model\nUse parameterization from Ottar Bjornstad (a.k.a., “The Measles Man”)\nOpen population of constant size (birth rate = death rate (\\(\\mu\\)))\nInclude vaccinated fraction \\(p\\)\nModel is a damped oscillator\n\nBased on this parameterization, what is the life expectancy of individuals in the population? How long is the latent period? How long are cases infectious?\n\nseir &lt;- function(t,x,parms){\n    S &lt;- x[1]\n    E &lt;- x[2]\n    I &lt;- x[3]\n    R &lt;- x[4]\n  with(as.list(parms),{\n    dS &lt;- mu*(N*(1-p)-S) - beta*S*I/N\n    dE &lt;- beta*S*I/N - (mu + sigma)*E\n    dI &lt;- sigma*E-(mu+gamma)*I\n    dR &lt;- gamma*I-mu*R+ mu*N*p\n    res &lt;- c(dS,dE,dI,dR)\n    list(res)\n  })\n}\n\ntimes &lt;-  seq(0, 30, by = 1/52)\nparms &lt;-  c(mu = 1/75, N = 1, p = 0, beta = 1250, sigma = 365/7, gamma = 365/7)\nxstart = c(S = 0.06, E = 0, I = 0.001, R = 0)\nstateMatrix &lt;-  as.data.frame(lsoda(xstart, times, seir, parms))\n##\ncolnames(stateMatrix) &lt;- c(\"time\",\"S\",\"E\", \"I\",\"R\")\nplot(stateMatrix[,\"time\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Time\", ylab=\"Fraction Infected\")\n\n\n\n\n\n\n\n\nSpiralize! (i.e., plot the phase portrait)\n\nplot(stateMatrix[,\"S\"], stateMatrix[,\"I\"], type=\"l\", lwd=2, col=\"blue\",\n     xlab=\"Susceptible\", ylab=\"Infected\")\n\n\n\n\n\n\n\n\nCalculate the power spectrum. We’ll trim the plot for all periods greater than 2.5 yrs, since the power is essentially zero for all such periods.\n\nspec &lt;- spectrum(stateMatrix$I, log=\"no\", spans=c(2,2), plot=FALSE)\ndelta &lt;- 1/52\nplot(spec$freq[1:84]/delta, 2*spec$spec[1:84], type=\"l\", lwd=2, col=\"red\", xlab=\"Period\", ylab=\"Spectrum\")\n\n\n\n\n\n\n\nfmax &lt;- which(spec$spec==max(spec$spec))\n1/spec$freq[fmax]\n\n[1] 123.0769",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "odes.html#lorenz-attractor",
    "href": "odes.html#lorenz-attractor",
    "title": "5  ODEs in R",
    "section": "5.4 Lorenz Attractor",
    "text": "5.4 Lorenz Attractor\nThe Lorenz Attractor is a classic model for dynamical systems and I include it to give you another example of numerically integrating a system of equations in deSolve. It certainly makes for cool phase portraits.\n\nlorenz &lt;- function(t, state, p) {\n    with(as.list(c(state, parms)), {\n\n        dx &lt;- sigma*(y - x)\n        dy &lt;- x*(rho - z) - y\n        dz &lt;- x*y - beta*z\n\n        list(c(dx, dy, dz))\n    })\n}\n\nparms &lt;- c(sigma=10, beta=8/3, rho=28)\ny0 &lt;- c(x = 1, y = 1, z = 1)\ny0p &lt;- y0 + c(1e-6, 0, 0)\ntimes &lt;- seq(0, 100, 0.01)\n\nout &lt;- ode(y = y0, times = times, func = lorenz, parms = parms)\nout2 &lt;- ode(y = y0p, times = times, func = lorenz, parms = parms)\n\nplot(out[,\"x\"], out[,\"y\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"x\", ylab = \"y\")\n\n\n\n\n\n\n\nplot(out[,\"x\"], out[,\"z\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"x\", ylab = \"z\")\n\n\n\n\n\n\n\nplot(out[,\"y\"], out[,\"z\"], type=\"l\", lwd=0.25, main = \"Lorenz butterfly\", xlab = \"y\", ylab = \"z\")\n\n\n\n\n\n\n\n\nEven cooler than the static phase-portraits, we can animate the dynamics of the model on the underlying attractor. We create the animation using the animation package. Here’s an example of its usage:\n\nlibrary(animation)\nsaveVideo({\n    ani.options(interval = 0.05)\n    for (i in seq(2,10000,by=2)) {\n    plot(out[,\"x\"], out[,\"z\"], type = \"l\", col=grey(0.85),\n         xlim = c(-20, 20), ylim = c(0, 50), xlab=\"x\", ylab=\"z\")\n    points(out[i,\"x\"], out[i,\"z\"], pch=19, col=ifelse(out[i,\"x\"]&lt;0,\"red\",\"blue\"))}\n}, video.name = \"lorenz1.mp4\", other.opts = \"-pix_fmt yuv420p -b 300k\")\n\n\nWhen the \\(x\\) and \\(z\\) variables are positively correlated, the point is colored blue. When they are negatively correlated, the point is colored red. If you were to measure this system, whether you saw a positive or a negative correlation between \\(x\\) and \\(z\\) would depend on where in the attractor the system was. Different researchers studying the system at different times could get very different ideas about the system if they didn’t understand the underlying attractor. This is an important lesson from complex systems.\n\n\n\n\nHastings, Alan, and Thomas Powell. 1991. “Chaos in a Three-Species Food Chain.” Ecology 72 (3): 896–903. https://doi.org/10.2307/1940591.\n\n\nRosenzweig, M. L., and R. H. MacArthur. 1963. “Graphical Representation and Stability Conditions of Predator-Prey Interactions.” The American Naturalist 97 (895): 209–23. https://doi.org/10.1086/282272.\n\n\nStevens, M. Henry. 2009. A Primer of Ecology with R. New York: Springer. https://doi.org/10.1007/978-0-387-89882-7.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baalen, M. van, and M. W. Sabelis. 1995. “The Dynamics of Multiple\nInfection and the Evolution of Virulence.” American\nNaturalist 146 (6): 881–910. http://www.jstor.org/stable/2463102.\n\n\nBelovsky, G. E. 1987. “Hunter-Gatherer Foraging: A Linear\nProgramming Approach.” Journal of Anthropological\nArchaeology 6 (1): 29–76. https://doi.org/10.1016/0278-4165(87)90016-X.\n\n\nCharnov, Eric L. 1976. “Optimal Foraging, the Marginal Value\nTheorem.” Theoretical Population Biology 9 (2): 129–36.\nhttps://doi.org/10.1016/0040-5809(76)90040-X.\n\n\n———. 1997. “Trade-Off-Invariant Rules for Evolutionary Stable Life\nHistories.” Nature 387 (6631): 393–94. https://doi.org/10.1038/387393a0.\n\n\nEfferson, Charles, Sonja Vogt, and Ernst Fehr. 2020. “The Promise\nand the Peril of Using Social Influence to Reverse Harmful\nTraditions.” Nature Human Behaviour 4 (1): 55–68. https://doi.org/10.1038/s41562-019-0768-2.\n\n\nGadgil, Madhav, and William H. Bossert. 1970. “Life Historical\nConsequences of Natural Selection.” The American\nNaturalist 104 (935): 1–24. http://www.jstor.org/stable/2459070.\n\n\nHastings, Alan, and Thomas Powell. 1991. “Chaos in a Three-Species\nFood Chain.” Ecology 72 (3): 896–903. https://doi.org/10.2307/1940591.\n\n\nHenrich, Joseph. 2004. “Demography and Cultural Evolution: How\nAdaptive Cultural Processes Can Produce Maladaptive Losses: The\nTasmanian Case.” American Antiquity 69 (2):\n197–214. https://doi.org/10.2307/4128416.\n\n\nHolt, R. D., A. P. Dobson, M. Begon, R. G. Bowers, and E. M. Schauber.\n2003. “Parasite Establishment in Host Communities.”\nEcology Letters 6 (9): 837–42. https://doi.org/10.1046/j.1461-0248.2003.00501.x.\n\n\nJones, J. H. 2009. “The Force of Selection on the Human Life\nCycle.” Evolution and Human Behavior 30 (5): 305–14. https://doi.org/10.1016/j.evolhumbehav.2009.01.005.\n\n\nLeslie, P., and B. Winterhalder. 2002. “Demographic Consequences\nof Unpredictability in Fertility Outcomes.” American Journal\nof Human Biology 14 (2): 168–83. https://doi.org/10.1002/ajhb.10044.\n\n\nLevins, R. 1968. Evolution in Changing Environments. Princeton:\nPrinceton University Press.\n\n\nLevins, Richard. 1962. “Theory of Fitness in a Heterogeneous\nEnvironment. I. The Fitness Set and Adaptive\nFunction.” The American Naturalist 96 (891): 361–73. http://www.jstor.org/stable/2458725.\n\n\nMay, R. M. 1976. “Simple Mathematical-Models with Very Complicated\nDynamics.” Nature 261 (5560): 459–67. https://doi.org/10.1038/261459a0.\n\n\nNair, Jayakrishnan, Adam Wierman, and Bert Zwart. 2022. The\nFundamentals of Heavy Tails: Properties, Emergence, and Estimation.\nCambridge Series in Statistical and Probabilistic Mathematics.\nCambridge: Cambridge University Press.\n\n\nNoy-Meir, Imanuel. 1975. “Stability of Grazing Systems: An\nApplication of Predator-Prey Graphs.” Journal of Ecology\n63 (2): 459–81. https://doi.org/10.2307/2258730.\n\n\nOrians, Gordon H. 1969. “On the Evolution of Mating Systems in\nBirds and Mammals.” The American Naturalist 103 (934):\n589–603. https://doi.org/10.1086/282628.\n\n\nParker, G. A., and R. A. Stuart. 1976. “Animal Behavior as a\nStrategy Optimizer: Evolution of Resource Assessment Strategies and\nOptimal Emigration Thresholds.” American Naturalist 110\n(976): 1055–76. https://doi.org/10.1086/283126.\n\n\nRogers, A. R. 1988. “Does Biology Constrain Culture?”\nAmerican Anthropologist 90 (4): 819–31. https://doi.org/10.1525/aa.1988.90.4.02a00030.\n\n\nRogers, E. M. 2003. Diffusion of Innovations. 5th ed. New York:\nFree Press.\n\n\nRosenzweig, M. L., and R. H. MacArthur. 1963. “Graphical\nRepresentation and Stability Conditions of Predator-Prey\nInteractions.” The American Naturalist 97 (895): 209–23.\nhttps://doi.org/10.1086/282272.\n\n\nScheffer, M. 2009. Critical Transitions in Nature and Society.\nPrinceton: Princeton University Press.\n\n\nSmith, C. C., and S. D. Fretwell. 1974. “The Optimal Balance\nBetween Size and Number of Offspring.” American\nNaturalist 108: 499–506. https://www.jstor.org/stable/2459681.\n\n\nStevens, M. Henry. 2009. A Primer of Ecology with\nR. New York: Springer. https://doi.org/10.1007/978-0-387-89882-7.\n\n\nWeitzman, M. L. 2009. “On Modeling and Interpreting the Economics\nof Catastrophic Climate Change.” The Review of Economics and\nStatistics XCI (1): 1–19. https://doi.org/10.1162/rest.91.1.1\n.\n\n\nZeeman, E. C. 1976. “Catastrophe Theory.” Scientific\nAmerican 234 (4): 65–65 &. https://doi.org/10.1038/scientificamerican0476-65.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "gettingR.html",
    "href": "gettingR.html",
    "title": "1  Getting Started in R",
    "section": "",
    "text": "1.1 Setting up R and RStudio\nThese notes will help get you started with R. They are mostly quite non-graphical, focusing instead on the basics of the R language.\nTo get started, you need to do two things:\nR is the software. RStudio is what is known as an Integrated Development Environment (IDE). Basically, an IDE is a way of running base software the provides various tools to make the coding experience easier (e.g., an editor with syntax highlighting, debugger, and interactive graphics facilities).\nHere is a brief video explaining what to do:\nWhat follows are some old notes that are very Base-R focused. The thing is, even though most people use the tidyverse tools these days, it’s still valuable to understand Base-R (since that’s what everything else is built upon!). This is particularly true for the material that forms the bulk of these notes. Making theoretical scientific figures (chapter Chapter 3) is simply easier in Base-R. You typically don’t need to do complex data wrangling to make a theoretical figure, so the powerful data-manipulation tools of dplyr, for instance, are unnecessary. Sometimes, we want to utilize the grid package, which underlies the tidyverse graphics library ggplot2, directly. Again, I find this more straightforward in Base-R.\nThe igraph package for drawing graphs (a.k.a., “networks”) that we discuss in chapter Chapter 4 also runs in Base-R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#setting-up-r-and-rstudio",
    "href": "gettingR.html#setting-up-r-and-rstudio",
    "title": "1  Getting Started in R",
    "section": "",
    "text": "Download R CRAN\nDownload and Install RStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#what-is-r",
    "href": "gettingR.html#what-is-r",
    "title": "1  Getting Started in R",
    "section": "1.2 What Is R?",
    "text": "1.2 What Is R?\n\nR is statistical numerical software\nR is a “dialect” of the S statistical programming language\nR is a system for interactive data analysis\nR is a high-level programming language\nR is free\nR is state-of-the-art in statistical computing. It is what many (most?) research statisticians use in their work",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#why-use-r",
    "href": "gettingR.html#why-use-r",
    "title": "1  Getting Started in R",
    "section": "1.3 Why Use R?",
    "text": "1.3 Why Use R?\n\nR is FREE! That, by itself, is almost enough. No complicated licensing. Broad dissemination of research methodologies and results, etc.\nR is available for a variety of computer platforms (e.g., Linux, MacOS, Windows).\nR is widely used by professional statisticians, social scientists, biologists, demographers, and other scientists. This increases the likelihood that code will exist to do a calculation you might want to do.\nR has remarkable online help lists, tutorials, etc.\nR represents the state-of-the-art in statistical computing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#wouldnt-something-menu-driven-be-easier",
    "href": "gettingR.html#wouldnt-something-menu-driven-be-easier",
    "title": "1  Getting Started in R",
    "section": "1.4 Wouldn’t Something Menu-Driven Be Easier?",
    "text": "1.4 Wouldn’t Something Menu-Driven Be Easier?\n\nFallacious thinking\n\nFor teaching, text-based input is always better\nExample code can be copied and input exactly; you can then tweak it and see what happens, facilitating the learning process\n\nAn example\nWhat follows is a pretty complicated graph of the grooming interactions of a group of rhesus monkeys, Macaca mulatta, observed by Sade (1972)\nWith the code I used to generate this graph, you can recreate the figure exactly. Try it! It doesn’t matter if you have no idea what you’re doing yet. That’s the point.\nThe only thing you need to make this figure is to install and load the library igraph",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#rhesus-money-grooming-network",
    "href": "gettingR.html#rhesus-money-grooming-network",
    "title": "1  Getting Started in R",
    "section": "1.5 Rhesus Money Grooming Network",
    "text": "1.5 Rhesus Money Grooming Network\n\nrequire(igraph)\nrhesus &lt;- read.table(\"./data/sade1.txt\", skip=1, header=FALSE)\nrhesus &lt;- as.matrix(rhesus)\nnms &lt;- c(\"066\", \"R006\", \"CN\", \"ER\", \"CY\", \"EC\", \"EZ\", \"004\", \"065\", \"022\", \"076\", \n         \"AC\", \"EK\", \"DL\", \"KD\", \"KE\")\nsex &lt;- c(rep(\"M\",7), rep(\"F\",9))\ndimnames(rhesus)[[1]] &lt;- nms\ndimnames(rhesus)[[2]] &lt;- nms\ngrhesus &lt;- graph_from_adjacency_matrix(rhesus, weighted=TRUE)\nV(grhesus)$sex &lt;- sex\n\nrhesus.layout &lt;- layout.kamada.kawai(grhesus)\nplot(grhesus, \n     edge.width=log10(E(grhesus)$weight)+1, \n     edge.arrow.width=0.5,\n     vertex.label=V(grhesus)$name,\n     vertex.label.family=\"Helvetica\",\n     vertex.color=as.numeric(V(grhesus)$sex==\"F\")+5, \n     layout=rhesus.layout)\n\n\n\n\nRhesus monkey grooming network (Slade 1972).\n\n\n\n\n\nA note on loading data: the above code loads a data file apparently called \"./data/sade1.txt\"\nWhat does that mean?\nThe one dot followed by a slash, ./, means to go into the sub-directory called data, which is in our current working directory, and read the text file called sade1.txt\nIf the data sub-directory was actually in, say, the same directory where our working directory is located (i.e., they were two sub-directories of the same higher-level folder), we would use two dots, ../, which means to go out one directory in the hierarchy\nThis is actually not R but the underlying OS file system\nLearning about the file/directory structure of your computer is actually an important (and under-appreciated) data-science skill\nCheck out the fantastic MIT course The Missing Semester of Your CS Education for information on various tools that can really improve your workflows and general skill-level",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#a-few-conventions-and-other-helpful-bits",
    "href": "gettingR.html#a-few-conventions-and-other-helpful-bits",
    "title": "1  Getting Started in R",
    "section": "1.6 A Few Conventions and Other Helpful Bits",
    "text": "1.6 A Few Conventions and Other Helpful Bits\n\nThere are some things that you will see over and over in the code embedded in this document\nThe assignment operator &lt;- is used to assign a value to a name.\nThe value is on the right-hand side of the operator and the name is on the left side\nYou can use = for assignment, but I don’t recommend it (it doesn’t work at all levels, makes the code harder to read, etc.)\nDifferent environments make it more or less easy to use &lt;-. In RStudio, hit the option key and the minus sign simultaneously\nComments are marked by #: anything following the hash will be ignored by R\nUse comments liberally to help you (and others) understand your code\nIn these notes, the output that you would see on your own command line will be white following a grey box (the input). Frequently, it will begin with a [1], which indicates the first element of a vector\nSometimes I enclose a command in parentheses; this is simply to force R to echo the output (for pedagogical purposes)\n\n\n# a comment\nx &lt;- c(1,2,3)\n(y &lt;- c(4,5,6))\n\n[1] 4 5 6\n\n\n\nYou will probably want to seek help on functions. At the command line, simply type a question mark followed immediately by the function you want to query, ?function.name\n\nIn Rstudio, you may be given auto-complete suggestions, which you can click on to save typing\n\nWhen you are done with your R session, type q() at the command line\nR will ask you if you want to save your workspace. For now, you probably don’t.\nCheck out the much more comprehensive Introduction to R for all the language details.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#r-as-a-calculator",
    "href": "gettingR.html#r-as-a-calculator",
    "title": "1  Getting Started in R",
    "section": "1.7 R as a Calculator",
    "text": "1.7 R as a Calculator\n\n# addition\n2+2\n\n[1] 4\n\n# multiplication\n2*3\n\n[1] 6\n\na &lt;- 2\nb &lt;- 3\na*b\n\n[1] 6\n\n# division\n2/3\n\n[1] 0.6666667\n\nb/a\n\n[1] 1.5\n\n1/b/a\n\n[1] 0.1666667\n\n# note order of operations!\n1/(b/a)\n\n[1] 0.6666667\n\n# parentheses can override order of operations\n# an exponential\nexp(-2)\n\n[1] 0.1353353\n\n# why we age\nr &lt;- 0.02\nexp(-r*45)\n\n[1] 0.4065697\n\n# something more tricky\nexp(log(2))\n\n[1] 2\n\n# generate 20 normally distributed random numbers\nrnorm(20)\n\n [1]  1.749470956  0.916718564 -0.178373474 -0.327876015  0.192697504\n [6]  0.060440148  1.108932911  0.036229325 -0.967395410 -0.858294855\n[11]  1.070746372 -1.077977391  0.795229962 -1.637697198  0.259469862\n[16] -0.007341621  0.358928489  1.288766001  0.408694783 -0.468418438",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#data-types",
    "href": "gettingR.html#data-types",
    "title": "1  Getting Started in R",
    "section": "1.8 Data Types",
    "text": "1.8 Data Types\n\nNumeric\n\nAll numbers in R are of the form double (i.e., double-precision floating point numbers). This can be a bit confusing for people who are used to languages with integer data types (like, most languages!). Entering something that looks like an integer doesn’t mean it is.\n\n\n\n# it looks like an integer, but don't be fooled!\na &lt;- 2\nis.numeric(a)\n\n[1] TRUE\n\nis.integer(a)\n\n[1] FALSE\n\nis.double(a)\n\n[1] TRUE\n\n\n\nInteger\n\nOK, technically R does have an integer class, but it is used very rarely and many functions will convert integers into doubles anyway. If you really must have an integer (e.g., because you are passing output to external C or FORTRAN code that expects it), add the suffix L to the entered number.\n\n\n\na &lt;- 2L\nis.integer(a)\n\n[1] TRUE\n\n\n\nCharacter\n\nStrings are represented by the character data class.\n\n\n\n(countries &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\n\n[1] \"Uganda\"   \"Tanzania\" \"Kenya\"    \"Rwanda\"  \n\nas.character(1:5)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\n\nFactor\n\nFactors are a data type for encoding categorical data. Notice that factors are printed without the quotes. This is because R stores them as a set of codes. Data of type “factor” are different from data of type “character” (which is what plain text is). Note the difference below between factor and character data. Because factors get used in statistical models, they are actually represented as numbers (the levels) that have associated names. Vectors, on the other hand, are just lists of numbers.\n\n\n\ncountries &lt;- factor(c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\ncountries\n\n[1] Uganda   Tanzania Kenya    Rwanda  \nLevels: Kenya Rwanda Tanzania Uganda\n\n# a trick to get some insight into how factors are handled by R\nunclass(countries)\n\n[1] 4 3 1 2\nattr(,\"levels\")\n[1] \"Kenya\"    \"Rwanda\"   \"Tanzania\" \"Uganda\"  \n\ncountries1 &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\")\ncountries1 == unclass(countries1)\n\n[1] TRUE TRUE TRUE TRUE\n\ncountries == unclass(countries)\n\n[1] FALSE FALSE FALSE FALSE\n\n\n\nLogical\n\nTRUE and FALSE are reserved keywords, while T and F are global constants set to these. These logical variables are essential tools for subsetting data. You also use them extensively in setting optional arguments of functions.\n\n\n\nt.or.f &lt;- c(T,F,F,T,T)\nis.logical(t.or.f)\n\n[1] TRUE\n\naaa &lt;- c(1,2,3,4,5)\n# subset\naaa[t.or.f]\n\n[1] 1 4 5\n\n\n\nList\n\nYou can mix different types of data in a list using the command list(). This is useful when you write your own functions and want to output multiple things. Use the function str() to give you information about a list.\n\n\n\nchild1 &lt;- list(name=\"mary\", child.age=6,\nstatus=\"foster\",mother.alive=F, father.alive=T, parents.ages=c(24,35))\nstr(child1)\n\nList of 6\n $ name        : chr \"mary\"\n $ child.age   : num 6\n $ status      : chr \"foster\"\n $ mother.alive: logi FALSE\n $ father.alive: logi TRUE\n $ parents.ages: num [1:2] 24 35",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#coercion",
    "href": "gettingR.html#coercion",
    "title": "1  Getting Started in R",
    "section": "1.9 Coercion",
    "text": "1.9 Coercion\n\nSometimes you have data in one type but need it in a different type\nR provides a variety of methods to coerce data from one type to another\nThese methods are carried out by functions that begin with as.xxx, where xxx is the data type to which you are coercing\n\n\ncountries &lt;- factor(c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\"))\nas.character(countries)\n\n[1] \"Uganda\"   \"Tanzania\" \"Kenya\"    \"Rwanda\"  \n\nas.numeric(countries)\n\n[1] 4 3 1 2\n\n# werk it backwards\ncountries1 &lt;- c(\"Uganda\", \"Tanzania\", \"Kenya\", \"Rwanda\")\nas.factor(countries1)\n\n[1] Uganda   Tanzania Kenya    Rwanda  \nLevels: Kenya Rwanda Tanzania Uganda\n\n# sometimes you want your numbers to actually be strings (e.g., when you make labels or column names)\nas.character(1:5)\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n# there actually is an integer class; it just doesn't get used much at all\na &lt;- 2\nis.integer(a)\n\n[1] FALSE\n\nis.integer(as.integer(a))\n\n[1] TRUE\n\n\n\nYou can check the class of an object using functions that begin with is.xxx, where xxx is the data type you are querying (like is.integer() above)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#creating-vectors",
    "href": "gettingR.html#creating-vectors",
    "title": "1  Getting Started in R",
    "section": "1.10 Creating Vectors",
    "text": "1.10 Creating Vectors\n\nA vector is a list of numbers – it turns out everything in R is represented as a vector but that doesn’t affect your life much.\nIn order to create a vector, you use the the function c(), which concatenates a list of items (hence the “c”).\nYou will use this a lot and it’s a super-common mistake to forget the c() when putting together a list of numbers, factors, etc.\nIf you do forget it, you will get a syntax error\nOften we want either regularly spaced vectors or a vector of one value repeated. R has a number of facilities to perform these operations.\n\n\n( manual &lt;- c(1,3,5,7,9))\n\n[1] 1 3 5 7 9\n\n( count &lt;- 1:20 )\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n( ages &lt;- seq(0,85,by=5) )\n\n [1]  0  5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85\n\n( ones &lt;- rep(1,10) )\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n( fourages &lt;- rep(c(1,2),c(5,10)) )\n\n [1] 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n\n( equalspace &lt;- seq(1,5, length=20) )\n\n [1] 1.000000 1.210526 1.421053 1.631579 1.842105 2.052632 2.263158 2.473684\n [9] 2.684211 2.894737 3.105263 3.315789 3.526316 3.736842 3.947368 4.157895\n[17] 4.368421 4.578947 4.789474 5.000000\n\n\n\nYou can use rep() to repeat values.\nSometimes this can be tricky: the second argument tells R how many repetitions.\nThis argument can be a vector and this, along with the possibility of a vector of the items you want repeated too, allows you to create quite complex patterns very easily.\n\n\nrep(2,10)\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\nrep(c(1,2),10)\n\n [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n\nrep(c(1,2), c(5,10))\n\n [1] 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n\nrep(\"R roolz!\", 3)\n\n[1] \"R roolz!\" \"R roolz!\" \"R roolz!\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#creating-matrices",
    "href": "gettingR.html#creating-matrices",
    "title": "1  Getting Started in R",
    "section": "1.11 Creating Matrices",
    "text": "1.11 Creating Matrices\n\nAs we said, a vector is a list of numbers\nA matrix is a rectangular array of numbers – it is a vector of vectors, with the numbers indexed by row and column.\nOne way to create matrices is to “bind” columns together using the commands cbind() or rbind().\n\n\n# age distribution of Gombe chimps in 1980 and 1986\ncx1980 &lt;- c(7, 13, 8, 13, 5, 35, 9)\ncx1988 &lt;- c(9, 11, 15, 8, 9, 38, 0)\n( C &lt;- cbind(cx1980, cx1988) )\n\n     cx1980 cx1988\n[1,]      7      9\n[2,]     13     11\n[3,]      8     15\n[4,]     13      8\n[5,]      5      9\n[6,]     35     38\n[7,]      9      0\n\n# another way\nC &lt;- c(cx1980, cx1988)\n(  C &lt;- matrix(C, nrow=7, ncol=2) )\n\n     [,1] [,2]\n[1,]    7    9\n[2,]   13   11\n[3,]    8   15\n[4,]   13    8\n[5,]    5    9\n[6,]   35   38\n[7,]    9    0\n\n\n\nWhat happens if we try to bind columns of different lengths?\n\n\n# age distribution at Tai; Boesch uses fewer age classes\ncxboesch &lt;- c(18,10,15,30)\n( C &lt;- cbind(C,cxboesch) )\n\nWarning in cbind(C, cxboesch): number of rows of result is not a multiple of\nvector length (arg 2)\n\n\n           cxboesch\n[1,]  7  9       18\n[2,] 13 11       10\n[3,]  8 15       15\n[4,] 13  8       30\n[5,]  5  9       18\n[6,] 35 38       10\n[7,]  9  0       15\n\n\n\nBoth the warning message and the output can seem a little odd to the uninitiated\nR uses a recycling rule for filling out vectors and matrices\nWhen you try to put together things that are neither the same length nor multiples of each other, you get a warning\nWe can use the recycling rule to make a matrix of ones:\n\n\n( X &lt;- matrix(1,nr=3,nc=3) )\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n\n\n\nNote that using the short version of nrow, nr, is sufficient. This is often true – you can use the minimum name that is unambiguous.\nThe matrix() command requires at least 3 arguments: (1) a vector of numbers that will form the elements of the matrix, (2) the number of rows, and (3) the number of columns.\nFor small matrices, you might want to enter the vectors of values manually\nIf you do this, it’s important to know that R fills matrices column-wise (the standard for FORTRAN and definitely not the way most people actually work!).\nUse the optional argument byrow=TRUE to make R read in the data row-wise\n\n\n# cross-classified data on hair/eye color \nfreq &lt;- c(32,11,10,3,  38,50,25,15,  10,10,7,7,  3,30,5,8)\nhair &lt;- c(\"Black\", \"Brown\", \"Red\", \"Blond\")\neyes &lt;- c(\"Brown\", \"Blue\", \"Hazel\", \"Green\")\nfreqmat &lt;- matrix(freq, nr=4, nc=4, byrow=TRUE)\ndimnames(freqmat)[[1]] &lt;- hair\ndimnames(freqmat)[[2]] &lt;- eyes\nfreqmat\n\n      Brown Blue Hazel Green\nBlack    32   11    10     3\nBrown    38   50    25    15\nRed      10   10     7     7\nBlond     3   30     5     8\n\n# might as well do something with it\nmosaicplot(freqmat)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#data-frames",
    "href": "gettingR.html#data-frames",
    "title": "1  Getting Started in R",
    "section": "1.12 Data Frames",
    "text": "1.12 Data Frames\n\nA data frame is an R object which stores a data matrix. A data frame is essentially a list of variables which are all the same length. A single data frame can hold different types of variables.\nTo access a variable contained in a data frame, use the data frame name followed by the variable name, separated by a dollar sign, $.\n\n\n# five columns of data\nsatu &lt;- c(1,2,3,4,5)\ndua &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\ntiga &lt;- sample(c(TRUE,FALSE),5,replace=TRUE)\nempat &lt;- LETTERS[7:11]\nlima &lt;- rnorm(5)\n# construct a data frame\n(collection &lt;- data.frame(satu,dua,tiga,empat,lima))\n\n  satu dua  tiga empat        lima\n1    1   a FALSE     G -0.24012272\n2    2   b  TRUE     H  1.62867872\n3    3   c FALSE     I  1.34427662\n4    4   d  TRUE     J  0.04494688\n5    5   e FALSE     K  0.11910426\n\n# extract the third variable\ncollection$tiga\n\n[1] FALSE  TRUE FALSE  TRUE FALSE\n\n\n\nby default, data.frame() will produce row numbers (seen to the left of the first column in the data frame collection above)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#directories-and-paths",
    "href": "gettingR.html#directories-and-paths",
    "title": "1  Getting Started in R",
    "section": "1.13 Directories and Paths",
    "text": "1.13 Directories and Paths\n\nR uses a working directory. The default can be set in the Preferences or using an initialization file (i.e., a file that is always read when R starts up).\nIf you read in a file without specifying a path, R will search in the working directory; if there is no file matching the name you provide, you receive an error message\nWe can query the working directory using the command getwd() and we can change it using setwd()\nYou can always load a file by giving either a full or relative path\n\n\ngetwd()\n\n[1] \"/Users/jhj1/Teaching/graphics\"\n\n#setwd(\"/Users/jhj1/Projects/git/AABA2023_Workshop/Markdown\")\n## can't actually change it because it screws up the rendering!\n\n\nSetting the working directory is actually not recommended\nIt is not a good scientific practice that favors replicability/interoperability/etc.\nIt’s generally better to use R Projects in RStudio (as we do in this workshop)\nTo start an R Project, either double-click on the .RProj file in the project’s directory or clicking on the R Project menu button in the upper right corner of your RStudio frame\nTo share the work you have done in an R Project with collaborators, students, or scientists looking to replicate your work, simply share the folder containing the .RProj file\nWhen you quit R, you will be asked if you want to save your R session\nIf a session has previously been saved in your working directory, there will be a copy of the workspace in the R binary format named .RData\nWhen R is started in a particular directory, if there is an .RData file in that directory, it will load automatically\nThis can lead to some surprising behavior if you don’t know that it can happen\nAutomatically saving and loading workspaces is also not recommended\nBest scientific practice involves constructing your workspace using broadly-interoperable data formats (e.g., .csv files) and scripts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#reading-files",
    "href": "gettingR.html#reading-files",
    "title": "1  Getting Started in R",
    "section": "1.14 Reading Files",
    "text": "1.14 Reading Files\n\nThere are a number of ways to read data into R. Probably the easiest and most frequently used involves reading data from plain-text (ASCII) files. These files can be space, tab, or comma delimited.\nYou can create these files in a spreadsheet program like Excel or output them from most other statistical packages.\nYou can read these from a local directory or from an internet source\nR expects delimited files to be “white-space delimited” with values separated by either tabs or spaces and rows separated by carriage returns\nIt’s always a good idea to specify whether or not you have a header (i.e., column names). If you don’t, say header=FALSE; if you do, obviously, say header=TRUE\n\n\n# read a space-delimitted file (a sociomatrix of kids 17 kids aggressive acts toward each other)\n(kids &lt;- read.table(\"./data/strayer_strayer1976-fig2.txt\", header=FALSE))\n\n   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17\n1   0  1  3  4  1  0  0  1  1   0   1   0   7   0   1   0   0\n2   1  0  7  8  2  1  1 12  3   0   1   1   4   1   0   0   2\n3   1  4  0  7  3  2  2  0  1   0   8   1   5   5   0   0   1\n4   3  3  2  0  3  1 13  3  5   1   0   0   8   3   0   2   1\n5   1  0  0  3  0  4  6  0  8   5   1   0   1   3   0   2   1\n6   0  0  0  0  0  0  2  8 11   0   4   0   4   3   0   1   0\n7   1  0  1  9  3  4  0  2  0   0   1   0   7   9   1   1   0\n8   0  0  0  1  1  1  2  0  7   5   1   1   1   0   0   0   0\n9   1  1  1  2  5 11  0  3  0   0   0   0   1   0   0   0   0\n10  0  0  0  0  0  0  1  0  0   0   0  11   0   1   0   1   4\n11  4  0  4  3  3  2  1  0  0   0   0   3  11   5   0   2   2\n12  0  0  0  0  0  0  0  0  0   2   0   0   2   0   8   0   0\n13  0  1  9  3  0  3  6  0  0   0  11   2   0   1   0   7   5\n14  0  1  4  0  1  2  1  0  0   0   1   0   1   0   0   0   0\n15  0  0  0  0  0  0  0  0  0   0   0   3   0   0   0   0   0\n16  0  0  0  0  0  1  0  0  0   0   0   1   1   0   0   0   0\n17  0  0  0  0  0  0  0  0  1   1   0   0   0   0   0   0   0\n\n\n\nIf your file is delimited by something other than spaces, it is a good idea to use a slightly different function, read.delim() and specify exactly what the delimiter is\nFrequently, there will be non-tabular information at the top of a file (e.g., meta-data describing the data set). Use the skip=n option, where n is the number of lines you want skipped.\n\n\nquercus &lt;- read.delim(\"./data/quercus.txt\", skip=24, sep=\"\\t\", header=TRUE)\nhead(quercus)\n\n                    Species   Region Range acorn.size tree.height\n1           Quercus alba L. Atlantic 24196        1.4          27\n2  Quercus bicolor Willd.   Atlantic  7900        3.4          21\n3 Quercus macrocarpa Michx. Atlantic 23038        9.1          25\n4  Quercus prinoides Willd. Atlantic 17042        1.6           3\n5         Quercus Prinus L. Atlantic  7646       10.5          24\n6    Quercus stellata Wang. Atlantic 19938        2.5          17",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#the-workspace",
    "href": "gettingR.html#the-workspace",
    "title": "1  Getting Started in R",
    "section": "1.15 The Workspace",
    "text": "1.15 The Workspace\n\nR handles data in a manner that is different than many statistical packages.\nIn particular, you are not limited to a single rectangular data matrix at a time.\nThe workspace holds all the objects (e.g., data frames, variables, functions) that you have created or read in.\nYou can essentially have as many data frames as your machine’s memory will allow.\nTo find out what lurks in your workspace, use objects() command.\nTo remove an object, use rm().\nIf you really want to clear your whole workspace, you can use the following syntax: rm(list=ls()). Beware, though. Once you do this, you don’t get the data back.\n\n\nobjects()\n\n [1] \"a\"             \"aaa\"           \"ages\"          \"b\"            \n [5] \"C\"             \"child1\"        \"collection\"    \"count\"        \n [9] \"countries\"     \"countries1\"    \"cx1980\"        \"cx1988\"       \n[13] \"cxboesch\"      \"dua\"           \"empat\"         \"equalspace\"   \n[17] \"eyes\"          \"fourages\"      \"freq\"          \"freqmat\"      \n[21] \"grhesus\"       \"hair\"          \"kids\"          \"lima\"         \n[25] \"manual\"        \"nms\"           \"ones\"          \"quercus\"      \n[29] \"r\"             \"rhesus\"        \"rhesus.layout\" \"satu\"         \n[33] \"sex\"           \"t.or.f\"        \"tiga\"          \"x\"            \n[37] \"X\"             \"y\"            \n\nrm(aaa)\nrm(list=ls())\nobjects()\n\ncharacter(0)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#scope",
    "href": "gettingR.html#scope",
    "title": "1  Getting Started in R",
    "section": "1.16 Scope",
    "text": "1.16 Scope\n\nBecause the R workspace can contain many different variables and even multiple data frames, you must be aware of scope\nWhen we extract columns of a data frame (e.g., if we wanted to plot them) we need to use the syntax data.frame$col.name\n\n\n## load it again because we cleared all objects!\nquercus &lt;- read.delim(\"./data/quercus.txt\", skip=24, sep=\"\\t\", header=TRUE)\nplot(quercus$tree.height, quercus$acorn.size, pch=16, col=\"red\", xlab=\"Tree Height (m)\", ylab=\"Acorn Size (cm3)\")\n\n\n\n\n\n\n\n\n\nIt can be a hassle having to type the data frame name (and dollar sign) over and over again\nWith the with() function, we can set up a local scoping rule that allows us to drop the need to type the data frame name (and dollar sign) to access columns of a data frame\n\n\nwith(quercus, plot(tree.height, acorn.size, pch=16, col=\"blue\", xlab=\"Tree Height (m)\", ylab=\"Acorn Size (cm3)\"))\n\n\n\n\n\n\n\n\n\nApparently, there are R users who gladly use with and those who hate its use. I fall into the former category.\nNote that this is a very Base-R perspective. the tidyverse (e.g., ggplot2, etc.) changes many of these issues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#indexing-and-subsetting",
    "href": "gettingR.html#indexing-and-subsetting",
    "title": "1  Getting Started in R",
    "section": "1.17 Indexing and Subsetting",
    "text": "1.17 Indexing and Subsetting\n\nIndex (and access) the elements of a vector using square brackets. myvec[1] takes the first element of a vector called myvec.\nUse the colon (:) operator for sequences. myvec[1:5] takes the first five elements of myvec.\nR is unusual in that it allows negative indexing: myvec[-1] takes all elements of except the first one. To exclude a sequence, you need to place the sequence within parentheses: myvec[-(1:5)].\nVector indices don’t have to be consecutive: myvec[c(2,5,1,11)].\n\n\nmyvec &lt;- c(1,2,3,4,5,6,66,77,7,8,9,10)\nmyvec[1]\n\n[1] 1\n\nmyvec[1:5]\n\n[1] 1 2 3 4 5\n\nmyvec[-1]\n\n [1]  2  3  4  5  6 66 77  7  8  9 10\n\nmyvec[-(1:5)]\n\n[1]  6 66 77  7  8  9 10\n\n# try without the parentheses\n#myvec[-1:5]\nmyvec[c(2,5,1,11)]\n\n[1] 2 5 1 9\n\n\n\nAccess the elements of a data frame using the dollar sign. Subsetting anything other than a data frame uses square brackets.\n\n\ndim(quercus)\n\n[1] 39  5\n\nsize &lt;- quercus$acorn.size\nsize[1:3] #first 3 elements\n\n[1] 1.4 3.4 9.1\n\nsize[17]  #only element 17\n\n[1] 4.8\n\nsize[-39] #all but the last element\n\n [1]  1.4  3.4  9.1  1.6 10.5  2.5  0.9  6.8  1.8  0.3  0.9  0.8  2.0  1.1  0.6\n[16]  1.8  4.8  1.1  3.6  1.1  1.1  3.6  8.1  3.6  1.8  0.4  1.1  1.2  4.1  1.6\n[31]  2.0  5.5  5.9  2.6  6.0  1.0 17.1  0.4\n\nsize[c(3,6,9)] # elements 3,6,9\n\n[1] 9.1 2.5 1.8\n\nsize[quercus$Region==\"California\"] # use a logical test to subset\n\n [1]  4.1  1.6  2.0  5.5  5.9  2.6  6.0  1.0 17.1  0.4  7.1\n\nquercus[3,4] # access an element of an array or data frame by X[row,col]\n\n[1] 9.1\n\nquercus[,\"tree.height\"]\n\n [1] 27.0 21.0 25.0  3.0 24.0 17.0 15.0  0.3 24.0 11.0 15.0 23.0 24.0  3.0 13.0\n[16] 30.0  9.0 27.0  9.0 24.0 23.0 27.0 24.0 23.0 18.0  9.0  9.0  4.0 18.0  6.0\n[31] 17.0 20.0 30.0 23.0 26.0 21.0 15.0  1.0 18.0\n\n\n\nThe comma with nothing in front of it means take every row in the column named \"tree.height\".",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#more-subsetting",
    "href": "gettingR.html#more-subsetting",
    "title": "1  Getting Started in R",
    "section": "1.18 More Subsetting",
    "text": "1.18 More Subsetting\n\nPositive indices include, negative indices exclude elements\n1:3 means a sequence from 1 to 3\nYou can only use a single negative subscript, i.e., you can’t use quercus$acorn.size[-1:3]\nOf course, you can get around this by enclosing the vector in parentheses quercus$acorn.size[-(1:3)]\nThe logical operators are == (equal), != (not equal), and the various greater than/less than symbols: &gt;, &gt;=, &lt;, &lt;=\nFurther logicals are & (and), | (or), ! (not), && (another and), || (another or)\n& and ! work elementwise on vectors: element 1 is compared in the two vectors, then element 2, and so on\n&& and || are tricky. These logical tests evaluate left to right, examining only the first element of each vector (they go until a result is determined for ||).\n\nWhy would you want that?? In general, you don’t. It makes some calculations faster.\n\nWhen you refer to a variable in a data frame, you must specify the data frame name followed a dollar sign and the variable name quercus$acorn.size\nTesting for equality is just a special case of a logical test. We frequently want to identify numbers either above or below some criterion.\n\n\nmyvec &lt;- c(1,2,3,4,5,6,66,77,7,8,9,10)\nmyvec &lt;- myvec[myvec&lt;=10]\nmyvec\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:7\n# elements that are greater than 2 but less than 6\n(x&gt;2) & (x&lt;6)\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n\n\nis.even &lt;- rep(c(FALSE, TRUE),5)\n(evens &lt;- myvec[is.even])\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#missing-values",
    "href": "gettingR.html#missing-values",
    "title": "1  Getting Started in R",
    "section": "1.19 Missing Values",
    "text": "1.19 Missing Values\n\nNA is a special code for missing data.\nNA pretty much means “Don’t Know.”\nThe presence of NA values in your data set can lead to some surprising consequences.\nYou can’t test for a NA the way you would test for any other value (i.e., using the == operator) since variable==NA is like asking in English, is the variable equal to some number I don’t know? How could you know that?!\nIt also doesn’t make any sense to add one to something you don’t know what it is – 1+NA is meaningless!\nR therefore provides the function is.na() that allows us to subset using logicals.\n\n\naaa &lt;- c(1,2,3,NA,4,5,6,NA,NA,7,8,9,NA,10)\naaa &lt;- aaa[!is.na(aaa)]\naaa\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nHere we used the not-operator (!) to index everything that is not an NA\nThis is actually probably the most common way of using is.na().\n\n\naaa &lt;- c(1,2,3,NA,4,5,6,NA,NA,7,8,9,NA,10)\nis.na(aaa)\n\n [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13]  TRUE FALSE\n\n!is.na(aaa)\n\n [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[13] FALSE  TRUE\n\n\n\nThere are a couple other special values of objects\nOne is Inf, which means “infinity.”” It may result from dividing zero by zero.\nAnother is NaN, which means “not a number.” You will get this is, e.g., you try to take a logarithm of a negative number.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#summarizing-data",
    "href": "gettingR.html#summarizing-data",
    "title": "1  Getting Started in R",
    "section": "1.20 Summarizing Data",
    "text": "1.20 Summarizing Data\n\nThe function table() is a very useful way of exploring data\n\n\n# generate 100 Poisson random numbers with mean/variance=5\naaa &lt;- rpois(100,5)\ntable(aaa)\n\naaa\n 1  2  3  4  5  6  7  8  9 10 11 12 \n 5  9 14 13 17 16  9 10  3  2  1  1 \n\n\n\ndonner &lt;- read.table(\"./data/donner.dat\", header=TRUE, skip=2)\n# survival=0 == died; male=0 == female\nwith(donner, table(male,survival))\n\n    survival\nmale  0  1\n   0  5 10\n   1 20 10\n\n# table along 3 dimensions\nwith(donner, table(male,survival,age))\n\n, , age = 15\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 0\n\n, , age = 18\n\n    survival\nmale 0 1\n   0 0 0\n   1 0 1\n\n, , age = 20\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 1\n\n, , age = 21\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 0\n\n, , age = 22\n\n    survival\nmale 0 1\n   0 0 1\n   1 0 0\n\n, , age = 23\n\n    survival\nmale 0 1\n   0 0 1\n   1 2 1\n\n, , age = 24\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 0\n\n, , age = 25\n\n    survival\nmale 0 1\n   0 1 1\n   1 5 1\n\n, , age = 28\n\n    survival\nmale 0 1\n   0 0 0\n   1 2 2\n\n, , age = 30\n\n    survival\nmale 0 1\n   0 0 0\n   1 3 1\n\n, , age = 32\n\n    survival\nmale 0 1\n   0 0 2\n   1 0 1\n\n, , age = 35\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 40\n\n    survival\nmale 0 1\n   0 0 1\n   1 1 1\n\n, , age = 45\n\n    survival\nmale 0 1\n   0 2 0\n   1 0 0\n\n, , age = 46\n\n    survival\nmale 0 1\n   0 0 0\n   1 0 1\n\n, , age = 47\n\n    survival\nmale 0 1\n   0 1 0\n   1 0 0\n\n, , age = 50\n\n    survival\nmale 0 1\n   0 1 0\n   1 0 0\n\n, , age = 57\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 60\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 62\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\n, , age = 65\n\n    survival\nmale 0 1\n   0 0 0\n   1 1 0\n\ncage &lt;- rep(0,length(donner$age))\n# simplify by defining 2 age classes: over/under 25\ncage[donner$age&lt;=25] &lt;- 1\ncage[donner$age&gt;25] &lt;- 2\ndonner &lt;- data.frame(donner,cage=cage)\nwith(donner, table(male,survival,cage))\n\n, , cage = 1\n\n    survival\nmale  0  1\n   0  1  7\n   1  9  4\n\n, , cage = 2\n\n    survival\nmale  0  1\n   0  4  3\n   1 11  6\n\n\n\nIt’s sometimes useful to sort a vector\n\n\naaa &lt;- rpois(100,5)\nsort(aaa)\n\n  [1]  0  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3\n [26]  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  5  5\n [51]  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6\n [76]  6  6  6  7  7  7  7  7  7  7  7  7  8  8  8  8  9  9  9  9  9 10 11 11 17\n\n# decreasing order\nsort(aaa,decreasing=TRUE)\n\n  [1] 17 11 11 10  9  9  9  9  9  8  8  8  8  7  7  7  7  7  7  7  7  7  6  6  6\n [26]  6  6  6  6  6  6  6  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n [51]  5  5  4  4  4  4  4  4  4  4  4  4  4  4  3  3  3  3  3  3  3  3  3  3  3\n [76]  3  3  3  3  3  3  3  3  3  2  2  2  2  2  2  2  2  2  2  2  1  1  1  1  0\n\n\n\nSorting data frames is a bit more involved, but still straightforward\nuse the function order()\n\n\n# five columns of data again\nsatu &lt;- c(1,2,3,4,5)\ndua &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\ntiga &lt;- sample(c(TRUE,FALSE),5,replace=TRUE)\nempat &lt;- LETTERS[7:11]\nlima &lt;- rnorm(5)\n# construct a data frame\n(collection &lt;- data.frame(satu,dua,tiga,empat,lima))\n\n  satu dua  tiga empat       lima\n1    1   a  TRUE     G -0.5836921\n2    2   b FALSE     H  0.9042409\n3    3   c  TRUE     I -0.1804774\n4    4   d FALSE     J  1.3276193\n5    5   e  TRUE     K  0.3944727\n\no &lt;- order(collection$lima)\ncollection[o,]\n\n  satu dua  tiga empat       lima\n1    1   a  TRUE     G -0.5836921\n3    3   c  TRUE     I -0.1804774\n5    5   e  TRUE     K  0.3944727\n2    2   b FALSE     H  0.9042409\n4    4   d FALSE     J  1.3276193\n\n\n\nthere are definitely better ways to do this using tidy tools like dplyr::arrange()!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#naming-data",
    "href": "gettingR.html#naming-data",
    "title": "1  Getting Started in R",
    "section": "1.21 Naming Data",
    "text": "1.21 Naming Data\n\nThe matrix of aggressive interactions among kids had neither column nor row names\nWe can add the codes used in the Strayer and Strayer (1976) paper\n\n\n## load it again because we cleared all objects!\nkids &lt;- read.table(\"./data/strayer_strayer1976-fig2.txt\", header=FALSE)\nkid.names &lt;- c(\"Ro\",\"Ss\",\"Br\",\"If\",\"Td\",\"Sd\",\"Pe\",\"Ir\",\"Cs\",\"Ka\",\n                \"Ch\",\"Ty\",\"Gl\",\"Sa\", \"Me\",\"Ju\",\"Sh\")\ncolnames(kids) &lt;- kid.names\nrownames(kids) &lt;- kid.names\nkids\n\n   Ro Ss Br If Td Sd Pe Ir Cs Ka Ch Ty Gl Sa Me Ju Sh\nRo  0  1  3  4  1  0  0  1  1  0  1  0  7  0  1  0  0\nSs  1  0  7  8  2  1  1 12  3  0  1  1  4  1  0  0  2\nBr  1  4  0  7  3  2  2  0  1  0  8  1  5  5  0  0  1\nIf  3  3  2  0  3  1 13  3  5  1  0  0  8  3  0  2  1\nTd  1  0  0  3  0  4  6  0  8  5  1  0  1  3  0  2  1\nSd  0  0  0  0  0  0  2  8 11  0  4  0  4  3  0  1  0\nPe  1  0  1  9  3  4  0  2  0  0  1  0  7  9  1  1  0\nIr  0  0  0  1  1  1  2  0  7  5  1  1  1  0  0  0  0\nCs  1  1  1  2  5 11  0  3  0  0  0  0  1  0  0  0  0\nKa  0  0  0  0  0  0  1  0  0  0  0 11  0  1  0  1  4\nCh  4  0  4  3  3  2  1  0  0  0  0  3 11  5  0  2  2\nTy  0  0  0  0  0  0  0  0  0  2  0  0  2  0  8  0  0\nGl  0  1  9  3  0  3  6  0  0  0 11  2  0  1  0  7  5\nSa  0  1  4  0  1  2  1  0  0  0  1  0  1  0  0  0  0\nMe  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0\nJu  0  0  0  0  0  1  0  0  0  0  0  1  1  0  0  0  0\nSh  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0\n\n\n\ncolnames() and rownames() are convenience functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#working-on-lists",
    "href": "gettingR.html#working-on-lists",
    "title": "1  Getting Started in R",
    "section": "1.22 Working on Lists",
    "text": "1.22 Working on Lists\n\napply() applies a function along the margins of a matrix\nlapply() applies a function to a list and generates a list as its output\nsapply() is similar to lapply() but generates a vector as its output\n\n\n# cross-tabulation of sex partners by race/ethnicity from NHSLS\nsextable &lt;- read.csv(\"./data/nhsls_sextable.txt\", header=FALSE)\ndimnames(sextable)[[1]] &lt;- c(\"white\",\"black\",\"hispanic\",\"asian\",\"other\")\ndimnames(sextable)[[2]] &lt;- c(\"white\",\"black\",\"hispanic\",\"asian\",\"other\")\n# take a peek at it\nsextable\n\n         white black hispanic asian other\nwhite     1131    12       16     3    15\nblack        5   268        5     0     0\nhispanic    39     1      115     0     3\nasian       12     0        0    10     4\nother        7     0        1     0    18\n\n# calculate marginals\n(row.sums &lt;- apply(sextable,1,sum))\n\n   white    black hispanic    asian    other \n    1177      278      158       26       26 \n\n(col.sums &lt;- apply(sextable,2,sum))\n\n   white    black hispanic    asian    other \n    1194      281      137       13       40 \n\n# using sapply() gives similar output\nsapply(sextable,sum)\n\n   white    black hispanic    asian    other \n    1194      281      137       13       40 \n\n# create a list -- each element of the list has a different length\naaa &lt;- list(alpha = 1:10, beta = rnorm(50), x = sample(1:100, 100, replace=TRUE))\nlapply(aaa,mean)\n\n$alpha\n[1] 5.5\n\n$beta\n[1] -0.0248892\n\n$x\n[1] 50.63\n\n# more compact as a vector\nsapply(aaa,mean)\n\n     alpha       beta          x \n 5.5000000 -0.0248892 50.6300000 \n\n# compare the output of sapply() to lapply()\nlapply(sextable,sum)\n\n$white\n[1] 1194\n\n$black\n[1] 281\n\n$hispanic\n[1] 137\n\n$asian\n[1] 13\n\n$other\n[1] 40\n\n\n\nThe apply family of functions used to be more widely used and have been largely supplanted by the immensely powerful tools in dplyr and the tidyverse more generally",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#flow-control-if",
    "href": "gettingR.html#flow-control-if",
    "title": "1  Getting Started in R",
    "section": "1.23 Flow Control: if",
    "text": "1.23 Flow Control: if\n\nif allows you to conditionally evaluate expressions.\nThe basic syntax of an if statement is: if(condition) true.branch else false.branch\nThe else part of the statement is optional\n\n\n(coin &lt;- sample(c(\"heads\",\"tails\"),1))\n\n[1] \"tails\"\n\nif(coin==\"tails\") b &lt;- 1 else b &lt;- 0\nb\n\n[1] 1\n\n\n\nSometimes you can use the very efficient ifelse statement\nifelse takes three arguments: (1) the logical test, (2) the result if TRUE, (3) the result if FALSE\n\n\nx &lt;- 4:-2\n# sqrt(x) produces warnings, but using ifelse to check works without producing warings\nsqrt(ifelse(x &gt;= 0, x, NA))\n\n[1] 2.000000 1.732051 1.414214 1.000000 0.000000       NA       NA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#flow-control-for",
    "href": "gettingR.html#flow-control-for",
    "title": "1  Getting Started in R",
    "section": "1.24 Flow Control: for",
    "text": "1.24 Flow Control: for\n\nIf you want to repeat an action over and over again you need a loop\nLoops are mostly generated using for statements\nThe basic syntax of a for loop is: for(item in sequence) statement(s)\n\n\nx &lt;- 1:5\nfor(i in 1:5) print(x[i])\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\nThat’s a pretty silly for loop – there are much more important uses of for loops!\nIf there are multiple statements executed by a for loop, those statements must be enclosed in curly braces, {}\nWe need to be careful with for loops because they can slow code down, particularly when they are nested and the number of iterations is very large.\nVectorizing and using mapping functions like apply and its relatives can greatly speed your code up",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "gettingR.html#using-packages",
    "href": "gettingR.html#using-packages",
    "title": "1  Getting Started in R",
    "section": "1.25 Using Packages",
    "text": "1.25 Using Packages\n\nMuch of the functionality of R comes from the many contributed packages\nTo use a package, you must first install it\nThis can be done at the command line using install.packages(package_name)\nIt is often more convenient to use a menu command\nin RStudio this is under Tools&gt;Install Packages...\nOnce a package is installed, you must load it in order to use it\nDo this using the library() command\n\n\nlibrary(igraph)\n# might as well do something with it\n# a small graph\ng &lt;- make_graph( c(1,2, 1,3, 2,3, 3,5), n=5 )\nplot(g)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started in R</span>"
    ]
  },
  {
    "objectID": "drawing.html",
    "href": "drawing.html",
    "title": "3  Theoretical Scientific Figures in R",
    "section": "",
    "text": "3.1 Introduction\nR has powerful graphics capabilities. While we typically use these for plotting data, we can also make publication-quality plots for elucidating theoretical topics as well.\nThese notes are a very tentative start to a much larger body of work. I hope they are nonetheless helpful in their rather incomplete form.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Theoretical Scientific Figures in `R`</span>"
    ]
  },
  {
    "objectID": "graphs.html",
    "href": "graphs.html",
    "title": "4  Actual Graphs",
    "section": "",
    "text": "4.1 Some Definitions\nIn this chapter, I will focus on drawing graphs using the R package igraph. I have a more thorough introduction to graphs elsewhere\nA graph is simply a collection of vertices (or nodes) and edges (or ties). We can denote this \\(\\mathcal{G}(V,E)\\), where \\(V\\) is a the vertex set and \\(E\\) is the edge set. The vertices of the graph represent the actors in the social system. These are usually individual people, but they could be households, geographical localities, institutions, or other social entities. The edges of the graph represent the relations between these entities (e.g., “is friends with” or “has sexual intercourse with” or “sends money to”). These edges can be directed (e.g., “sends money to”) or undirected (e.g., “within 2 meters of”).\nWhen the relations that define the graph are directional, we have a directed graph or digraph.\nGraphs (and digraphs) can be binary (i.e., presence/absence of a relationship) or valued (e.g., “groomed five times in the observation period”, “sent $100”).\nA graph (with no self-loops) with \\(n\\) vertices has \\({n \\choose 2} = n(n-1)/2\\) possible unordered pairs. This number (which can get very big!) is important for defining the density of a graph, i.e., the fraction of all possible relations that actually exist in a network.\nA bipartite graph is a graph where all the nodes of a graph can be partitioned into two sets \\(\\mathcal{V}_1\\) and \\(\\mathcal{V}_2\\) such that for all edges in the graph connects and unordered pair where one vertex comes from \\(\\mathcal{V}_1\\) and the other from \\(\\mathcal{V}_2\\). Often called an “affiliation graph” as bipartite graphs are used to represent people’s affiliations to organizations or events.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Actual Graphs</span>"
    ]
  },
  {
    "objectID": "odes.html",
    "href": "odes.html",
    "title": "5  ODEs in R",
    "section": "",
    "text": "5.1 Lotka-Volterra Model\nWe can also use R to numerically integrate systems of ordinary differential equations (ODEs). These are commonly used in ecology and epidemiology.\nWe do this using the package deSolve, which has some excellent learning resources to support it (check out the vignettes).\nThe Italian biologist Humberto D’Ancona noted that during the first World War, the composition of fish in the markets around the Adriatic Sea changed substantially. During the war, the percentage of predatory fish for sale in the markets of Trieste, Fiume, and Venice increased. D’Ancona had no explanation for this and approached his father-in-law, the eminent mathematician Vito Volterra, with the riddle. Volterra’s solution forms the foundation for nearly all subsequent theory regarding the interaction of species within communities. The great American biologist and demographer, Alfred Lotka, developed the same framework about the same time and the equations have since been known as the Lotka-Volterra model for predatory/prey dynamics.\nThe classical theory of species interactions is attributable to Alfred Lotka and Vito Volterra involves reducing communities to a single consumer-resource relationship – typically between a primary consumer (i.e., a herbivore) and a secondary consumer (i.e., a carnivore).\nThe assumptions of Lotka-Volterra model include: - in the absence of a predator, the prey population increases exponentially - in the absence of prey, the predator population decays exponentially - per capita rate of kill a linear function of prey density - each kill contributes equally to predator growth\nrequire(deSolve)\nlv &lt;- function(t, x, parms) {\n  with(as.list(parms), {\n    dx1 &lt;- r1*x[1] - c1*x[1]*x[2]\n    dx2 &lt;- -r2*x[2] + c2*x[1]*x[2]\n    results &lt;- c(dx1,dx2)\n    list(results)\n  })\n}\n\nxstart &lt;- c(x1=10,x2=1)\ntimes &lt;- seq(0,100,length=1001)\nparms &lt;- c(r1=0.5,r2=0.5, c1=0.1,c2=0.02)\nout1 &lt;- as.data.frame(ode(xstart,times,lv,parms))\n\nwith(out1, plot(time, x1, type=\"l\", lwd=3, col=\"red\", \n                xlab=\"Time\", ylab=\"Population Size\", xlim=c(0,100), ylim=c(0,90)))\nwith(out1, lines(time, x2, ,lwd=3, col=\"blue\"))\nlegend(\"topleft\",c(\"prey\",\"predator\"), lwd=3, col=c(\"red\",\"blue\"))\n\n\n\n\n\n\n\nwith(out1, plot(x1, x2, type=\"l\", col=\"magenta\", lwd=3, xlab=\"Prey Population Size\", ylab=\"Predator Population Size\",\n                xlim=c(0,90), ylim=c(0,20)))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ODEs in `R`</span>"
    ]
  },
  {
    "objectID": "interpreting.html",
    "href": "interpreting.html",
    "title": "2  Interpreting Scientific Figures",
    "section": "",
    "text": "2.1 Introduction\nUnderstanding scientific figures is an important part of becoming a scientist or a critical consumer of scientific information. This is a skill that, alas, is generally not taught in most schools. Here, I will try to provide a gentle introduction to reading scientific figures, especially theoretical plots. In chapter Chapter 3, we go into some detail on how to generate scientific plots in R.\nWe use theory in science to bring order to the complexity we observe in the world. Theory generates our hypotheses but it also guides us in what we observe, how we measure it, and what we should find surprising. Surprise is essential for the scientific enterprise because it is the surprise that comes when we observe something novel from a process we thought we understood that generates innovation and explanation.\nA couple starting points. We will use some very basic calculus here: Derivatives, second derivatives, and Taylor series, which give us slopes, curvature, and tangents for making figures.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Interpreting Scientific Figures</span>"
    ]
  }
]